{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Open-Source Search Engine with Apache Lucene / Solr Provides integrated research tools for easier searching, monitoring, analytics, discovery & text mining (of heterogenous & large document sets & news) with free software on your own server. Search engine (Fulltext search) Easy full text search across multiple data sources and many different file formats. Just enter a search query (which can include powerful search operators ) and navigate through the results. Thesaurus & Grammar (Semantic search) Based on a thesaurus the multilingual semantic search engine will find synonyms, hyponyms and aliases , too. Using heuristics for grammar rules like stemming it can find other word forms, too. Interactive filters (Faceted search) Easy navigation through many results with interactive filters (faceted search) which aggregate an overview over and interactive filters for (meta) data like authors, organizations, persons, places, dates, products, tags or document types. Exploration, browsing & preview (Exploratory search) Explore your data or search results with an overview of aggregated search results by different facets with named entities (i.e. file paths, tags, persons, locations, organisations or products) , while browsing with comfortable navigation through search results or document sets. View previews (i.e. PDF, extracted Text, Table rows or Images). Analyze or review document sets by preview, extracted text or wordlists for textmining . Collaborative annotation & tagging (Social search & collaborative filtering) Tag your documents with keywords, categories, names or text notes that are not included in the original content to find them better later (document management & knowledge management) or in other research or search contexts or to be able to filter annotated or tagged documents by interactive filters (faceted search). Or evaluate, value or assess or filter documents (i.e. for validation or collaborative filtering). Data Visualization (Dataviz) Visualizing data such as: - document dates as trend charts - text analysis as word clouds - connections and networks in visual graph view - view results with geodata as interactive maps . Monitoring: Alerts & Watchlists (Newsfeeds) Stay informed via watchlists for: - news alerts from media monitoring - activity streams of new or changed documents on file shares You can subscribe to searches and filters as RSS-Newsfeed and get notifications when there are changed or new documents, news or search results for your keywords, search context or filter. Supports different file formats Open Semantic Search can help you index and search your data whether you are working with: - structured data like databases, tables or spreadsheets - unstructured data like text documents - E-Mails - even scanned legacy documents - text files - Microsoft Office, OpenOffice, and LibreOffice docuemnts including Excel and Calc - PDF - CSV - Images (photos, pictures, JPG, TIFF) - Videos And that isn't all, see a full list of supported file formats . Supports multiple data sources You can find all your data in one place. Search many different data sources like files and folders, file server, file shares , databases , websites, Content Management Systems, RSS-Feeds and more. The Connectors and Importers of the Extract Transform Load (ETL) framework for Data Integration connect and combine multiple data sources and, as an integrated document analysis and data enrichment framework, it enhances the data with the analysis results of diverse analytics tools. Automatic text recognition Optical character recognition (OCR) or automatic text recognition for images and text content stored in graphical format like scanned legacy documents, screenshots or photographed documents in the form of image files or embedded in PDF files. Open-Source enterprise search and information retrieval technology based on interoperable open standards Mobile (Responsive Design) Open Semantic Search can be used with every desktop (Linux, Windows or Mac) and web browser. With its responsive design and open standards like HTML5 it is possible to search with tablets, smartphones and other mobile devices as well. Metadata management (RDF) Structure your research, investigation, navigation, document sets, collections, metadata forms or notes in a Semantic Wiki, Drupal or another content management system (CMS) or with an innovative annotation framework with taxonomies and custom fields for tagging documents, annotations, linking relationships, mapping and structured notes. You can integrate powerful and flexible metadata management or annotation tools using interoperable open standards like the Resource Description Framework (RDF) and the Simple Knowledge Organization System ( SKOS ). Filesystem monitoring Using file monitoring , new or changed files are indexed within seconds without requiring frequent recrawls (which is not possible often if there are many files). Colleagues are able to find new data immediately without (often forgotten) uploads to a data or document management system (DMS) or filling out a data registration form for each new or changed document or dataset in a data management system, data registry or digital asset management (DAM) system.","title":"About"},{"location":"#open-source-search-engine-with-apache-lucene-solr","text":"Provides integrated research tools for easier searching, monitoring, analytics, discovery & text mining (of heterogenous & large document sets & news) with free software on your own server.","title":"Open-Source Search Engine with Apache Lucene / Solr"},{"location":"#search-engine-fulltext-search","text":"Easy full text search across multiple data sources and many different file formats. Just enter a search query (which can include powerful search operators ) and navigate through the results.","title":"Search engine (Fulltext search)"},{"location":"#thesaurus-grammar-semantic-search","text":"Based on a thesaurus the multilingual semantic search engine will find synonyms, hyponyms and aliases , too. Using heuristics for grammar rules like stemming it can find other word forms, too.","title":"Thesaurus &amp; Grammar (Semantic search)"},{"location":"#interactive-filters-faceted-search","text":"Easy navigation through many results with interactive filters (faceted search) which aggregate an overview over and interactive filters for (meta) data like authors, organizations, persons, places, dates, products, tags or document types.","title":"Interactive filters (Faceted search)"},{"location":"#exploration-browsing-preview-exploratory-search","text":"Explore your data or search results with an overview of aggregated search results by different facets with named entities (i.e. file paths, tags, persons, locations, organisations or products) , while browsing with comfortable navigation through search results or document sets. View previews (i.e. PDF, extracted Text, Table rows or Images). Analyze or review document sets by preview, extracted text or wordlists for textmining .","title":"Exploration, browsing &amp; preview (Exploratory search)"},{"location":"#collaborative-annotation-tagging-social-search-collaborative-filtering","text":"Tag your documents with keywords, categories, names or text notes that are not included in the original content to find them better later (document management & knowledge management) or in other research or search contexts or to be able to filter annotated or tagged documents by interactive filters (faceted search). Or evaluate, value or assess or filter documents (i.e. for validation or collaborative filtering).","title":"Collaborative annotation &amp; tagging (Social search &amp; collaborative filtering)"},{"location":"#data-visualization-dataviz","text":"Visualizing data such as: - document dates as trend charts - text analysis as word clouds - connections and networks in visual graph view - view results with geodata as interactive maps .","title":"Data Visualization (Dataviz)"},{"location":"#monitoring-alerts-watchlists-newsfeeds","text":"Stay informed via watchlists for: - news alerts from media monitoring - activity streams of new or changed documents on file shares You can subscribe to searches and filters as RSS-Newsfeed and get notifications when there are changed or new documents, news or search results for your keywords, search context or filter.","title":"Monitoring: Alerts &amp; Watchlists (Newsfeeds)"},{"location":"#supports-different-file-formats","text":"Open Semantic Search can help you index and search your data whether you are working with: - structured data like databases, tables or spreadsheets - unstructured data like text documents - E-Mails - even scanned legacy documents - text files - Microsoft Office, OpenOffice, and LibreOffice docuemnts including Excel and Calc - PDF - CSV - Images (photos, pictures, JPG, TIFF) - Videos And that isn't all, see a full list of supported file formats .","title":"Supports different file\u00a0formats"},{"location":"#supports-multiple-data-sources","text":"You can find all your data in one place. Search many different data sources like files and folders, file server, file shares , databases , websites, Content Management Systems, RSS-Feeds and more. The Connectors and Importers of the Extract Transform Load (ETL) framework for Data Integration connect and combine multiple data sources and, as an integrated document analysis and data enrichment framework, it enhances the data with the analysis results of diverse analytics tools.","title":"Supports multiple data sources"},{"location":"#automatic-text-recognition","text":"Optical character recognition (OCR) or automatic text recognition for images and text content stored in graphical format like scanned legacy documents, screenshots or photographed documents in the form of image files or embedded in PDF files.","title":"Automatic text recognition"},{"location":"#open-source-enterprise-search-and-information-retrieval-technology-based-on-interoperable-open-standards","text":"","title":"Open-Source enterprise search and information retrieval technology based on interoperable open standards"},{"location":"#mobile-responsive-design","text":"Open Semantic Search can be used with every desktop (Linux, Windows or Mac) and web browser. With its responsive design and open standards like HTML5 it is possible to search with tablets, smartphones and other mobile devices as well.","title":"Mobile (Responsive Design)"},{"location":"#metadata-management-rdf","text":"Structure your research, investigation, navigation, document sets, collections, metadata forms or notes in a Semantic Wiki, Drupal or another content management system (CMS) or with an innovative annotation framework with taxonomies and custom fields for tagging documents, annotations, linking relationships, mapping and structured notes. You can integrate powerful and flexible metadata management or annotation tools using interoperable open standards like the Resource Description Framework (RDF) and the Simple Knowledge Organization System ( SKOS ).","title":"Metadata management (RDF)"},{"location":"#filesystem-monitoring","text":"Using file monitoring , new or changed files are indexed within seconds without requiring frequent recrawls (which is not possible often if there are many files). Colleagues are able to find new data immediately without (often forgotten) uploads to a data or document management system (DMS) or filling out a data registration form for each new or changed document or dataset in a data management system, data registry or digital asset management (DAM) system.","title":"Filesystem monitoring"},{"location":"connector/db/","text":"Full text search, faceted search & textmining in a database (SQL DB) Connectors for Relational Database Management Systems (RDBMS) & Structured Query Lanaguage (SQL) to Solr or Elastic Search To be able to use simple and powerful search user interfaces (UI) for full text search , faceted search, semantic search and ontology or thesaurus based text mining research tools on structured data from fields of a SQL database, import their tables to the search index: Index SQL databases like MySQL or Postgresql There are multiple ways or open source connectors for import SQL databases like MySQL, MariaDB, PostgreSQL and other SQL databases based on Structured Query Language (SQL) or supported by Open Database Connectivity (ODBC) or Java DataBase Connectivity (JDBC) to Apache Solr index: Import SQL database to Solr search index Free Software for indexing SQL to Apache Solr: * Apache Manifold JDBC Connector * Apache Nifi - Read from ExecuteSQL and write to Solr index * Setup the built in RDBMS data import handler for your database * Export your database to a CSV format and use the build in CSV import of Solr * Write a short Python script for database import based on Open Semantic ETL with a concrete SQL query, write the columns to the data variable (data type is a Python dictionary i.e. data['columnname'] = columnvalue and export/write it to index by calling etl.process(data=data) * Other Extract Transform Load (ETL) - Frameworks like Talend Open Studio Import SQL database to Elastic Search index Free Software for indexing SQL to Elastic Search: * Apache Manifold JDBC Connector * Apache Nifi - Read from ExecuteSQL and write to Elastic Search index * Elastic search JDBC Importer * Export your database to a CSV format and use the build in CSV import of Solr * Write a short Python script for database import based on Open Semantic ETL with a concrete SQL query, write the columns to the data variable (data type is a Python dictionary i.e. data['columnname'] = columnvalue and export/write it to index by calling etl.process(data=data) * Other Extract Transform Load (ETL) - Frameworks like Talend Open Studio","title":"Full text search, faceted search & textmining in a database (SQL DB)"},{"location":"connector/db/#full-text-search-faceted-search-textmining-in-a-database-sql-db","text":"","title":"Full text search, faceted search &amp; textmining in a database (SQL DB)"},{"location":"connector/db/#connectors-for-relational-database-management-systems-rdbms-structured-query-lanaguage-sql-to-solr-or-elastic-search","text":"To be able to use simple and powerful search user interfaces (UI) for full text search , faceted search, semantic search and ontology or thesaurus based text mining research tools on structured data from fields of a SQL database, import their tables to the search index:","title":"Connectors for Relational Database Management Systems (RDBMS) &amp; Structured Query Lanaguage (SQL) to Solr or Elastic Search"},{"location":"connector/db/#index-sql-databases-like-mysql-or-postgresql","text":"There are multiple ways or open source connectors for import SQL databases like MySQL, MariaDB, PostgreSQL and other SQL databases based on Structured Query Language (SQL) or supported by Open Database Connectivity (ODBC) or Java DataBase Connectivity (JDBC) to Apache Solr index:","title":"Index SQL databases like MySQL or Postgresql"},{"location":"connector/db/#import-sql-database-to-solr-search-index","text":"Free Software for indexing SQL to Apache Solr: * Apache Manifold JDBC Connector * Apache Nifi - Read from ExecuteSQL and write to Solr index * Setup the built in RDBMS data import handler for your database * Export your database to a CSV format and use the build in CSV import of Solr * Write a short Python script for database import based on Open Semantic ETL with a concrete SQL query, write the columns to the data variable (data type is a Python dictionary i.e. data['columnname'] = columnvalue and export/write it to index by calling etl.process(data=data) * Other Extract Transform Load (ETL) - Frameworks like Talend Open Studio","title":"Import SQL database to Solr search index"},{"location":"connector/db/#import-sql-database-to-elastic-search-index","text":"Free Software for indexing SQL to Elastic Search: * Apache Manifold JDBC Connector * Apache Nifi - Read from ExecuteSQL and write to Elastic Search index * Elastic search JDBC Importer * Export your database to a CSV format and use the build in CSV import of Solr * Write a short Python script for database import based on Open Semantic ETL with a concrete SQL query, write the columns to the data variable (data type is a Python dictionary i.e. data['columnname'] = columnvalue and export/write it to index by calling etl.process(data=data) * Other Extract Transform Load (ETL) - Frameworks like Talend Open Studio","title":"Import SQL database to Elastic Search index"},{"location":"connector/files/","text":"Crawl and index files, file folders or file servers How to index files like Word documents, PDF files and whole document folders to Apache Solr or Elastic Search? This connector and command line tools crawl and index directories and files from your filesystem and index it to Apache Solr or Elastic Search for full text search and text mining . If you use Linux that means you can crawl whatever is mountable to Linux into an Apache Solr or Elastic Search index or into a triplestore. Index different file system types to Solr or Elastic Search This can be a hard disk or partitions formated with fat , ext3 , ext4 or a file server connected via ntfs , file shares like smb or even sshfs or sftp on servers, private file sharing services like Seafile or OwnCloud on own servers or Dropbox, Amazon or other storage services in the cloud. Data enrichment by different data analytic tools This connector integrates enhanced data enrichment and data analysis plugins like automatic text recognition (OCR) for images and photos (i.e. as files like PNG, JPG, GIF ...) or inside PDFs (i.e.scanned Documents) using Tesseract OCR. Usage Index a file or directory: Web admin interface Using the web admin interface * Open the page Files * Enter filename to the form * Press button \"crawl\" Command line Using the command line interface (CLI): opensemanticsearch-index-file *filename* API Using the REST-API: http://127.0.0.1/search-apps/api/index-file?uri=*/home/opensemanticsearch/readme.txt* Config Config file for indexing files: */etc/opensemanticsearch/connector-files*","title":"Crawl and index files, file folders or file servers"},{"location":"connector/files/#crawl-and-index-files-file-folders-or-file-servers","text":"","title":"Crawl and index files, file folders or file servers"},{"location":"connector/files/#how-to-index-files-like-word-documents-pdf-files-and-whole-document-folders-to-apache-solr-or-elastic-search","text":"This connector and command line tools crawl and index directories and files from your filesystem and index it to Apache Solr or Elastic Search for full text search and text mining . If you use Linux that means you can crawl whatever is mountable to Linux into an Apache Solr or Elastic Search index or into a triplestore.","title":"How to index files like Word documents, PDF files and whole document folders to Apache Solr or Elastic Search?"},{"location":"connector/files/#index-different-file-system-types-to-solr-or-elastic-search","text":"This can be a hard disk or partitions formated with fat , ext3 , ext4 or a file server connected via ntfs , file shares like smb or even sshfs or sftp on servers, private file sharing services like Seafile or OwnCloud on own servers or Dropbox, Amazon or other storage services in the cloud.","title":"Index different file system types to Solr or Elastic Search"},{"location":"connector/files/#data-enrichment-by-different-data-analytic-tools","text":"This connector integrates enhanced data enrichment and data analysis plugins like automatic text recognition (OCR) for images and photos (i.e. as files like PNG, JPG, GIF ...) or inside PDFs (i.e.scanned Documents) using Tesseract OCR.","title":"Data enrichment by different data analytic tools"},{"location":"connector/files/#usage","text":"Index a file or directory:","title":"Usage"},{"location":"connector/files/#web-admin-interface","text":"Using the web admin interface * Open the page Files * Enter filename to the form * Press button \"crawl\"","title":"Web admin interface"},{"location":"connector/files/#command-line","text":"Using the command line interface (CLI): opensemanticsearch-index-file *filename*","title":"Command line"},{"location":"connector/files/#api","text":"Using the REST-API: http://127.0.0.1/search-apps/api/index-file?uri=*/home/opensemanticsearch/readme.txt*","title":"API"},{"location":"connector/files/#config","text":"Config file for indexing files: */etc/opensemanticsearch/connector-files*","title":"Config"},{"location":"connector/files/pdf/","text":"Index PDF files for search and text mining with Solr or Elastic Search How to index a PDF file or many PDF documents for full text search and text mining You can search and do textmining with the content of many PDF documents, since the content of PDF files is extracted and text in images were recognized by optical character recognition (OCR) automatically. Indexing a PDF file to the Solr or Elastic Search Therefore you have to index the PDF documents or file directories or file shares that contain PDF documents to the Solr or Elastic Search server index: Desktop search If you use Open Semantic Desktop Search , just copy the PDF files to a directory that is indexed automatically or add the directory with the PDF files to shared folders for indexing and restart the virtual machine or press the \"Index\" button within the VM. File monitoring If you use an file share where file monitoring is active, just copy the PDF files to an monitored folder or file directory and wait, until they are indexed automatically. Web admin interface Using the web admin interface * Open the page Files * Enter filename of the PDF file to the form * Press button \"crawl\" Command line Using the command line interface (CLI): opensemanticsearch-index-file *filename* API Using the REST-API within your tools, script or within your browser: http://127.0.0.1/search-apps/api/index-file?uri=*/home/opensemanticsearch/document.pdf* Indexing a folder with PDF files to the Solr or Elastic Search You can index whole folders with PDF documents to Apache Solr or Elastic Search the same way. Just use the name of the file directory or folder instead of a single file name. Config Config file for indexing files: */etc/opensemanticsearch/connector-files*","title":"Index PDF files for search and text mining with Solr or Elastic Search"},{"location":"connector/files/pdf/#index-pdf-files-for-search-and-text-mining-with-solr-or-elastic-search","text":"","title":"Index PDF files for search and text mining with Solr or Elastic Search"},{"location":"connector/files/pdf/#how-to-index-a-pdf-file-or-many-pdf-documents-for-full-text-search-and-text-mining","text":"You can search and do textmining with the content of many PDF documents, since the content of PDF files is extracted and text in images were recognized by optical character recognition (OCR) automatically.","title":"How to index a PDF file or many PDF documents for full text search and text mining"},{"location":"connector/files/pdf/#indexing-a-pdf-file-to-the-solr-or-elastic-search","text":"Therefore you have to index the PDF documents or file directories or file shares that contain PDF documents to the Solr or Elastic Search server index:","title":"Indexing a PDF file to the Solr or Elastic Search"},{"location":"connector/files/pdf/#desktop-search","text":"If you use Open Semantic Desktop Search , just copy the PDF files to a directory that is indexed automatically or add the directory with the PDF files to shared folders for indexing and restart the virtual machine or press the \"Index\" button within the VM.","title":"Desktop search"},{"location":"connector/files/pdf/#file-monitoring","text":"If you use an file share where file monitoring is active, just copy the PDF files to an monitored folder or file directory and wait, until they are indexed automatically.","title":"File monitoring"},{"location":"connector/files/pdf/#web-admin-interface","text":"Using the web admin interface * Open the page Files * Enter filename of the PDF file to the form * Press button \"crawl\"","title":"Web admin interface"},{"location":"connector/files/pdf/#command-line","text":"Using the command line interface (CLI): opensemanticsearch-index-file *filename*","title":"Command line"},{"location":"connector/files/pdf/#api","text":"Using the REST-API within your tools, script or within your browser: http://127.0.0.1/search-apps/api/index-file?uri=*/home/opensemanticsearch/document.pdf*","title":"API"},{"location":"connector/files/pdf/#indexing-a-folder-with-pdf-files-to-the-solr-or-elastic-search","text":"You can index whole folders with PDF documents to Apache Solr or Elastic Search the same way. Just use the name of the file directory or folder instead of a single file name.","title":"Indexing a folder with PDF files to the Solr or Elastic Search"},{"location":"connector/files/pdf/#config","text":"Config file for indexing files: */etc/opensemanticsearch/connector-files*","title":"Config"},{"location":"connector/hypothesis/","text":"Import Hypothesis web annotations, tags & documents to Solr and Elastic Search Integration of Hypothesis with Solr & Elastic Search The integrated Open Source visual annotation tool Hypothesis provides an powerful visual user interface for (collaborative) web annotation and tagging by human editors, teams and groups that supports not only to tag documents and add page notes, but allows you to annotate documents and web pages within the text, even for single words, names, parts of sentences, sentences or paragraphs. Import annotations and tags from Hypothesis for semantic search, faceted search, analytics & textmining By the web user interface for configuration of datasources in the tab Hypothesis you can easyly setup the import / indexing of annotations, tags and annotated documents from the Hypothesis API to the Solr or Elasticsearch search index by data enrichment of indexed documents in search index by hypothesis annotations and tags: If you want not only to import public annotations, you must setup your private API token, that you can find in the settings menu \"developer\" when logged in in hypothes.is: Filters In the tab filter(s) you can set from which user or group annotations should be imported. If you are using the public hypothesis web service, you must set such a filter to prevent import of all public annotations from all users of the web service. Full text search in document content with annotations and text After setup of Hypothesis as a datasource and the first run of the import you can search with full text search the full document content combined/enriched with your notes and tags. Faceted search and interactive filters by Hypothesis tags Additionally you get interactive filters for faceted search, text analytics and text mining by your hypothes.is tags. Automatic named entity recognition / named entity extraction for tagged or annotated documents and web pages Additionally automatic analysis like Named Entity Recognition / automatic extraction of persons, organizations and places by machine learning will be done automatically for documents and web pages you tagged with Hypothesis. Limitations Since hypothesis doesn't support an export to standards like RDF for export (yet?), there is an own plugin for the hypothesis API. The plugin is in early development, so there are some limitations yet: Until implementation of API paging within the next weeks, the plugin imports only the last 200 annotations per user or group. So you have to set the delta time to a period where your last annotations are within this limit.","title":"Import Hypothesis web annotations, tags & documents to Solr and Elastic Search"},{"location":"connector/hypothesis/#import-hypothesis-web-annotations-tags-documents-to-solr-and-elastic-search","text":"","title":"Import Hypothesis web annotations, tags &amp; documents to Solr and Elastic Search"},{"location":"connector/hypothesis/#integration-of-hypothesis-with-solr-elastic-search","text":"The integrated Open Source visual annotation tool Hypothesis provides an powerful visual user interface for (collaborative) web annotation and tagging by human editors, teams and groups that supports not only to tag documents and add page notes, but allows you to annotate documents and web pages within the text, even for single words, names, parts of sentences, sentences or paragraphs.","title":"Integration of Hypothesis with Solr &amp; Elastic Search"},{"location":"connector/hypothesis/#import-annotations-and-tags-from-hypothesis-for-semantic-search-faceted-search-analytics-textmining","text":"By the web user interface for configuration of datasources in the tab Hypothesis you can easyly setup the import / indexing of annotations, tags and annotated documents from the Hypothesis API to the Solr or Elasticsearch search index by data enrichment of indexed documents in search index by hypothesis annotations and tags: If you want not only to import public annotations, you must setup your private API token, that you can find in the settings menu \"developer\" when logged in in hypothes.is:","title":"Import annotations and tags from Hypothesis for semantic search, faceted search, analytics &amp; textmining"},{"location":"connector/hypothesis/#filters","text":"In the tab filter(s) you can set from which user or group annotations should be imported. If you are using the public hypothesis web service, you must set such a filter to prevent import of all public annotations from all users of the web service.","title":"Filters"},{"location":"connector/hypothesis/#full-text-search-in-document-content-with-annotations-and-text","text":"After setup of Hypothesis as a datasource and the first run of the import you can search with full text search the full document content combined/enriched with your notes and tags.","title":"Full text search in document content with annotations and text"},{"location":"connector/hypothesis/#faceted-search-and-interactive-filters-by-hypothesis-tags","text":"Additionally you get interactive filters for faceted search, text analytics and text mining by your hypothes.is tags.","title":"Faceted search and interactive filters by Hypothesis tags"},{"location":"connector/hypothesis/#automatic-named-entity-recognition-named-entity-extraction-for-tagged-or-annotated-documents-and-web-pages","text":"Additionally automatic analysis like Named Entity Recognition / automatic extraction of persons, organizations and places by machine learning will be done automatically for documents and web pages you tagged with Hypothesis.","title":"Automatic named entity recognition / named entity extraction for tagged or annotated documents and web pages"},{"location":"connector/hypothesis/#limitations","text":"Since hypothesis doesn't support an export to standards like RDF for export (yet?), there is an own plugin for the hypothesis API. The plugin is in early development, so there are some limitations yet: Until implementation of API paging within the next weeks, the plugin imports only the last 200 annotations per user or group. So you have to set the delta time to a period where your last annotations are within this limit.","title":"Limitations"},{"location":"connector/rdf/","text":"How to index Linked Data from Resource Description Framework (RDF) to Solr or Elastic Search Import and index linked data from semantic knowledge graph for full text search, faceted search and text mining The open source extract transform load (ETL) plugin enhance_rdf for granular indexing of Resource Description Framework (RDF) knowledge graphs is a data enrichment plugin for RDF files or RDF connector for SPARQL endpoints or triplestores. Since files in Resource Description Framework (RDF) format are a structured graph and more like a dataset or database with multiple or many different entries than a single document, it doesn't extract and index a RDF file only as one whole single file or document like standard document files. It is indexing the RDF data more granular by distinguishing different entities or RDF subjects: Aggregates RDF triples to Solr or Elastic search documents For each entity or RDF subject in all RDF triples it generates an own searchable semi-structured (for faceted search) but text based (for full text search) document with the content type knowledge graph . Or in other words it creates for each entity or subject from the RDF graph a database row with columns for each property containing all readable and searchable text. Therefore it index each URI/subject as Solr or Elastic Search document with its properties (RDF predicates) as facets/fields/database columns and its objects (RDF objects) as values. Transform references like URIs to human readable and searchable labels For full text search capabilities this Extract Transform Load (ETL) plugin transforms referenced URIs to their label(s), i.e. from properties like rdfs:label or SKOS labels from an Simple Knowledge Organisation System (SKOS) based thesaurus . User interfaces for full text search, faceted search, fuzzy search & text analysis in linked data knowledge graphs So you can use easy search user interfaces (UI) for full text search, interactive filters (faceted search) & text mining on Resource Description Framework (RDF) knowledge graphs How to import and index an linked data graph from a file in Resource Description Framework (RDF) The RDF enhancer plugin (free open source software) is enabled if not disabled by exclude enhance_rdf from config['plugins'] . Then just index the RDF file by one of the following methods or tools for indexing files : * copy it to an folder that is configured for indexing in your desktop search environment * copy it to an folder with active file monitoring * use the command line tool opensemanticsearch-index-file * or call the file indexing API in a scheduler like Cron or from your own tools and scripts How to import and index an linked data from a graph database or triple store (SPARQL) Use the command line tool etl_sparql to import graphs or part of graphs from triple stores like Apache Jena by SPARQL queries. opensemanticsearch-index-sparql *endpoint* \"*SPARQL query*\" Open Source tools for import RDF to Solr or Elastic Search Alternate free software and open source tools for import of RDF data or SPARQL results to Solr or Elastic Search : * SolrRDF - Solr plugin for RDF - Elastic Search River for RDF Elastic Search Plugin for importing RDF from Apache Jena","title":"How to index Linked Data from Resource Description Framework (RDF) to Solr or Elastic Search"},{"location":"connector/rdf/#how-to-index-linked-data-from-resource-description-framework-rdf-to-solr-or-elastic-search","text":"","title":"How to index Linked Data from Resource Description Framework (RDF) to Solr or Elastic Search"},{"location":"connector/rdf/#import-and-index-linked-data-from-semantic-knowledge-graph-for-full-text-search-faceted-search-and-text-mining","text":"The open source extract transform load (ETL) plugin enhance_rdf for granular indexing of Resource Description Framework (RDF) knowledge graphs is a data enrichment plugin for RDF files or RDF connector for SPARQL endpoints or triplestores. Since files in Resource Description Framework (RDF) format are a structured graph and more like a dataset or database with multiple or many different entries than a single document, it doesn't extract and index a RDF file only as one whole single file or document like standard document files. It is indexing the RDF data more granular by distinguishing different entities or RDF subjects:","title":"Import and index linked data from semantic knowledge graph for full text search, faceted search and text mining"},{"location":"connector/rdf/#aggregates-rdf-triples-to-solr-or-elastic-search-documents","text":"For each entity or RDF subject in all RDF triples it generates an own searchable semi-structured (for faceted search) but text based (for full text search) document with the content type knowledge graph . Or in other words it creates for each entity or subject from the RDF graph a database row with columns for each property containing all readable and searchable text. Therefore it index each URI/subject as Solr or Elastic Search document with its properties (RDF predicates) as facets/fields/database columns and its objects (RDF objects) as values.","title":"Aggregates RDF triples to Solr or Elastic search documents"},{"location":"connector/rdf/#transform-references-like-uris-to-human-readable-and-searchable-labels","text":"For full text search capabilities this Extract Transform Load (ETL) plugin transforms referenced URIs to their label(s), i.e. from properties like rdfs:label or SKOS labels from an Simple Knowledge Organisation System (SKOS) based thesaurus .","title":"Transform references like URIs to human readable and searchable labels"},{"location":"connector/rdf/#user-interfaces-for-full-text-search-faceted-search-fuzzy-search-text-analysis-in-linked-data-knowledge-graphs","text":"So you can use easy search user interfaces (UI) for full text search, interactive filters (faceted search) & text mining on Resource Description Framework (RDF) knowledge graphs","title":"User interfaces for full text search, faceted search, fuzzy search &amp; text analysis in linked data knowledge graphs"},{"location":"connector/rdf/#how-to-import-and-index-an-linked-data-graph-from-a-file-in-resource-description-framework-rdf","text":"The RDF enhancer plugin (free open source software) is enabled if not disabled by exclude enhance_rdf from config['plugins'] . Then just index the RDF file by one of the following methods or tools for indexing files : * copy it to an folder that is configured for indexing in your desktop search environment * copy it to an folder with active file monitoring * use the command line tool opensemanticsearch-index-file * or call the file indexing API in a scheduler like Cron or from your own tools and scripts","title":"How to import and index an linked data graph from a file in Resource Description Framework (RDF)"},{"location":"connector/rdf/#how-to-import-and-index-an-linked-data-from-a-graph-database-or-triple-store-sparql","text":"Use the command line tool etl_sparql to import graphs or part of graphs from triple stores like Apache Jena by SPARQL queries. opensemanticsearch-index-sparql *endpoint* \"*SPARQL query*\"","title":"How to import and index an linked data from a graph database or triple store (SPARQL)"},{"location":"connector/rdf/#open-source-tools-for-import-rdf-to-solr-or-elastic-search","text":"Alternate free software and open source tools for import of RDF data or SPARQL results to Solr or Elastic Search : * SolrRDF - Solr plugin for RDF - Elastic Search River for RDF Elastic Search Plugin for importing RDF from Apache Jena","title":"Open Source tools for import RDF to Solr or Elastic Search"},{"location":"connector/rss/","text":"RSS-Feed or Newsfeed Indexing a RSS-Newsfeed to Solr. Will download and index the linked websites or documents, not only the content of the rss (often only teaser). Usage Using the web user interface RSS-Feed manager If you want to manage many feeds and import feeds periodically, you might want to use the web user interface RSS-Feed manager . Using the web admin interface Open the page RSS-Feeds Enter the URL of the feed to the form Press button \"crawl\" Using the commandline opensemanticsearch-index-rss *http://www.opensemanticsearch.org/feed* Using the REST-API http://127.0.0.1/search-apps/api/index-rss?uri=*http://www.opensemanticsearch.org/feed* Alternates You can use Apache ManifoldCF to import RSS-Feeds into to Solr.","title":"RSS-Feed or Newsfeed"},{"location":"connector/rss/#rss-feed-or-newsfeed","text":"Indexing a RSS-Newsfeed to Solr. Will download and index the linked websites or documents, not only the content of the rss (often only teaser).","title":"RSS-Feed or Newsfeed"},{"location":"connector/rss/#usage","text":"","title":"Usage"},{"location":"connector/rss/#using-the-web-user-interface-rss-feed-manager","text":"If you want to manage many feeds and import feeds periodically, you might want to use the web user interface RSS-Feed manager .","title":"Using the web user interface RSS-Feed manager"},{"location":"connector/rss/#using-the-web-admin-interface","text":"Open the page RSS-Feeds Enter the URL of the feed to the form Press button \"crawl\"","title":"Using the web admin interface"},{"location":"connector/rss/#using-the-commandline","text":"opensemanticsearch-index-rss *http://www.opensemanticsearch.org/feed*","title":"Using the commandline"},{"location":"connector/rss/#using-the-rest-api","text":"http://127.0.0.1/search-apps/api/index-rss?uri=*http://www.opensemanticsearch.org/feed*","title":"Using the REST-API"},{"location":"connector/rss/#alternates","text":"You can use Apache ManifoldCF to import RSS-Feeds into to Solr.","title":"Alternates"},{"location":"connector/scraper/","text":"title: Web scraping: Extract structured data from websites authors: - Markus Mandalka Web scraping: Extract structured data from websites Import structured data scraped from websites to the search server This connector integrates the Open-Source Scraping-Framework Scrapy , a Python framework for ETL (Extract, transform and load) to build a customized crawler, parser, data scraper and converter for extracting structured data from websites. This is for websites which dont offer their data in structured open standard formats like RDF, XML, JSON or CSV for which there are comfortable and easy to use connectors yet, so we have to extract all data from HTML pages with a more complicated individual scraper. Where to write the data: Solr dynamic fields If you dont want to use standard fields like title, author and content you dont have to change the config file schema.xml (which defines the fields of the Solr index). Our Solr server is preconfigurated with dynamic fields so that you can fill standard fields like title or contnet or additional dynamic fields like yourfield_b for one boolean, yourfield_bs for booleans, yourfield_s for a string and yourfield_ss for some strings or yourfield_t for a text or yourfield_tt for some texts to be filled with data. Have a look to schema.xml or managed-schema to see all possible data types of dynamic fields, like boolean, string, dates, text and so on. Enable new fields or facets in the user interface If you did not use preconfigurated fields like tags (fieldname is tag_ss ) and want to use them not only to find data but as interactive filters (facets) for the navigation: To enable your own additional fields as facets (interactive filters) in the user interface just map the technical Solr fieldnames to user friendly labels in the config of the user interface with the option $cfg[facet] .","title":"Index"},{"location":"connector/scraper/#web-scraping-extract-structured-data-from-websites","text":"","title":"Web scraping: Extract structured data from websites"},{"location":"connector/scraper/#import-structured-data-scraped-from-websites-to-the-search-server","text":"This connector integrates the Open-Source Scraping-Framework Scrapy , a Python framework for ETL (Extract, transform and load) to build a customized crawler, parser, data scraper and converter for extracting structured data from websites. This is for websites which dont offer their data in structured open standard formats like RDF, XML, JSON or CSV for which there are comfortable and easy to use connectors yet, so we have to extract all data from HTML pages with a more complicated individual scraper.","title":"Import structured data scraped from websites to the search server"},{"location":"connector/scraper/#where-to-write-the-data-solr-dynamic-fields","text":"If you dont want to use standard fields like title, author and content you dont have to change the config file schema.xml (which defines the fields of the Solr index). Our Solr server is preconfigurated with dynamic fields so that you can fill standard fields like title or contnet or additional dynamic fields like yourfield_b for one boolean, yourfield_bs for booleans, yourfield_s for a string and yourfield_ss for some strings or yourfield_t for a text or yourfield_tt for some texts to be filled with data. Have a look to schema.xml or managed-schema to see all possible data types of dynamic fields, like boolean, string, dates, text and so on.","title":"Where to write the data: Solr dynamic fields"},{"location":"connector/scraper/#enable-new-fields-or-facets-in-the-user-interface","text":"If you did not use preconfigurated fields like tags (fieldname is tag_ss ) and want to use them not only to find data but as interactive filters (facets) for the navigation: To enable your own additional fields as facets (interactive filters) in the user interface just map the technical Solr fieldnames to user friendly labels in the config of the user interface with the option $cfg[facet] .","title":"Enable new fields or facets in the user interface"},{"location":"connector/web/","text":"Crawler for indexing websites Index a single Webpage You can index an single Webpage (or an image or an pdf file on a webserver, including automatic text recognition by OCR, if enabled in the connector config /etc/opensemanticsearch/connector-web ): Start indexing by web interface To start indexing a single web page via the web interface (i.e. http://localhost/search/admin/crawl ): Just write the url into the uri field and submit the form. Command line tool Or use or integrate (i.e. in a crontab or in your own scripts) this command line tool: opensemanticsearch-index-web *http://www.opensemanticweb.org/* REST-API Using the REST-API: http://127.0.0.1/search-apps/api/index-web?uri=*http://www.opensemanticsearch.org/* Crawl whole websites or parts of a website You can index a whole website with the web crawler module of Apache ManifoldCF . With its Webinterface you can setup a homepage, a sitemap or a RSS-Feed as the start point and set how deep the crawl should be. Its possible to setup rules which parts to crawl and which to exclude. Another software for crawling a website is Scrapy.","title":"Crawler for indexing websites"},{"location":"connector/web/#crawler-for-indexing-websites","text":"","title":"Crawler for indexing websites"},{"location":"connector/web/#index-a-single-webpage","text":"You can index an single Webpage (or an image or an pdf file on a webserver, including automatic text recognition by OCR, if enabled in the connector config /etc/opensemanticsearch/connector-web ):","title":"Index a single Webpage"},{"location":"connector/web/#start-indexing-by-web-interface","text":"To start indexing a single web page via the web interface (i.e. http://localhost/search/admin/crawl ): Just write the url into the uri field and submit the form.","title":"Start indexing by web interface"},{"location":"connector/web/#command-line-tool","text":"Or use or integrate (i.e. in a crontab or in your own scripts) this command line tool: opensemanticsearch-index-web *http://www.opensemanticweb.org/*","title":"Command line tool"},{"location":"connector/web/#rest-api","text":"Using the REST-API: http://127.0.0.1/search-apps/api/index-web?uri=*http://www.opensemanticsearch.org/*","title":"REST-API"},{"location":"connector/web/#crawl-whole-websites-or-parts-of-a-website","text":"You can index a whole website with the web crawler module of Apache ManifoldCF . With its Webinterface you can setup a homepage, a sitemap or a RSS-Feed as the start point and set how deep the crawl should be. Its possible to setup rules which parts to crawl and which to exclude. Another software for crawling a website is Scrapy.","title":"Crawl whole websites or parts of a website"},{"location":"dev/","text":"Development of free open source search engine software Free Software Open Semantic Search is Open Source software based on Apache Lucene / Solr Enterprise Search and Python. So you are allowed to read, to change and to extend the source code. Source code The source code is included within the packages. There is a git repository on Github . Contact the developers Contant interested free software developers directly: E-Mail: info \u00e4t opensemanticsearch.org Communicate to or work on the open source software project on Github: Open Semantic Search project on Github Architecture Software architecture: Components, services, modules and plugins Search or read data for additional applications If you use Solr as your database or index, use the Rest-API of Solr for searching or / and getting data in XML or JSON format or use Solr Client APIs to get data with your favorite programming language. Or if you use a triple store as database use its standard SPARQL interface. Libraries for different programming languages Libraries, bindings, and modules that exist for the explicit purpose of integrating Solr with other applications or programming languages Data enrichment and data analysis plugins Development of own Solr plugins Development of own data enrichment plugins Development of own data enrichment plugins with Python Roadmap Plans for the future .","title":"Development of free open source search engine software"},{"location":"dev/#development-of-free-open-source-search-engine-software","text":"","title":"Development of free open source search engine software"},{"location":"dev/#free-software","text":"Open Semantic Search is Open Source software based on Apache Lucene / Solr Enterprise Search and Python. So you are allowed to read, to change and to extend the source code.","title":"Free Software"},{"location":"dev/#source-code","text":"The source code is included within the packages. There is a git repository on Github .","title":"Source code"},{"location":"dev/#contact-the-developers","text":"Contant interested free software developers directly: E-Mail: info \u00e4t opensemanticsearch.org Communicate to or work on the open source software project on Github: Open Semantic Search project on Github","title":"Contact the developers"},{"location":"dev/#architecture","text":"Software architecture: Components, services, modules and plugins","title":"Architecture"},{"location":"dev/#search-or-read-data-for-additional-applications","text":"If you use Solr as your database or index, use the Rest-API of Solr for searching or / and getting data in XML or JSON format or use Solr Client APIs to get data with your favorite programming language. Or if you use a triple store as database use its standard SPARQL interface.","title":"Search or read data for additional applications"},{"location":"dev/#libraries-for-different-programming-languages","text":"Libraries, bindings, and modules that exist for the explicit purpose of integrating Solr with other applications or programming languages","title":"Libraries for different programming languages"},{"location":"dev/#data-enrichment-and-data-analysis-plugins","text":"Development of own Solr plugins Development of own data enrichment plugins Development of own data enrichment plugins with Python","title":"Data enrichment and data analysis plugins"},{"location":"dev/#roadmap","text":"Plans for the future .","title":"Roadmap"},{"location":"dev/enhancer/","text":"Developing own data analysis and data enrichment plugins There are different ways to enrich data with own enhancer plugins: Write your own data enrichment plugin for our connectors with a few lines of Python code for our Open Semantic ETL framework. Data enrichment web service or API: Develop or use a web service or REST-API for additional data or analytics and read the results with our RDF enhancer or call such a webservice in an own Python data enrichment plugin If you use Solr as your database or index, develop a Plugin for the Solr Update Processor (i.e. in Javascript or in Java) Write your own enhancer engine for Apache Stanbol in Java Write analysis or data enrichment in whatever programming language you use, read needed data from Solr, Elastic Search or SPARQL interface with programming libraries or tools of your choice and write your results into the index or database yourself, for example using the update mechanism (instead of an overwriting insert command) in Solr extending the existing document with additional values or fields / facets / properties instead of overwriting it with new values.","title":"Developing own data analysis and data enrichment plugins"},{"location":"dev/enhancer/#developing-own-data-analysis-and-data-enrichment-plugins","text":"There are different ways to enrich data with own enhancer plugins: Write your own data enrichment plugin for our connectors with a few lines of Python code for our Open Semantic ETL framework. Data enrichment web service or API: Develop or use a web service or REST-API for additional data or analytics and read the results with our RDF enhancer or call such a webservice in an own Python data enrichment plugin If you use Solr as your database or index, develop a Plugin for the Solr Update Processor (i.e. in Javascript or in Java) Write your own enhancer engine for Apache Stanbol in Java Write analysis or data enrichment in whatever programming language you use, read needed data from Solr, Elastic Search or SPARQL interface with programming libraries or tools of your choice and write your results into the index or database yourself, for example using the update mechanism (instead of an overwriting insert command) in Solr extending the existing document with additional values or fields / facets / properties instead of overwriting it with new values.","title":"Developing own data analysis and data enrichment plugins"},{"location":"dev/enhancer/python/","text":"How to develop your own Python plugin for data enrichment Example Just * add a new python file to the enhancer directory * enable this plugin in your connector and * implement the object myOwnEnhancerName the function process with the parameters parameters and data which * reads some parameters from variable parameters and * write to the variable data to index your results: Example: import etl class myOwnEnhancerName(object): def process ( parameters={}, data={} ): # do some analyses, i.e. with text uri\\_length = len ( parameters['id'] ) #write some analysis results to the index into a facet etl.append(data, facet='uri\\_length\\_i', values=uri\\_length) #return results return parameters, data Plugin directory Save your enhancer *myOwnEnhancerName*.py into the subdir etl of your in python library directory (i.e. /usr/lib/python2.7/). Config Add your enhancer to the enhancer chain in the config of your connector(s), which shall enhance their data with it: Enable your data enrichment plugin Add the plugin name to the config of the connector, for example in /etc/opensemanticsearch/connector-files : config['plugins'].append('*myOwnEnhancerName*') Custom config options for your plugin Optionally you can add your own config options, so you have more flexibility, because not everything has to be hard coded in your plugin Example config['myOwnEnhancerName'] = 'Own Enhancer' Your plugin can read this config options from the variable parameters Variables Read / set parameters from/to the variable parameters The variable parameters contains the config and some analysis results from enrichment plugins running before so you can use now as parameter for your plugin. For example you can run your plugin only for certain document types or mime types or run a language detection plugin before you use its result as parameter for a OCR plugin. Example data: print parameters { 'id: 'file:///documents/document.pdf', 'content_type': 'application/pdf' } Write results and values for indexing to the variable data The variable data is for adding facets and values to the index. To save and index your plugins results, just add them to the variable data . Example: data['tags'] = 'myTopic' data['location'] = 'Berlin' So the variable data will look like after running some enhancer plugins: print data { 'content_type': 'application/pdf', 'filesize': 12345, 'tags': {'Open Source', 'Free software'}, 'email_ss': { 'info@opensemanticsearch.org', 'support@opensemanticsearch.org' } } Functions Process Just implement the function process with the parameters parameters and data . If the plugin is enabled, this function will be called by the connector before indexing the document. ... def process ( parameters={}, data={} ): # do some analyses, i.e. with text textsize = parameters[text'].size #write some analysis results to the index into a facet opensemanticsearch\\_connector.append(data, facet='textsize', values=textsize) #return return parameters, data You can use all Python functions and libraries for very easy or very complex analysis. Append data Since if there are values for the same facet before because of another plugin running before and with the first example how to write resulrs to the variable data, you would overwrite existing data and for adding data easier, you can use the function etl.append to add additional values from your analysis results to the index: etl.append(data=data, facet = 'email', values = 'info@opensemanticsearch.org') The parameter values can be a value like a integer or a string or multiple values as a python list. Abort processing To abort further processing and final indexing you can set parameters['break'] = True So you can develop additional filters like for example the existing URL blacklist filter. Another example plugin using regular expressions Here an example, how to extract the email-adresses and store it to the facet email with just a few lines of python code to enable exploratory search, interactive filters and aggregated overviews for email-addresses, too. Its only an programming example, since extracting regular expressions is a standard plugin and to extract email-adresses its default config. # Data enrichment plugin for extracting email-adresses # Extracting email adresses and write them to facet email\\_ss # import the connector, so we can add our analysis to the indexed document import opensemanticsearch\\_connector # import python module for regular expressions import re class enhance\\_email(object): def process ( parameters={}, data={} ): # regular expression matching email-adresses regex = r'[\\w\\.-]+@[\\w\\.-]+' # facet / column where to store/index it facet = \"email\" # find all emailadresses with a regular expression matches = re.findall(regex, parameters['text']) if matches: # add the list matches to the facet opensemanticsearch\\_connector.append(data, facet, matches) return parameters, data","title":"How to develop your own Python plugin for data enrichment"},{"location":"dev/enhancer/python/#how-to-develop-your-own-python-plugin-for-data-enrichment","text":"","title":"How to develop your own Python plugin for data enrichment"},{"location":"dev/enhancer/python/#example","text":"Just * add a new python file to the enhancer directory * enable this plugin in your connector and * implement the object myOwnEnhancerName the function process with the parameters parameters and data which * reads some parameters from variable parameters and * write to the variable data to index your results: Example: import etl class myOwnEnhancerName(object): def process ( parameters={}, data={} ): # do some analyses, i.e. with text uri\\_length = len ( parameters['id'] ) #write some analysis results to the index into a facet etl.append(data, facet='uri\\_length\\_i', values=uri\\_length) #return results return parameters, data","title":"Example"},{"location":"dev/enhancer/python/#plugin-directory","text":"Save your enhancer *myOwnEnhancerName*.py into the subdir etl of your in python library directory (i.e. /usr/lib/python2.7/).","title":"Plugin directory"},{"location":"dev/enhancer/python/#config","text":"Add your enhancer to the enhancer chain in the config of your connector(s), which shall enhance their data with it:","title":"Config"},{"location":"dev/enhancer/python/#enable-your-data-enrichment-plugin","text":"Add the plugin name to the config of the connector, for example in /etc/opensemanticsearch/connector-files : config['plugins'].append('*myOwnEnhancerName*')","title":"Enable your data enrichment plugin"},{"location":"dev/enhancer/python/#custom-config-options-for-your-plugin","text":"Optionally you can add your own config options, so you have more flexibility, because not everything has to be hard coded in your plugin Example config['myOwnEnhancerName'] = 'Own Enhancer' Your plugin can read this config options from the variable parameters","title":"Custom config options for your plugin"},{"location":"dev/enhancer/python/#variables","text":"","title":"Variables"},{"location":"dev/enhancer/python/#read-set-parameters-fromto-the-variable-parameters","text":"The variable parameters contains the config and some analysis results from enrichment plugins running before so you can use now as parameter for your plugin. For example you can run your plugin only for certain document types or mime types or run a language detection plugin before you use its result as parameter for a OCR plugin. Example data: print parameters { 'id: 'file:///documents/document.pdf', 'content_type': 'application/pdf' }","title":"Read / set parameters from/to the variable parameters"},{"location":"dev/enhancer/python/#write-results-and-values-for-indexing-to-the-variable-data","text":"The variable data is for adding facets and values to the index. To save and index your plugins results, just add them to the variable data . Example: data['tags'] = 'myTopic' data['location'] = 'Berlin' So the variable data will look like after running some enhancer plugins: print data { 'content_type': 'application/pdf', 'filesize': 12345, 'tags': {'Open Source', 'Free software'}, 'email_ss': { 'info@opensemanticsearch.org', 'support@opensemanticsearch.org' } }","title":"Write results and values for indexing to the variable data"},{"location":"dev/enhancer/python/#functions","text":"","title":"Functions"},{"location":"dev/enhancer/python/#process","text":"Just implement the function process with the parameters parameters and data . If the plugin is enabled, this function will be called by the connector before indexing the document. ... def process ( parameters={}, data={} ): # do some analyses, i.e. with text textsize = parameters[text'].size #write some analysis results to the index into a facet opensemanticsearch\\_connector.append(data, facet='textsize', values=textsize) #return return parameters, data You can use all Python functions and libraries for very easy or very complex analysis.","title":"Process"},{"location":"dev/enhancer/python/#append-data","text":"Since if there are values for the same facet before because of another plugin running before and with the first example how to write resulrs to the variable data, you would overwrite existing data and for adding data easier, you can use the function etl.append to add additional values from your analysis results to the index: etl.append(data=data, facet = 'email', values = 'info@opensemanticsearch.org') The parameter values can be a value like a integer or a string or multiple values as a python list.","title":"Append data"},{"location":"dev/enhancer/python/#abort-processing","text":"To abort further processing and final indexing you can set parameters['break'] = True So you can develop additional filters like for example the existing URL blacklist filter.","title":"Abort processing"},{"location":"dev/enhancer/python/#another-example-plugin-using-regular-expressions","text":"Here an example, how to extract the email-adresses and store it to the facet email with just a few lines of python code to enable exploratory search, interactive filters and aggregated overviews for email-addresses, too. Its only an programming example, since extracting regular expressions is a standard plugin and to extract email-adresses its default config. # Data enrichment plugin for extracting email-adresses # Extracting email adresses and write them to facet email\\_ss # import the connector, so we can add our analysis to the indexed document import opensemanticsearch\\_connector # import python module for regular expressions import re class enhance\\_email(object): def process ( parameters={}, data={} ): # regular expression matching email-adresses regex = r'[\\w\\.-]+@[\\w\\.-]+' # facet / column where to store/index it facet = \"email\" # find all emailadresses with a regular expression matches = re.findall(regex, parameters['text']) if matches: # add the list matches to the facet opensemanticsearch\\_connector.append(data, facet, matches) return parameters, data","title":"Another example plugin using regular expressions"},{"location":"doc/","text":"Documentation Documentation for users Usage How to search and how to sort, explore and filter search results Search operators Tagging and annotation Adding structure for exploratory search, aggregated overviews and interactive filters with facets and named entities (Fuzzy) Search for many entries with a list Documentation for admins Getting started Quick start tutorial: How to getting started Installation How to install the search engine Configuration Configuration of the search engine Administration Administration of the search engine Documentation for admins and developers Modules Architecture and modules","title":"Documentation"},{"location":"doc/#documentation","text":"","title":"Documentation"},{"location":"doc/#documentation-for-users","text":"","title":"Documentation for users"},{"location":"doc/#usage","text":"How to search and how to sort, explore and filter search results Search operators Tagging and annotation Adding structure for exploratory search, aggregated overviews and interactive filters with facets and named entities (Fuzzy) Search for many entries with a list","title":"Usage"},{"location":"doc/#documentation-for-admins","text":"","title":"Documentation for admins"},{"location":"doc/#getting-started","text":"Quick start tutorial: How to getting started","title":"Getting started"},{"location":"doc/#installation","text":"How to install the search engine","title":"Installation"},{"location":"doc/#configuration","text":"Configuration of the search engine","title":"Configuration"},{"location":"doc/#administration","text":"Administration of the search engine","title":"Administration"},{"location":"doc/#documentation-for-admins-and-developers","text":"","title":"Documentation for admins and developers"},{"location":"doc/#modules","text":"Architecture and modules","title":"Modules"},{"location":"doc/admin/","text":"Administration Ingestion and indexing You can start import and indexing tasks via web admin interface in the menu \" Datasources \" via REST-API or via command line interface (CLI) Logging Where you find logs: Logfiles Task queue How to manage and monitor running imports, crawls and tasks: Task queue management and monitoring Data storage Where you find stored data: Data storage","title":"Administration"},{"location":"doc/admin/#administration","text":"","title":"Administration"},{"location":"doc/admin/#ingestion-and-indexing","text":"You can start import and indexing tasks via web admin interface in the menu \" Datasources \" via REST-API or via command line interface (CLI)","title":"Ingestion and indexing"},{"location":"doc/admin/#logging","text":"Where you find logs: Logfiles","title":"Logging"},{"location":"doc/admin/#task-queue","text":"How to manage and monitor running imports, crawls and tasks: Task queue management and monitoring","title":"Task queue"},{"location":"doc/admin/#data-storage","text":"Where you find stored data: Data storage","title":"Data storage"},{"location":"doc/admin/client/","text":"Client integration Browser integration To spare some seconds to call the search engine bookmark to get the search interface you can easily integrate the search with the web browser search field (i.e. Mozilla Firefox, Internet Explorer, Chrome or Opera) via OpenSearch standard. Desktop integrated search If you use Unity or Gnome as your Linux Desktop, you can integrate search results to the search lenses, so can search pressing just a key anf fully integrated with your desktop instead of having to start a client or browser and to call the URL. Filemanager If you don't use filesystem monitoring for indexing all changes automatically, it is possible to integrate to start indexing files easily and comfortable with the context menu of your standard filemanager (i.e. Explorer in Windows or Nautilus in Debian or Ubuntu Linux)","title":"Client integration"},{"location":"doc/admin/client/#client-integration","text":"","title":"Client integration"},{"location":"doc/admin/client/#browser-integration","text":"To spare some seconds to call the search engine bookmark to get the search interface you can easily integrate the search with the web browser search field (i.e. Mozilla Firefox, Internet Explorer, Chrome or Opera) via OpenSearch standard.","title":"Browser integration"},{"location":"doc/admin/client/#desktop-integrated-search","text":"If you use Unity or Gnome as your Linux Desktop, you can integrate search results to the search lenses, so can search pressing just a key anf fully integrated with your desktop instead of having to start a client or browser and to call the URL.","title":"Desktop integrated search"},{"location":"doc/admin/client/#filemanager","text":"If you don't use filesystem monitoring for indexing all changes automatically, it is possible to integrate to start indexing files easily and comfortable with the context menu of your standard filemanager (i.e. Explorer in Windows or Nautilus in Debian or Ubuntu Linux)","title":"Filemanager"},{"location":"doc/admin/cmd/","text":"Command line interfaces (CLI) Administration of Open Semantic Search via command line interface (CLI): Task queue Command line interfaces for managing tasks: Task queue management Indexing files Index a file opensemanticsearch-index-file filename Index directories opensemanticsearch-index-dir directoryname Monitor files and directories Index changed files in realtime (no expensive recrawl needed) opensemanticsearch-filemonitoring filename Or monitor and index all new and changed files in a directory recursive (including subdirectories) opensemanticsearch-filemonitoring directorynamename Monitor directories automatically If you installed the module opensemanticsearch-trigger-filemonitoring-daemon it will start automatically while booting and monitor all directories and files set in the config file /etc/opensemanticsearch/filemonitoring . Indexing Newsfeeds RSS-Newsfeeds Index full feed from RSS (download and index all articles linked in a RSS-Newsfeed). opensemanticsearch-index-rss http://www.opensemanticsearch.org/feed Delete from index The delete tool will only delete data in the index once, so after new indexing or recrawl the deleted documents will be indexed again. If you want to exclude documents from indexing, you should use blacklisting instead. Delete a web page from index opensemanticsearch-delete http://www.opensemanticsearch.org/ Delete a file from index opensemanticsearch-delete file:///directory/file Empty the index Empty the index to get a new or empty index without indexed documents: opensemanticsearch-delete --empty Start or stop services If you installed the Debian or Ubuntu packages, the services are started while booting automatically. They can be controlled via the linux command service: Solr index service Start: service solr start Stop: service solr stop","title":"Command line interface (CLI)"},{"location":"doc/admin/cmd/#command-line-interfaces-cli","text":"Administration of Open Semantic Search via command line interface (CLI):","title":"Command line interfaces (CLI)"},{"location":"doc/admin/cmd/#task-queue","text":"Command line interfaces for managing tasks: Task queue management","title":"Task queue"},{"location":"doc/admin/cmd/#indexing-files","text":"","title":"Indexing files"},{"location":"doc/admin/cmd/#index-a-file","text":"opensemanticsearch-index-file filename","title":"Index a file"},{"location":"doc/admin/cmd/#index-directories","text":"opensemanticsearch-index-dir directoryname","title":"Index directories"},{"location":"doc/admin/cmd/#monitor-files-and-directories","text":"Index changed files in realtime (no expensive recrawl needed) opensemanticsearch-filemonitoring filename Or monitor and index all new and changed files in a directory recursive (including subdirectories) opensemanticsearch-filemonitoring directorynamename","title":"Monitor files and directories"},{"location":"doc/admin/cmd/#monitor-directories-automatically","text":"If you installed the module opensemanticsearch-trigger-filemonitoring-daemon it will start automatically while booting and monitor all directories and files set in the config file /etc/opensemanticsearch/filemonitoring .","title":"Monitor directories automatically"},{"location":"doc/admin/cmd/#indexing-newsfeeds","text":"","title":"Indexing Newsfeeds"},{"location":"doc/admin/cmd/#rss-newsfeeds","text":"Index full feed from RSS (download and index all articles linked in a RSS-Newsfeed). opensemanticsearch-index-rss http://www.opensemanticsearch.org/feed","title":"RSS-Newsfeeds"},{"location":"doc/admin/cmd/#delete-from-index","text":"The delete tool will only delete data in the index once, so after new indexing or recrawl the deleted documents will be indexed again. If you want to exclude documents from indexing, you should use blacklisting instead.","title":"Delete from index"},{"location":"doc/admin/cmd/#delete-a-web-page-from-index","text":"opensemanticsearch-delete http://www.opensemanticsearch.org/","title":"Delete a web page from index"},{"location":"doc/admin/cmd/#delete-a-file-from-index","text":"opensemanticsearch-delete file:///directory/file","title":"Delete a file from index"},{"location":"doc/admin/cmd/#empty-the-index","text":"Empty the index to get a new or empty index without indexed documents: opensemanticsearch-delete --empty","title":"Empty the index"},{"location":"doc/admin/cmd/#start-or-stop-services","text":"If you installed the Debian or Ubuntu packages, the services are started while booting automatically. They can be controlled via the linux command service:","title":"Start or stop services"},{"location":"doc/admin/cmd/#solr-index-service","text":"Start: service solr start Stop: service solr stop","title":"Solr index service"},{"location":"doc/admin/config/","text":"Config Standard installations and most parts should be usable out of the box without further configuration. Many config options are set by a web user interface in the menu Configuration : So in most cases you don't have to configure complex (partially outdated) config file options described below: Search server Language of the index If your language is not english and you don't use a localized package or virtual machine: Most important because not changeable for yet indexed documents is to change the language for stemming before indexing documents. User interface Config file The config file of the user interface is /etc/solr-php-ui/config.php Language of the user interface The default language is english. To switch the language of the user interface to german set the option $language to de # Set language to german $language = \"de\"; Scheduler: Starting (re)crawls automatically If you don't use filemonitoring (and even then you should sometimes recrawl, if something failed or was changed at a bad moment), you should recrawl data sources from time to time automatically. If you use our connectors and want most flexibility use Cron and add a Cronjob using our command line tools into a Crontab or you call our webservice (REST-API) from a script or another webservice (i.e. webcron). File indexer Config file The config file for indexing files is /etc/opensemanticsearch/connector-files . There you can enable OCR for images, like described below: Data enrichment: Enrich content Enhancers enrich the content with additional data, metadata or analytics (i.e. tags, OCR or named entity extraction), which helps to find better the original content, to filter or to navigate. OCR (automatic text recognition in graphical formats) Automatic text recognition (OCR) is off by default, because it slows down indexing. It uses many processor ressources and will need many seconds for each graphic file. Enable OCR (OCR is enabled by default in the virtual machine packages like Open Semantic Desktop Search or Open Semantic Search Appliance) Install the package tesseract-ocr (included in your Linux distribution): apt-get install tesseract-ocr If you enabled OCR, should enable OCR for images inside PDF files, too, since many PDF files are scans and do contain much text data only as graphics: Add (uncomment) the PDF OCR Plugin:: #Enable OCR for images inside PDF files config['plugins'].append('enhance_pdf_ocr') OCR language Setting OCR language to an other language than english: 1. Install the tesseract language package (for german: tesseract-ocr-deu ). See the list of available languages for Debian or Ubuntu . 2. set option ocr_language to the language of your documents. Default is eng for english (in tesseract its eng , not en !). For german set deu (in tesseract its not de !): `# language for automatic text recognition (ocr) config['ocr_lang'] = \"eng\" config['ocr_lang'] = \"deu\"` Or set the OCR language to multiple languages, which are used in your documents: # language for automatic text recognition (ocr) config['ocr_lang'] = \"eng+deu\" Enrich with metadata from RDF sources (Resource Desciption Framework) In /etc/opensemanticsearch/enhancer-rdf you can configure servers or services for metadata (like annotations or tagging) which is accessable as open standard RDF (Ressource Description Framework) and map them to Solr fields or facets. Adding custom fields / custom facets To be able to use external, independent and modular tools and components writing directly to the Solr index, there is more than one place to configure mappings of new fields: * Your metadata plattform (i.e. Drupal ) where you edit them or a scraping plattform where you read them saving the data in fields with fieldnames * the connectors / importers and enhancers which read this data from the (meta) data source and write it to Solr * and the user interface which reads this fields from Solr and shows them under a human readable label to the user. You can configure additional facets (interactive filters): Mapping from database fields or RDF properties to custom fields / custom facets in Solr within the connector / importer / enhancer Config in which Solr-fields to write the additional data from the (meta)datasource: If you use a RDF datasource, find out the name / URI (in semantic Web and RDF the \"field name\" is an url, too) of the external fields (i.e. a standard fields (like Dublin core metadata standard fields) or a custom field in Drupal ). Add this external fieldnames or uris mapping them to a internal Solr field (standard fields, standard facets or additional custom facets ) in /etc/solr/enancer-rdf . Example: config['meta_property2facet'] = { 'http://purl.org/dc/terms/location': 'location_ss', 'http://semantic-mediawiki.org/swivt/1.0#specialProperty_dat': 'meta_date_dts' } Enable additional Solr custom fields / custom facets in the user interface Config which of this new or additional Solr-fields should be shown in the user interface and under which caption: Use the option $cfg[facets] in config/config.php to add custom facets in the user interface: Setup additional Solr-fields (i.e. filled from below configurated connectors / importers or enhancers like additional RDF Metadata sources (i.e. your tagging and anntoation metadata server) or fields in which you write scraped data ) and map them a human readable title / label. Example: // Additional facets (f.e. fields imported by a connector or enhancer which should be shown as interactive filter in the sidebar) $cfg['facets']['yourfacet_s'] = array ('label'=>'Additional own facet'); $cfg['facets']['anotherfacet_s'] = array ('label'=>'Another additonal facet');","title":"Configuration"},{"location":"doc/admin/config/#config","text":"Standard installations and most parts should be usable out of the box without further configuration. Many config options are set by a web user interface in the menu Configuration : So in most cases you don't have to configure complex (partially outdated) config file options described below:","title":"Config"},{"location":"doc/admin/config/#search-server","text":"","title":"Search server"},{"location":"doc/admin/config/#language-of-the-index","text":"If your language is not english and you don't use a localized package or virtual machine: Most important because not changeable for yet indexed documents is to change the language for stemming before indexing documents.","title":"Language of the index"},{"location":"doc/admin/config/#user-interface","text":"","title":"User interface"},{"location":"doc/admin/config/#config-file","text":"The config file of the user interface is /etc/solr-php-ui/config.php","title":"Config file"},{"location":"doc/admin/config/#language-of-the-user-interface","text":"The default language is english. To switch the language of the user interface to german set the option $language to de # Set language to german $language = \"de\";","title":"Language of the user interface"},{"location":"doc/admin/config/#scheduler-starting-recrawls-automatically","text":"If you don't use filemonitoring (and even then you should sometimes recrawl, if something failed or was changed at a bad moment), you should recrawl data sources from time to time automatically. If you use our connectors and want most flexibility use Cron and add a Cronjob using our command line tools into a Crontab or you call our webservice (REST-API) from a script or another webservice (i.e. webcron).","title":"Scheduler: Starting (re)crawls automatically"},{"location":"doc/admin/config/#file-indexer","text":"","title":"File indexer"},{"location":"doc/admin/config/#config-file_1","text":"The config file for indexing files is /etc/opensemanticsearch/connector-files . There you can enable OCR for images, like described below:","title":"Config file"},{"location":"doc/admin/config/#data-enrichment-enrich-content","text":"Enhancers enrich the content with additional data, metadata or analytics (i.e. tags, OCR or named entity extraction), which helps to find better the original content, to filter or to navigate.","title":"Data enrichment: Enrich content"},{"location":"doc/admin/config/#ocr-automatic-text-recognition-in-graphical-formats","text":"Automatic text recognition (OCR) is off by default, because it slows down indexing. It uses many processor ressources and will need many seconds for each graphic file.","title":"OCR (automatic text recognition in graphical formats)"},{"location":"doc/admin/config/#enable-ocr","text":"(OCR is enabled by default in the virtual machine packages like Open Semantic Desktop Search or Open Semantic Search Appliance) Install the package tesseract-ocr (included in your Linux distribution): apt-get install tesseract-ocr If you enabled OCR, should enable OCR for images inside PDF files, too, since many PDF files are scans and do contain much text data only as graphics: Add (uncomment) the PDF OCR Plugin:: #Enable OCR for images inside PDF files config['plugins'].append('enhance_pdf_ocr')","title":"Enable OCR"},{"location":"doc/admin/config/#ocr-language","text":"Setting OCR language to an other language than english: 1. Install the tesseract language package (for german: tesseract-ocr-deu ). See the list of available languages for Debian or Ubuntu . 2. set option ocr_language to the language of your documents. Default is eng for english (in tesseract its eng , not en !). For german set deu (in tesseract its not de !): `# language for automatic text recognition (ocr)","title":"OCR language"},{"location":"doc/admin/config/#configocr_lang-eng","text":"config['ocr_lang'] = \"deu\"` Or set the OCR language to multiple languages, which are used in your documents: # language for automatic text recognition (ocr) config['ocr_lang'] = \"eng+deu\"","title":"config['ocr_lang'] = \"eng\""},{"location":"doc/admin/config/#enrich-with-metadata-from-rdf-sources-resource-desciption-framework","text":"In /etc/opensemanticsearch/enhancer-rdf you can configure servers or services for metadata (like annotations or tagging) which is accessable as open standard RDF (Ressource Description Framework) and map them to Solr fields or facets.","title":"Enrich with metadata from RDF sources (Resource Desciption Framework)"},{"location":"doc/admin/config/#adding-custom-fields-custom-facets","text":"To be able to use external, independent and modular tools and components writing directly to the Solr index, there is more than one place to configure mappings of new fields: * Your metadata plattform (i.e. Drupal ) where you edit them or a scraping plattform where you read them saving the data in fields with fieldnames * the connectors / importers and enhancers which read this data from the (meta) data source and write it to Solr * and the user interface which reads this fields from Solr and shows them under a human readable label to the user. You can configure additional facets (interactive filters):","title":"Adding custom fields / custom facets"},{"location":"doc/admin/config/#mapping-from-database-fields-or-rdf-properties-to-custom-fields-custom-facets-in-solr-within-the-connector-importer-enhancer","text":"Config in which Solr-fields to write the additional data from the (meta)datasource: If you use a RDF datasource, find out the name / URI (in semantic Web and RDF the \"field name\" is an url, too) of the external fields (i.e. a standard fields (like Dublin core metadata standard fields) or a custom field in Drupal ). Add this external fieldnames or uris mapping them to a internal Solr field (standard fields, standard facets or additional custom facets ) in /etc/solr/enancer-rdf . Example: config['meta_property2facet'] = { 'http://purl.org/dc/terms/location': 'location_ss', 'http://semantic-mediawiki.org/swivt/1.0#specialProperty_dat': 'meta_date_dts' }","title":"Mapping from database fields or RDF properties to custom fields / custom facets in Solr within the connector / importer / enhancer"},{"location":"doc/admin/config/#enable-additional-solr-custom-fields-custom-facets-in-the-user-interface","text":"Config which of this new or additional Solr-fields should be shown in the user interface and under which caption: Use the option $cfg[facets] in config/config.php to add custom facets in the user interface: Setup additional Solr-fields (i.e. filled from below configurated connectors / importers or enhancers like additional RDF Metadata sources (i.e. your tagging and anntoation metadata server) or fields in which you write scraped data ) and map them a human readable title / label. Example: // Additional facets (f.e. fields imported by a connector or enhancer which should be shown as interactive filter in the sidebar) $cfg['facets']['yourfacet_s'] = array ('label'=>'Additional own facet'); $cfg['facets']['anotherfacet_s'] = array ('label'=>'Another additonal facet');","title":"Enable additional Solr custom fields / custom facets in the user interface"},{"location":"doc/admin/config/blacklist/","text":"Blacklists and whitelists Exclude URLs, domains, paths, files, file types or plugins from indexing to Solr or Elastic Search Entries in a blacklist or matching a pattern like prefixes (f.e. domains or file directories) or suffixes (f.e. a file suffix like \".css\") wont be indexed for search. So you can exclude spam or not relevant formats like CSS files. For example you might want not to index stylesheets (if you are not a web developer who might want to search for them). Blacklist format: One entry per line A blacklist is a plain text file containing one blacklist entry per line. There are different blacklists for different forms of blacklisting: Exclude webpages or filenames by URLs To exclude a webpage or a file just add the full URL or full filename to the default URL-Blacklist /etc/opensemanticsearch/blacklist/blacklist-url Exclude domains or paths by prefixes To exclude whole domains, subdomains, paths or subpaths, just add the domain or path to the default prefix-Blacklist /etc/opensemanticsearch/blacklist/blacklist-url-prefix Exclude file extensions by suffixes To exclude suffixes like some file extensions, just add the suffix like .css to the default suffix-Blacklist /etc/opensemanticsearch/blacklist/blacklist-url-suffix Exclude URIs matching text patterns by regular expressions To exclude all URIs matching a text pattern (regular expression) just add the regular expression to the default regex-Blacklist /etc/opensemanticsearch/blacklist/blacklist-url-regex Whitelist: Include even if matching a blacklisted pattern URLs in a whitelist will be indexed, even if blacklisted because matching a blacklisted domain, prefix, suffix or pattern. The configuration works the same way in config files /etc/opensemanticsearch/blacklist/whitelist-url and so on ... Configure additional blacklists (for divided or shared blacklists) If you want to use multiple blacklists for easier management of many blacklisted entries or to share blacklists or parts of your blacklists with other people, organizations or a closed or open data community, you can use and add more than the default blacklists: To add another blacklist of URLs or filenames just add to the config in /etc/opensemanticsearch/etl or in the config of the special connector config['blacklist'].append('/etc/opensemanticsearch/blacklist/blacklist-url-anOwnList') To add another blacklist of Domains or directories just add to the config in /etc/opensemanticsearch/etl or in the config of the special connector config['blacklist_prefix'].append('/etc/opensemanticsearch/blacklist/blacklist-url-prefix-anOwnList') To add another blacklist of file endings just add to the config in /etc/opensemanticsearch/etl or in the config of the special connector config['blacklist_suffix'].append('/etc/opensemanticsearch/blacklist/blacklist-url-suffix-anOwnList') To add another blacklist of regular expressions just add to the config in /etc/opensemanticsearch/etl or in the config of the special connector config['blacklist_regex'].append('/etc/opensemanticsearch/blacklist/blacklist-url-suffix-anOwnList') Blacklisting on plugin level Blacklists can be configured only for certain plugins. So blacklisting documents, paths, file endings or content types does only affect certain plugins or data enrichment or data analysis or document processing chain steps. So the blacklisted documents, paths, file endings or content types will be processed by all other plugins and indexed, but they would not be processed, enriched or analysed by the defined plugin. These config files work the same way like the standard blacklist config file types described before. So you can blacklist some files (URLs), paths (URL-prefixes) or file endings (URL suffixes). But there only active for the defined plugin by the path name inside the config path /etc/opensemanticsearch/blacklist , which has to match the plugin file name. Blacklisting URLs, file names (URLs), paths (URL prefixes), file endings (URL suffixes) on plugin level /etc/opensemanticsearch/blacklist/pluginname/blacklist-url /etc/opensemanticsearch/blacklist/pluginname/blacklist-url-prefix /etc/opensemanticsearch/blacklist/pluginname/blacklist-url-suffix /etc/opensemanticsearch/blacklist/pluginname/blacklist-url-regex /etc/opensemanticsearch/blacklist/pluginname/whitelist-url /etc/opensemanticsearch/blacklist/pluginname/whitelist-url-prefix /etc/opensemanticsearch/blacklist/pluginname/whitelist-url-suffix /etc/opensemanticsearch/blacklist/pluginname/whitelist-url-regex Blacklist content types on plugin level Important: This means not file endings (which would be url-suffix) but the content type guessed by Tikas document analysis! * /etc/opensemanticsearch/blacklist/pluginname/blacklist-contenttype * /etc/opensemanticsearch/blacklist/pluginname/blacklist-contenttype-prefix * /etc/opensemanticsearch/blacklist/pluginname/blacklist-contenttype-suffix * /etc/opensemanticsearch/blacklist/pluginname/blacklist-contenttype-regex * /etc/opensemanticsearch/blacklist/pluginname/whitelist-contenttype * /etc/opensemanticsearch/blacklist/pluginname/whitelist-contenttype-prefix * /etc/opensemanticsearch/blacklist/pluginname/whitelist-contenttype-suffix * /etc/opensemanticsearch/blacklist/pluginname/whitelist-contenttype-regex Example For example if you don't want to handle Open Document files which are extracted with Tika yet additionally as ZIP archive which it technically is and which would be extracted to index each file inside this ZIP archive, you can blacklist this content type for the Unzip plugin: Therefore add to the config in /etc/opensemanticsearch/blacklist/enhance_zip/blacklist-contenttype-prefix application/vnd.oasis.opendocument. Another possibility would be to blacklist the .odt suffix for the ZIP plugin enhance_zip : Therefore add to the config in /etc/opensemanticsearch/blacklist/enhance_zip/blacklist-url-suffix .odt .ODT .Odt Since this is preconfigured in the default setup and config file, you dont have to do add this manually, but it is an example how more complex blacklisting on plugin level works.","title":"Blacklists and whitelists"},{"location":"doc/admin/config/blacklist/#blacklists-and-whitelists","text":"","title":"Blacklists and whitelists"},{"location":"doc/admin/config/blacklist/#exclude-urls-domains-paths-files-file-types-or-plugins-from-indexing-to-solr-or-elastic-search","text":"Entries in a blacklist or matching a pattern like prefixes (f.e. domains or file directories) or suffixes (f.e. a file suffix like \".css\") wont be indexed for search. So you can exclude spam or not relevant formats like CSS files. For example you might want not to index stylesheets (if you are not a web developer who might want to search for them).","title":"Exclude URLs, domains, paths, files, file types or plugins from indexing to Solr or Elastic Search"},{"location":"doc/admin/config/blacklist/#blacklist-format-one-entry-per-line","text":"A blacklist is a plain text file containing one blacklist entry per line. There are different blacklists for different forms of blacklisting:","title":"Blacklist format: One entry per line"},{"location":"doc/admin/config/blacklist/#exclude-webpages-or-filenames-by-urls","text":"To exclude a webpage or a file just add the full URL or full filename to the default URL-Blacklist /etc/opensemanticsearch/blacklist/blacklist-url","title":"Exclude webpages or filenames by URLs"},{"location":"doc/admin/config/blacklist/#exclude-domains-or-paths-by-prefixes","text":"To exclude whole domains, subdomains, paths or subpaths, just add the domain or path to the default prefix-Blacklist /etc/opensemanticsearch/blacklist/blacklist-url-prefix","title":"Exclude domains or paths by prefixes"},{"location":"doc/admin/config/blacklist/#exclude-file-extensions-by-suffixes","text":"To exclude suffixes like some file extensions, just add the suffix like .css to the default suffix-Blacklist /etc/opensemanticsearch/blacklist/blacklist-url-suffix","title":"Exclude file extensions by suffixes"},{"location":"doc/admin/config/blacklist/#exclude-uris-matching-text-patterns-by-regular-expressions","text":"To exclude all URIs matching a text pattern (regular expression) just add the regular expression to the default regex-Blacklist /etc/opensemanticsearch/blacklist/blacklist-url-regex","title":"Exclude URIs matching text patterns by regular expressions"},{"location":"doc/admin/config/blacklist/#whitelist-include-even-if-matching-a-blacklisted-pattern","text":"URLs in a whitelist will be indexed, even if blacklisted because matching a blacklisted domain, prefix, suffix or pattern. The configuration works the same way in config files /etc/opensemanticsearch/blacklist/whitelist-url and so on ...","title":"Whitelist: Include even if matching a blacklisted pattern"},{"location":"doc/admin/config/blacklist/#configure-additional-blacklists-for-divided-or-shared-blacklists","text":"If you want to use multiple blacklists for easier management of many blacklisted entries or to share blacklists or parts of your blacklists with other people, organizations or a closed or open data community, you can use and add more than the default blacklists: To add another blacklist of URLs or filenames just add to the config in /etc/opensemanticsearch/etl or in the config of the special connector config['blacklist'].append('/etc/opensemanticsearch/blacklist/blacklist-url-anOwnList') To add another blacklist of Domains or directories just add to the config in /etc/opensemanticsearch/etl or in the config of the special connector config['blacklist_prefix'].append('/etc/opensemanticsearch/blacklist/blacklist-url-prefix-anOwnList') To add another blacklist of file endings just add to the config in /etc/opensemanticsearch/etl or in the config of the special connector config['blacklist_suffix'].append('/etc/opensemanticsearch/blacklist/blacklist-url-suffix-anOwnList') To add another blacklist of regular expressions just add to the config in /etc/opensemanticsearch/etl or in the config of the special connector config['blacklist_regex'].append('/etc/opensemanticsearch/blacklist/blacklist-url-suffix-anOwnList')","title":"Configure additional blacklists (for divided or shared blacklists)"},{"location":"doc/admin/config/blacklist/#blacklisting-on-plugin-level","text":"Blacklists can be configured only for certain plugins. So blacklisting documents, paths, file endings or content types does only affect certain plugins or data enrichment or data analysis or document processing chain steps. So the blacklisted documents, paths, file endings or content types will be processed by all other plugins and indexed, but they would not be processed, enriched or analysed by the defined plugin. These config files work the same way like the standard blacklist config file types described before. So you can blacklist some files (URLs), paths (URL-prefixes) or file endings (URL suffixes). But there only active for the defined plugin by the path name inside the config path /etc/opensemanticsearch/blacklist , which has to match the plugin file name.","title":"Blacklisting on plugin level"},{"location":"doc/admin/config/blacklist/#blacklisting-urls-file-names-urls-paths-url-prefixes-file-endings-url-suffixes-on-plugin-level","text":"/etc/opensemanticsearch/blacklist/pluginname/blacklist-url /etc/opensemanticsearch/blacklist/pluginname/blacklist-url-prefix /etc/opensemanticsearch/blacklist/pluginname/blacklist-url-suffix /etc/opensemanticsearch/blacklist/pluginname/blacklist-url-regex /etc/opensemanticsearch/blacklist/pluginname/whitelist-url /etc/opensemanticsearch/blacklist/pluginname/whitelist-url-prefix /etc/opensemanticsearch/blacklist/pluginname/whitelist-url-suffix /etc/opensemanticsearch/blacklist/pluginname/whitelist-url-regex","title":"Blacklisting URLs, file names (URLs), paths (URL prefixes), file endings (URL suffixes) on plugin level"},{"location":"doc/admin/config/blacklist/#blacklist-content-types-on-plugin-level","text":"Important: This means not file endings (which would be url-suffix) but the content type guessed by Tikas document analysis! * /etc/opensemanticsearch/blacklist/pluginname/blacklist-contenttype * /etc/opensemanticsearch/blacklist/pluginname/blacklist-contenttype-prefix * /etc/opensemanticsearch/blacklist/pluginname/blacklist-contenttype-suffix * /etc/opensemanticsearch/blacklist/pluginname/blacklist-contenttype-regex * /etc/opensemanticsearch/blacklist/pluginname/whitelist-contenttype * /etc/opensemanticsearch/blacklist/pluginname/whitelist-contenttype-prefix * /etc/opensemanticsearch/blacklist/pluginname/whitelist-contenttype-suffix * /etc/opensemanticsearch/blacklist/pluginname/whitelist-contenttype-regex","title":"Blacklist content types on plugin level"},{"location":"doc/admin/config/blacklist/#example","text":"For example if you don't want to handle Open Document files which are extracted with Tika yet additionally as ZIP archive which it technically is and which would be extracted to index each file inside this ZIP archive, you can blacklist this content type for the Unzip plugin: Therefore add to the config in /etc/opensemanticsearch/blacklist/enhance_zip/blacklist-contenttype-prefix application/vnd.oasis.opendocument. Another possibility would be to blacklist the .odt suffix for the ZIP plugin enhance_zip : Therefore add to the config in /etc/opensemanticsearch/blacklist/enhance_zip/blacklist-url-suffix .odt .ODT .Odt Since this is preconfigured in the default setup and config file, you dont have to do add this manually, but it is an example how more complex blacklisting on plugin level works.","title":"Example"},{"location":"doc/admin/config/log/","text":"Logfiles Locations of the log files: Webserver Logfile of your Apache webserver for PHP based Search UI, Django based Open Semantic Search apps and some HTTP/REST APIs: /var/log/apache2/access.log If something went wrong you find more info in /var/log/apache2/error.log . Open Semantic ETL Full logs are written with prefix etl_task to the logfile /var/log/syslog The last log entries of Open Semantic ETL can be seen by command service opensemanticetl status Apache Tika Logs of the main component Apache Tika for text extraction from files are written as java to the logfile /var/log/syslog The last log entries of Apache Tika can be seen by command service tika status and service tika-fake-ocr status Apache Solr search server Logfile of Apache Solr: /var/solr/logs/solr.log Solr admin web interface You can use the Solr admin interface ( http://localhost:8983/solr/ ) by using the menu option \"Logging\" ( http://localhost:8983/solr/#/~logging ) to see the last error messages. Verbosity Open Semantic ETL To be verbose while crawling, text extraction and data analysis, set the config option config['verbose'] = True in the config file /etc/opensemanicsearch/etl or in the special config file of a connector to log more info. Solr PHP UI Config cfg['debug'] = true; in the config file of the User interface to see, what was communicated to and from Solr.","title":"Logs"},{"location":"doc/admin/config/log/#logfiles","text":"Locations of the log files:","title":"Logfiles"},{"location":"doc/admin/config/log/#webserver","text":"Logfile of your Apache webserver for PHP based Search UI, Django based Open Semantic Search apps and some HTTP/REST APIs: /var/log/apache2/access.log If something went wrong you find more info in /var/log/apache2/error.log .","title":"Webserver"},{"location":"doc/admin/config/log/#open-semantic-etl","text":"Full logs are written with prefix etl_task to the logfile /var/log/syslog The last log entries of Open Semantic ETL can be seen by command service opensemanticetl status","title":"Open Semantic ETL"},{"location":"doc/admin/config/log/#apache-tika","text":"Logs of the main component Apache Tika for text extraction from files are written as java to the logfile /var/log/syslog The last log entries of Apache Tika can be seen by command service tika status and service tika-fake-ocr status","title":"Apache Tika"},{"location":"doc/admin/config/log/#apache-solr-search-server","text":"Logfile of Apache Solr: /var/solr/logs/solr.log","title":"Apache Solr search server"},{"location":"doc/admin/config/log/#solr-admin-web-interface","text":"You can use the Solr admin interface ( http://localhost:8983/solr/ ) by using the menu option \"Logging\" ( http://localhost:8983/solr/#/~logging ) to see the last error messages.","title":"Solr admin web interface"},{"location":"doc/admin/config/log/#verbosity","text":"","title":"Verbosity"},{"location":"doc/admin/config/log/#open-semantic-etl_1","text":"To be verbose while crawling, text extraction and data analysis, set the config option config['verbose'] = True in the config file /etc/opensemanicsearch/etl or in the special config file of a connector to log more info.","title":"Open Semantic ETL"},{"location":"doc/admin/config/log/#solr-php-ui","text":"Config cfg['debug'] = true; in the config file of the User interface to see, what was communicated to and from Solr.","title":"Solr PHP UI"},{"location":"doc/admin/config/named_entity_recognition/","text":"Named Entity Recogniton (NER) Automatic Named Entity Recognition by machine learning (ML) for automatic classification and annotation of text parts There are models trained with existing annotations of a large text corpus, so after that they can predict / guess if a part of a sentence is a name of a person, a name of an organization, a verb or a place. Named Entities Recognition with Stanford Named Entitiy Recognizer (Stanford NER) Despite Stanford NER is integrated and configured out of the box, automatic Named entities recognition needs many resources (mainly CPU time and RAM) while anylsis on document import or data enrichment, so you have to switch it on manually or you can only enrich some important documents or texts with such an automatic text analysis. To setup the automatic named entity recognition with Stanford Named Entitiy Recognizer (NER) . Setting language model How to improve Named Entities Extraction In many cases such Named Entity Recognition (NER) heuristics and machine learning works, but not all. To get the most analysis with less manual effort for meta data management, you can combine all methods and tools: Own domain knowledge by managing named entities manually and integrate, import or enrich with external knowledge or open data, extracting structured data with text patterns by regex and machine learning. Train the machine learning model You can train the machine learning model with additional structure or annotations to get better. Combine recognized Named Entities with results of additional methods like thesaurus of domain knowledge, lists of names, open data and precise rules But often rules more precise, especially for new or not very popular names. The problem with rules or the Named Entities manager is, that you have to know and manage all this names. Importing A list or ontology If a named you can define precise Text patterns by regular expressions i.e. for a special currency or for scanned forms. You can add names You can add rules by queries","title":"Named Entity Recogniton (NER)"},{"location":"doc/admin/config/named_entity_recognition/#named-entity-recogniton-ner","text":"","title":"Named Entity Recogniton (NER)"},{"location":"doc/admin/config/named_entity_recognition/#automatic-named-entity-recognition-by-machine-learning-ml-for-automatic-classification-and-annotation-of-text-parts","text":"There are models trained with existing annotations of a large text corpus, so after that they can predict / guess if a part of a sentence is a name of a person, a name of an organization, a verb or a place.","title":"Automatic Named Entity Recognition by machine learning (ML) for automatic classification and annotation of text parts"},{"location":"doc/admin/config/named_entity_recognition/#named-entities-recognition-with-stanford-named-entitiy-recognizer-stanford-ner","text":"Despite Stanford NER is integrated and configured out of the box, automatic Named entities recognition needs many resources (mainly CPU time and RAM) while anylsis on document import or data enrichment, so you have to switch it on manually or you can only enrich some important documents or texts with such an automatic text analysis. To setup the automatic named entity recognition with Stanford Named Entitiy Recognizer (NER) .","title":"Named Entities Recognition with Stanford Named Entitiy Recognizer (Stanford NER)"},{"location":"doc/admin/config/named_entity_recognition/#setting-language-model","text":"","title":"Setting language model"},{"location":"doc/admin/config/named_entity_recognition/#how-to-improve-named-entities-extraction","text":"In many cases such Named Entity Recognition (NER) heuristics and machine learning works, but not all. To get the most analysis with less manual effort for meta data management, you can combine all methods and tools: Own domain knowledge by managing named entities manually and integrate, import or enrich with external knowledge or open data, extracting structured data with text patterns by regex and machine learning.","title":"How to improve Named Entities Extraction"},{"location":"doc/admin/config/named_entity_recognition/#train-the-machine-learning-model","text":"You can train the machine learning model with additional structure or annotations to get better.","title":"Train the machine learning model"},{"location":"doc/admin/config/named_entity_recognition/#combine-recognized-named-entities-with-results-of-additional-methods-like-thesaurus-of-domain-knowledge-lists-of-names-open-data-and-precise-rules","text":"But often rules more precise, especially for new or not very popular names. The problem with rules or the Named Entities manager is, that you have to know and manage all this names. Importing A list or ontology If a named you can define precise Text patterns by regular expressions i.e. for a special currency or for scanned forms. You can add names You can add rules by queries","title":"Combine recognized Named Entities with results of additional methods like thesaurus of domain knowledge, lists of names, open data and precise rules"},{"location":"doc/admin/config/ocr/","text":"Automatic text recognition (OCR) for Solr or Elastic Search Automatic text recognition in images or scanned documents by Optical Character Recognition (OCR) Text stored in image formats like JPG, PNG, TIFF or GIF (i.e. scans, photos or screenshots) can not be found by standard full text search. So this enhancer enriches meta data of images like filename, format and size with results from automatic text recognition or optical character recognition (OCR) by free open source software like Tesseract OCR . Since many information is not searchable by full text search because its in graphical formats embedded in PDF documents (i.e. scans or screenshots instead of text format), the enhancer extracts images from PDF files for automatic text recognition (OCR), too. Enable OCR (OCR is enabled by default in the virtual machine packages like Open Semantic Desktop Search or Open Semantic Search Appliance) Install the package tesseract-ocr (included in your Linux distribution): apt-get install tesseract-ocr If you enabled OCR, should enable OCR for images inside PDF files, too, since many PDF files are scans and do contain much text data only as graphics: OCR of images embedded within PDF documents Add (uncomment) the PDF OCR plugin to enable OCR for images or scanned documents within PDF files: #Enable OCR for images inside PDF files config['plugins'].append('enhance_pdf_ocr') How to optimize OCR settings to improve OCR results You can optimize OCR results to find more by different ways, which you can combine for optimal OCR results: Scanning resolution If scanning documents yourself, scan or store the images with a higher resolution, so the OCR can analyse more details of the characters. Language of dictionary Since OCR uses a language specific dictionary, set the OCR language to your language or to multiple languages, which are used in your documents. Setting OCR language to an other language than english: 1. Install the tesseract language package (for german: tesseract-ocr-deu ). See the list of available languages for Debian or Ubuntu . 2. set option ocr_language to the language of your documents. Default is eng for english (in tesseract its eng , not en !). For german set deu (in tesseract its not de !): `# language for automatic text recognition (ocr) config['ocr_lang'] = \"eng\" config['ocr_lang'] = \"deu\"` Or set the OCR language to multiple languages, which are used in your documents: # language for automatic text recognition (ocr) config['ocr_lang'] = \"eng+deu\" Additional custom OCR dictionary entries from Thesaurus and Ontologies By the coming out of the box integration with the Tesseract OCR user word list or custom dictionary, your Concepts, words and names of named entities like organizations, places, locations or persons that are important for you so you added them to your thesaurus or which are included in lists of names or ontologies (for example lists of names of relevant persons from internal meta data sources or from open data sources like Wikidata ) you defined for faceted search/interactive filters and/or analytics/aggregated overviews are recognized better by OCR of scanned documents, too. Therefore your additional domain knowledge / vocabulary from thesaurus, lists and ontologies is used additional to the OCR dictionary by Tesseract option --user-words /etc/opensemanticsearch/ocr/dictionary.txt . Since in many scanned legacy files on paper names are fully written in uppercase, this autogenerated custom OCR dictionary / OCR wordlist includes the uppercase variant of each word, too. So you should consider to rebuild your index / reindex important files by force (so they are analyzed again & reindexed even if yet in index) after adding very important concepts or names to thesaurus or ontologies. Rotation and deskewing low quality scans before OCR Many documents are scanned skew. Additional deskewing of such low quality scans by Scantailor before OCR can improve the OCR results. Install Scantailor: apt-get install scantailor Enable additional optimization with Scantailor before OCR by adding uncomment the descewing plugin to your ETL config /etc/opensemanticsearch/etl : config['plugins'].append('enhance_ocr_descew') In default configuration the descewing plugin is disabled because it needs more time and CPU resources while indexing documents with images. Combining OCR results of multiple OCR tools No OCR engine is perfect. So in some research projects we used for example Abby Finereader to OCR the images in PDFs additionally to the integrated Open Source OCR Software Tesseract. Each of them recognized words or names the other software failed. By combining and indexing both OCR results for the same document, we could find many documents more. Therefore the Open Semantic ETL framework of Open Semantic Search is able to combine or unify and index analysis results of multiple analysis or OCR tools or OCR parameters for the same document or image. Train characters and fonts You can train the OCR with the special fonts used in your documents to improve the machine learning model for recognition of characters of this fonts. How to manage OCR failures Handle OCR errors by collaborative tagging and annotation For single documents with OCR errors you can add annotations or tags with the words that were recognized wrong by the OCR engine, so the search engine can find them despite this OCR errors because of the tags or annotations written correct. Manage OCR errors in thesaurus (Hidden labels) Manage common OCR errors for all documents and new documents by Thesaurus entries for management of OCR errors (Hidden labels) The recommender can analyse the corpus for typos/OCR errors of a thesaurus entry and recommends such misspellings for adding to the thesaurus as hidden label by one click. More information about improving OCR quality Improving the quality of the output of Tesseract OCR","title":"Automatic text recognition (OCR) for Solr or Elastic Search"},{"location":"doc/admin/config/ocr/#automatic-text-recognition-ocr-for-solr-or-elastic-search","text":"","title":"Automatic text recognition (OCR) for Solr or Elastic Search"},{"location":"doc/admin/config/ocr/#automatic-text-recognition-in-images-or-scanned-documents-by-optical-character-recognition-ocr","text":"Text stored in image formats like JPG, PNG, TIFF or GIF (i.e. scans, photos or screenshots) can not be found by standard full text search. So this enhancer enriches meta data of images like filename, format and size with results from automatic text recognition or optical character recognition (OCR) by free open source software like Tesseract OCR . Since many information is not searchable by full text search because its in graphical formats embedded in PDF documents (i.e. scans or screenshots instead of text format), the enhancer extracts images from PDF files for automatic text recognition (OCR), too.","title":"Automatic text recognition in images or scanned documents by Optical Character Recognition (OCR)"},{"location":"doc/admin/config/ocr/#enable-ocr","text":"(OCR is enabled by default in the virtual machine packages like Open Semantic Desktop Search or Open Semantic Search Appliance) Install the package tesseract-ocr (included in your Linux distribution): apt-get install tesseract-ocr If you enabled OCR, should enable OCR for images inside PDF files, too, since many PDF files are scans and do contain much text data only as graphics:","title":"Enable OCR"},{"location":"doc/admin/config/ocr/#ocr-of-images-embedded-within-pdf-documents","text":"Add (uncomment) the PDF OCR plugin to enable OCR for images or scanned documents within PDF files: #Enable OCR for images inside PDF files config['plugins'].append('enhance_pdf_ocr')","title":"OCR of images embedded within PDF documents"},{"location":"doc/admin/config/ocr/#how-to-optimize-ocr-settings-to-improve-ocr-results","text":"You can optimize OCR results to find more by different ways, which you can combine for optimal OCR results:","title":"How to optimize OCR settings to improve OCR results"},{"location":"doc/admin/config/ocr/#scanning-resolution","text":"If scanning documents yourself, scan or store the images with a higher resolution, so the OCR can analyse more details of the characters.","title":"Scanning resolution"},{"location":"doc/admin/config/ocr/#language-of-dictionary","text":"Since OCR uses a language specific dictionary, set the OCR language to your language or to multiple languages, which are used in your documents. Setting OCR language to an other language than english: 1. Install the tesseract language package (for german: tesseract-ocr-deu ). See the list of available languages for Debian or Ubuntu . 2. set option ocr_language to the language of your documents. Default is eng for english (in tesseract its eng , not en !). For german set deu (in tesseract its not de !): `# language for automatic text recognition (ocr)","title":"Language of dictionary"},{"location":"doc/admin/config/ocr/#configocr_lang-eng","text":"config['ocr_lang'] = \"deu\"` Or set the OCR language to multiple languages, which are used in your documents: # language for automatic text recognition (ocr) config['ocr_lang'] = \"eng+deu\"","title":"config['ocr_lang'] = \"eng\""},{"location":"doc/admin/config/ocr/#additional-custom-ocr-dictionary-entries-from-thesaurus-and-ontologies","text":"By the coming out of the box integration with the Tesseract OCR user word list or custom dictionary, your Concepts, words and names of named entities like organizations, places, locations or persons that are important for you so you added them to your thesaurus or which are included in lists of names or ontologies (for example lists of names of relevant persons from internal meta data sources or from open data sources like Wikidata ) you defined for faceted search/interactive filters and/or analytics/aggregated overviews are recognized better by OCR of scanned documents, too. Therefore your additional domain knowledge / vocabulary from thesaurus, lists and ontologies is used additional to the OCR dictionary by Tesseract option --user-words /etc/opensemanticsearch/ocr/dictionary.txt . Since in many scanned legacy files on paper names are fully written in uppercase, this autogenerated custom OCR dictionary / OCR wordlist includes the uppercase variant of each word, too. So you should consider to rebuild your index / reindex important files by force (so they are analyzed again & reindexed even if yet in index) after adding very important concepts or names to thesaurus or ontologies.","title":"Additional custom OCR dictionary entries from Thesaurus and Ontologies"},{"location":"doc/admin/config/ocr/#rotation-and-deskewing-low-quality-scans-before-ocr","text":"Many documents are scanned skew. Additional deskewing of such low quality scans by Scantailor before OCR can improve the OCR results. Install Scantailor: apt-get install scantailor Enable additional optimization with Scantailor before OCR by adding uncomment the descewing plugin to your ETL config /etc/opensemanticsearch/etl : config['plugins'].append('enhance_ocr_descew') In default configuration the descewing plugin is disabled because it needs more time and CPU resources while indexing documents with images.","title":"Rotation and deskewing low quality scans before OCR"},{"location":"doc/admin/config/ocr/#combining-ocr-results-of-multiple-ocr-tools","text":"No OCR engine is perfect. So in some research projects we used for example Abby Finereader to OCR the images in PDFs additionally to the integrated Open Source OCR Software Tesseract. Each of them recognized words or names the other software failed. By combining and indexing both OCR results for the same document, we could find many documents more. Therefore the Open Semantic ETL framework of Open Semantic Search is able to combine or unify and index analysis results of multiple analysis or OCR tools or OCR parameters for the same document or image.","title":"Combining OCR results of multiple OCR tools"},{"location":"doc/admin/config/ocr/#train-characters-and-fonts","text":"You can train the OCR with the special fonts used in your documents to improve the machine learning model for recognition of characters of this fonts.","title":"Train characters and fonts"},{"location":"doc/admin/config/ocr/#how-to-manage-ocr-failures","text":"","title":"How to manage OCR failures"},{"location":"doc/admin/config/ocr/#handle-ocr-errors-by-collaborative-tagging-and-annotation","text":"For single documents with OCR errors you can add annotations or tags with the words that were recognized wrong by the OCR engine, so the search engine can find them despite this OCR errors because of the tags or annotations written correct.","title":"Handle OCR errors by collaborative tagging and annotation"},{"location":"doc/admin/config/ocr/#manage-ocr-errors-in-thesaurus-hidden-labels","text":"Manage common OCR errors for all documents and new documents by Thesaurus entries for management of OCR errors (Hidden labels) The recommender can analyse the corpus for typos/OCR errors of a thesaurus entry and recommends such misspellings for adding to the thesaurus as hidden label by one click.","title":"Manage OCR errors in thesaurus (Hidden labels)"},{"location":"doc/admin/config/ocr/#more-information-about-improving-ocr-quality","text":"Improving the quality of the output of Tesseract OCR","title":"More information about improving OCR quality"},{"location":"doc/admin/config/password/","text":"Password protection If the following config files do not exist (which is the case after standard installation), there is open access for all without a password so everybody can find and read all indexed data and everyone can change, add or delete metadata configurations. How to set up a password To manage users and passwords for access to the search engine and for access to the management tools in a seperate other config file use the Apache webserver standard tool htpasswd : Access to the search engine Password protection for search users is configured in the config file /etc/solr-php-ui/.htpasswd . If this config file does not exist (which is the case after standard installation), there is open access for all without a password. Create password config file and add an user Create the password config file and add an user: htpasswd -c /etc/solr-php-ui/.htpasswd *username* Add another user(s) or change passwords Since the config file now exists, add users or change passwords: htpasswd /etc/solr-php-ui/.htpasswd *username* Access to the management tools Password protection for the management tools for datasources and metadata management (i.e. thesaurus or ontologies) is configured in the config file /etc/opensemanticsearch-django-webapps/.htpasswd . If this config file does not exist (which is the case after standard installation), there is open access for all without a password or with the search engine password, if one is set. Create password config file and add an user Create the password config file and add an user: htpasswd -c /etc/opensemanticsearch-django-webapps/.htpasswd *username* Add another user(s) or change passwords Since the config file now exists, add users or change passwords: htpasswd /etc/opensemanticsearch-django-webapps/.htpasswd *username* Django admin The admin access for Django Web UI or facet configs can be set within the directory /var/lib/opensemanticsearch by python3 manage.py createsuperuser","title":"Password protection"},{"location":"doc/admin/config/password/#password-protection","text":"If the following config files do not exist (which is the case after standard installation), there is open access for all without a password so everybody can find and read all indexed data and everyone can change, add or delete metadata configurations.","title":"Password protection"},{"location":"doc/admin/config/password/#how-to-set-up-a-password","text":"To manage users and passwords for access to the search engine and for access to the management tools in a seperate other config file use the Apache webserver standard tool htpasswd :","title":"How to set up a password"},{"location":"doc/admin/config/password/#access-to-the-search-engine","text":"Password protection for search users is configured in the config file /etc/solr-php-ui/.htpasswd . If this config file does not exist (which is the case after standard installation), there is open access for all without a password.","title":"Access to the search engine"},{"location":"doc/admin/config/password/#create-password-config-file-and-add-an-user","text":"Create the password config file and add an user: htpasswd -c /etc/solr-php-ui/.htpasswd *username*","title":"Create password config file and add an user"},{"location":"doc/admin/config/password/#add-another-users-or-change-passwords","text":"Since the config file now exists, add users or change passwords: htpasswd /etc/solr-php-ui/.htpasswd *username*","title":"Add another user(s) or change passwords"},{"location":"doc/admin/config/password/#access-to-the-management-tools","text":"Password protection for the management tools for datasources and metadata management (i.e. thesaurus or ontologies) is configured in the config file /etc/opensemanticsearch-django-webapps/.htpasswd . If this config file does not exist (which is the case after standard installation), there is open access for all without a password or with the search engine password, if one is set.","title":"Access to the management tools"},{"location":"doc/admin/config/password/#create-password-config-file-and-add-an-user_1","text":"Create the password config file and add an user: htpasswd -c /etc/opensemanticsearch-django-webapps/.htpasswd *username*","title":"Create password config file and add an user"},{"location":"doc/admin/config/password/#add-another-users-or-change-passwords_1","text":"Since the config file now exists, add users or change passwords: htpasswd /etc/opensemanticsearch-django-webapps/.htpasswd *username*","title":"Add another user(s) or change passwords"},{"location":"doc/admin/config/password/#django-admin","text":"The admin access for Django Web UI or facet configs can be set within the directory /var/lib/opensemanticsearch by python3 manage.py createsuperuser","title":"Django admin"},{"location":"doc/admin/config/scheduler/","text":"Scheduling (Cron) Cron If you use our connectors, just add Cron jobs using the command line tools the standard Linux way. Preconfigured cron jobs Import RSS feeds With the next version with integrated user interfaces for data import there will be a preconfigured Cron job that will look every minute for RSS feeds where the configured delta time is over, to import them again. Apache ManifoldCF If you use ManifoldCF connectors, you have to schedule them in the user interface of ManifoldCF, which has its own scheduler.","title":"Scheduling (Cron)"},{"location":"doc/admin/config/scheduler/#scheduling-cron","text":"","title":"Scheduling (Cron)"},{"location":"doc/admin/config/scheduler/#cron","text":"If you use our connectors, just add Cron jobs using the command line tools the standard Linux way.","title":"Cron"},{"location":"doc/admin/config/scheduler/#preconfigured-cron-jobs","text":"","title":"Preconfigured cron jobs"},{"location":"doc/admin/config/scheduler/#import-rss-feeds","text":"With the next version with integrated user interfaces for data import there will be a preconfigured Cron job that will look every minute for RSS feeds where the configured delta time is over, to import them again.","title":"Import RSS feeds"},{"location":"doc/admin/config/scheduler/#apache-manifoldcf","text":"If you use ManifoldCF connectors, you have to schedule them in the user interface of ManifoldCF, which has its own scheduler.","title":"Apache ManifoldCF"},{"location":"doc/admin/config/stemming/","text":"Finds more with grammar rules (Stemming) Considering grammar rules the search engine will find more. So if you search for company corruption you will find companies corrupted , too, even if for stupid computers this are different words or strings. Reducing words to their root form (stemming) That works because a stemming algorithm reduces words to their root form. For example the stemmer cuts suffixes like -ing or -ed or plural- s , so the search engine can find more forms of a word. So the words corrupt , corrupted and corruption will be indexed and searched all only with their root form corrupt . Preconfigurated stemming The search engine packages are preconfigurated to stemming with following grammar out of the box for documents which language is autodetected by OpenNLP integrated in Apache Tika: * English (EN) * Spanish (ES) * German / Deutsch (DE) * French / Francais (FR) * Portuguese (PT) * Dutch (NL) * Italian (IT) * Czech (CZ) * Romanian (RO) * Russian (RU) * Arabic (AR) * Persian (FA) More languages can be configured out of the box on demand (by the way would be nice, if you could donate us an translation of the search UI, too). Change the stemmer There is no perfect stemming algorithm for all situations but different more or less working stemming algorithms and implementations (stemmers). To switch to a more or less aggressive stemmer, edit the field definition of your language fields. For English grammar the field names are: text_en and synonyms_en , for German grammar the field names are text_de and synonyms_de and so on. Learn more about stemming algorithms, free open source stemmers and their configuration: Stemming algorithms Choosing a stemmer Stemming for Solr Stemming for Elastic Search Thesaurus (dictionary of linked words) and other ontologies (Linked Data) Another feature to find more is to use a thesaurus (a connected dictionary or a network of linked words or concepts) or other ontologies (linked data structures) . So for example the search engine can consider irregular verbs (for example went is not go with a suffix) and other irregular word forms, too. Additionally you will be able to find not only different forms of the same words but additionally connected words like synonyms or hyponyms. For example if you search for purple you would find violet , too. An out of the box integration of a dictionary and thesaurus based on open data from Wiktionary (the dictionary of Wikipedia) could be released next, if enough donations for that. Please donate with the subject Wiktionary for an earlier out of the box integration or if needed earlier, ask us for the config steps to setup this for your language (while setup you need a triplestore yet).","title":"Finds more with grammar rules (Stemming)"},{"location":"doc/admin/config/stemming/#finds-more-with-grammar-rules-stemming","text":"Considering grammar rules the search engine will find more. So if you search for company corruption you will find companies corrupted , too, even if for stupid computers this are different words or strings.","title":"Finds more with grammar rules (Stemming)"},{"location":"doc/admin/config/stemming/#reducing-words-to-their-root-form-stemming","text":"That works because a stemming algorithm reduces words to their root form. For example the stemmer cuts suffixes like -ing or -ed or plural- s , so the search engine can find more forms of a word. So the words corrupt , corrupted and corruption will be indexed and searched all only with their root form corrupt . Preconfigurated stemming The search engine packages are preconfigurated to stemming with following grammar out of the box for documents which language is autodetected by OpenNLP integrated in Apache Tika: * English (EN) * Spanish (ES) * German / Deutsch (DE) * French / Francais (FR) * Portuguese (PT) * Dutch (NL) * Italian (IT) * Czech (CZ) * Romanian (RO) * Russian (RU) * Arabic (AR) * Persian (FA) More languages can be configured out of the box on demand (by the way would be nice, if you could donate us an translation of the search UI, too). Change the stemmer There is no perfect stemming algorithm for all situations but different more or less working stemming algorithms and implementations (stemmers). To switch to a more or less aggressive stemmer, edit the field definition of your language fields. For English grammar the field names are: text_en and synonyms_en , for German grammar the field names are text_de and synonyms_de and so on. Learn more about stemming algorithms, free open source stemmers and their configuration: Stemming algorithms Choosing a stemmer Stemming for Solr Stemming for Elastic Search Thesaurus (dictionary of linked words) and other ontologies (Linked Data) Another feature to find more is to use a thesaurus (a connected dictionary or a network of linked words or concepts) or other ontologies (linked data structures) . So for example the search engine can consider irregular verbs (for example went is not go with a suffix) and other irregular word forms, too. Additionally you will be able to find not only different forms of the same words but additionally connected words like synonyms or hyponyms. For example if you search for purple you would find violet , too. An out of the box integration of a dictionary and thesaurus based on open data from Wiktionary (the dictionary of Wikipedia) could be released next, if enough donations for that. Please donate with the subject Wiktionary for an earlier out of the box integration or if needed earlier, ask us for the config steps to setup this for your language (while setup you need a triplestore yet).","title":"Reducing words to their root form (stemming)"},{"location":"doc/admin/config/synonyms/","text":"Setup of synonyms and aliases You can setup synonyms, aliases or alternate names with the web user interface thesaurus manager . Just add a name or concept with the synonym as an alias or import a thesaurus or ontology with synonyms to the ontologies manager . After that the module SKOS to Solr adds all alternate SKOS labels or aliases from thesaurus entries as synonym for their prefered labels to the synonym config file for Solr synonym filter . Please donate so we can integrate (technically existing) features with open data from Wiktionary, so most synonyms and more forms of irregular verbs (\"went\" is not only \"go\" with a suffix) will be preconfigured out of the box.","title":"Setup of synonyms and aliases"},{"location":"doc/admin/config/synonyms/#setup-of-synonyms-and-aliases","text":"You can setup synonyms, aliases or alternate names with the web user interface thesaurus manager . Just add a name or concept with the synonym as an alias or import a thesaurus or ontology with synonyms to the ontologies manager . After that the module SKOS to Solr adds all alternate SKOS labels or aliases from thesaurus entries as synonym for their prefered labels to the synonym config file for Solr synonym filter . Please donate so we can integrate (technically existing) features with open data from Wiktionary, so most synonyms and more forms of irregular verbs (\"went\" is not only \"go\" with a suffix) will be preconfigured out of the box.","title":"Setup of synonyms and aliases"},{"location":"doc/admin/connectors/","text":"Crawl, import and load data or ingest documents into the search index (Connectors & ETL) Crawler, connectors, data importer, data integration, document ingestion, transformation and converter We provide some light weight import / index tools / connectors i.e. for files and directories based on our open source framework for data integration, data extraction, data analysis and data enrichment . Since our open architecture is based on Solr with its open REST-API for which there are many powerful libraries for all programming languages and open standards for linked data and semantic web, you can use all other powerful frameworks, programming languages or services for crawling, ETL or web scraping which are interoperable with Solr or some open standards for databases (f.e. SQL) or data integration (f.e. RDF). For most cases there are many ready to use connectors for standard imports yet: Connector Files (filesystem and directories) Crawl and index directories, files and documents into Solr. Including automatic textrecognition (OCR) support for images and grafical formats included in PDF documents (i.e. scans) Learn more ... Connector RSS (RSS-Feed) Indexes Webpages from a RSS-Newsfeed Learn more ... Connector Web (HTTP) Crawl and index Websites into Solr index. Learn more ... Connector DB (SQL, MySQL, Postgresql) Index SQL databases like MySQL or PostgreSQL into Solr. Learn more ... Connector Scraper (Web scraping) ETL and webscraping framework to crawl, extract, transform and load structured data from websites (scraping). Learn more ...","title":"Crawl, import and load data or ingest documents into the search index (Connectors & ETL)"},{"location":"doc/admin/connectors/#crawl-import-and-load-data-or-ingest-documents-into-the-search-index-connectors-etl","text":"","title":"Crawl, import and load data or ingest documents into the search index (Connectors &amp; ETL)"},{"location":"doc/admin/connectors/#crawler-connectors-data-importer-data-integration-document-ingestion-transformation-and-converter","text":"We provide some light weight import / index tools / connectors i.e. for files and directories based on our open source framework for data integration, data extraction, data analysis and data enrichment . Since our open architecture is based on Solr with its open REST-API for which there are many powerful libraries for all programming languages and open standards for linked data and semantic web, you can use all other powerful frameworks, programming languages or services for crawling, ETL or web scraping which are interoperable with Solr or some open standards for databases (f.e. SQL) or data integration (f.e. RDF). For most cases there are many ready to use connectors for standard imports yet:","title":"Crawler, connectors, data importer, data integration, document ingestion, transformation and converter"},{"location":"doc/admin/connectors/#connector-files-filesystem-and-directories","text":"Crawl and index directories, files and documents into Solr. Including automatic textrecognition (OCR) support for images and grafical formats included in PDF documents (i.e. scans) Learn more ...","title":"Connector Files (filesystem and directories)"},{"location":"doc/admin/connectors/#connector-rss-rss-feed","text":"Indexes Webpages from a RSS-Newsfeed Learn more ...","title":"Connector RSS (RSS-Feed)"},{"location":"doc/admin/connectors/#connector-web-http","text":"Crawl and index Websites into Solr index. Learn more ...","title":"Connector Web (HTTP)"},{"location":"doc/admin/connectors/#connector-db-sql-mysql-postgresql","text":"Index SQL databases like MySQL or PostgreSQL into Solr. Learn more ...","title":"Connector DB (SQL, MySQL, Postgresql)"},{"location":"doc/admin/connectors/#connector-scraper-web-scraping","text":"ETL and webscraping framework to crawl, extract, transform and load structured data from websites (scraping). Learn more ...","title":"Connector Scraper (Web scraping)"},{"location":"doc/admin/install/","text":"Installation Installation on a desktop computer, laptop or notebook running on Linux, Windows or Mac (desktop search) Single desktop users can use the preconfigured toolbox bundle and virtual machine Open Semantic Desktop Search . Docker Coming soon. Installation on a Linux server or VM (organizations or research teams) Installation on a Linux server for social search or collaborative research for an organizations or a research team: Install Open Semantic Search on a Ubunto or Debian GNU/Linux server .","title":"Installation"},{"location":"doc/admin/install/#installation","text":"","title":"Installation"},{"location":"doc/admin/install/#installation-on-a-desktop-computer-laptop-or-notebook-running-on-linux-windows-or-mac-desktop-search","text":"Single desktop users can use the preconfigured toolbox bundle and virtual machine Open Semantic Desktop Search .","title":"Installation on a desktop computer, laptop or notebook running on Linux, Windows or Mac (desktop search)"},{"location":"doc/admin/install/#docker","text":"Coming soon.","title":"Docker"},{"location":"doc/admin/install/#installation-on-a-linux-server-or-vm-organizations-or-research-teams","text":"Installation on a Linux server for social search or collaborative research for an organizations or a research team: Install Open Semantic Search on a Ubunto or Debian GNU/Linux server .","title":"Installation on a Linux server or VM (organizations or research teams)"},{"location":"doc/admin/install/desktop_search/","text":"How to install Open Semantic Desktop Search Dependencies If not yet there download and install the following virtual machine host (available for free for Windows , Mac and Linux ) Virtual Box Installation Download the virtual machine image Open Semantic Desktop Search Start Virtual Box In the menu \" File \" start the option \" Import Appliance \". Choose the downloaded appliance file and start the import Configuration of document folders Edit the settings of the new virtual machine (choose the virtual machine in the left sidebar and click the \" settings \" button in the top bar). Add shared folders with documents : You can add one shared folder or multiple shared folders pointing to local folders on your harddisk to point to your documents, that should be indexed, searched and analysed. Activate the option Auto-mount . This folder(s) can be set read only, so you cannot accidently delete important documents because not familiar with the new user interfaces and desktop environment. Automatic indexing of new documents on virtual machine startup On startup of the virtual machine all (new) documents in the configured shared folders will be indexed. Desktop search usage After the files have been indexed you can search and analyse documents with Open Semantic Desktop Search . External index (optionally) If you want to store the index in a directory or filesystem outside the Virtual Machine, because the default size of the image is too small for your index, you can add a shared folder named index (activate the option Auto-mount ). Don't set this folder to read only, since the search engine has to store the index in it. System access and default passwords User The default password of the default user is empty and the GUI is configured for automatic login. Root access If you want full root access to the Linux System, for example to install additional packages or to change config files : Since as admin you have control of the virtual machine host and you have full access to the virtual machines data and so there is no root password. Terminal So in the terminal app you can get full root access by following command: su Console Or just login as root without password on the virtual machine console: Press CTRL-Right and F3 at once to get to the console. Login as root without password. Set a root password: passwd Exit the console: exit Return to Gnome GUI: Press CTRL-Right and F2 at once.","title":"How to install Open Semantic Desktop Search"},{"location":"doc/admin/install/desktop_search/#how-to-install-open-semantic-desktop-search","text":"","title":"How to install Open Semantic Desktop Search"},{"location":"doc/admin/install/desktop_search/#dependencies","text":"If not yet there download and install the following virtual machine host (available for free for Windows , Mac and Linux ) Virtual Box","title":"Dependencies"},{"location":"doc/admin/install/desktop_search/#installation","text":"Download the virtual machine image Open Semantic Desktop Search Start Virtual Box In the menu \" File \" start the option \" Import Appliance \". Choose the downloaded appliance file and start the import","title":"Installation"},{"location":"doc/admin/install/desktop_search/#configuration-of-document-folders","text":"Edit the settings of the new virtual machine (choose the virtual machine in the left sidebar and click the \" settings \" button in the top bar). Add shared folders with documents : You can add one shared folder or multiple shared folders pointing to local folders on your harddisk to point to your documents, that should be indexed, searched and analysed. Activate the option Auto-mount . This folder(s) can be set read only, so you cannot accidently delete important documents because not familiar with the new user interfaces and desktop environment.","title":"Configuration of document folders"},{"location":"doc/admin/install/desktop_search/#automatic-indexing-of-new-documents-on-virtual-machine-startup","text":"On startup of the virtual machine all (new) documents in the configured shared folders will be indexed.","title":"Automatic indexing of new documents on virtual machine startup"},{"location":"doc/admin/install/desktop_search/#desktop-search-usage","text":"After the files have been indexed you can search and analyse documents with Open Semantic Desktop Search .","title":"Desktop search usage"},{"location":"doc/admin/install/desktop_search/#external-index-optionally","text":"If you want to store the index in a directory or filesystem outside the Virtual Machine, because the default size of the image is too small for your index, you can add a shared folder named index (activate the option Auto-mount ). Don't set this folder to read only, since the search engine has to store the index in it.","title":"External index (optionally)"},{"location":"doc/admin/install/desktop_search/#system-access-and-default-passwords","text":"","title":"System access and default passwords"},{"location":"doc/admin/install/desktop_search/#user","text":"The default password of the default user is empty and the GUI is configured for automatic login.","title":"User"},{"location":"doc/admin/install/desktop_search/#root-access","text":"If you want full root access to the Linux System, for example to install additional packages or to change config files : Since as admin you have control of the virtual machine host and you have full access to the virtual machines data and so there is no root password.","title":"Root access"},{"location":"doc/admin/install/desktop_search/#terminal","text":"So in the terminal app you can get full root access by following command: su","title":"Terminal"},{"location":"doc/admin/install/desktop_search/#console","text":"Or just login as root without password on the virtual machine console: Press CTRL-Right and F3 at once to get to the console. Login as root without password. Set a root password: passwd Exit the console: exit Return to Gnome GUI: Press CTRL-Right and F2 at once.","title":"Console"},{"location":"doc/admin/install/search_server/","text":"How to install Open Semantic Search Server Open Semantic Desktop Search If you are an user and want only search for yourself, you maybe want to use the Open Semantic Desktop Search virtual machine , which is easier to install for single end users. Open Semantic Search Server How to setup a search engine on an intranet server running on an existing Debian GNU/Linux or Ubuntu Linux (web)server or within an existing virtual machine running a Debian GNU/Linux or Ubuntu guest: Debian GNU/Linux Download the packages and modules you want to use Get admin or super user (root) su Install the packages: dpkg --install open-semantic-search-server*.deb Install the dependencies: apt-get -f install Ubuntu Linux Download the packages and modules you want to use Install the package and its dependencies: sudo apt install ./open-semantic-search-server*.deb After installation stops continue by type in a q to end showing a status message of Solr installation script Index some files Index some files (i.e. the documentation directory of Linux): opensemanticsearch-index-dir /usr/share/doc Search Open your browser and call the new alias /search on your Apache webserver i.e.: http://localhost/search Enter a search query and enjoy the user interface for navigation, faceted search, exploratory search, interactive filters, analytics and visualizations . Config Set config options by the web admin interface . Secure access If you don't secure the used Apache webserver or this web application, all who can access your computers IP have full access to the search engine and so access to all indexed contents, too. With its API and web apps in /search-apps it is possible to index data or tag documents. If you don't want to secure the whole Apache web server (for example limit access to localhost or add a password protection with htaccess), you should limit access to the search in /etc/solr-php-ui/apache.conf and limit access to the REST-API and web apps in /etc/opensemanticsearch-django-webapps/apache.conf ... The initial password for the Django admin interface (i.e. for adding tags that are usable for documents tagging) is live URL Mapping If you want to setup a document search based on local files as a website with documents stored as local files in a local directory where users don't have access to your files and directories via the filesystem ( file:// ) you can setup a mapping to another protocoll (http://) and a directory where the files are available on your webserver. You can set the config option for mapping in /etc/opensemanticsearch/connector-files , where you can find the explaination how to do it. More config options You will find another flexible options in the config files of other modules in /etc/opensemanticsearch/ . Learn more ...","title":"How to install Open Semantic Search Server"},{"location":"doc/admin/install/search_server/#how-to-install-open-semantic-search-server","text":"","title":"How to install Open Semantic Search Server"},{"location":"doc/admin/install/search_server/#open-semantic-desktop-search","text":"If you are an user and want only search for yourself, you maybe want to use the Open Semantic Desktop Search virtual machine , which is easier to install for single end users.","title":"Open Semantic Desktop Search"},{"location":"doc/admin/install/search_server/#open-semantic-search-server","text":"How to setup a search engine on an intranet server running on an existing Debian GNU/Linux or Ubuntu Linux (web)server or within an existing virtual machine running a Debian GNU/Linux or Ubuntu guest:","title":"Open Semantic Search Server"},{"location":"doc/admin/install/search_server/#debian-gnulinux","text":"Download the packages and modules you want to use Get admin or super user (root) su Install the packages: dpkg --install open-semantic-search-server*.deb Install the dependencies: apt-get -f install","title":"Debian GNU/Linux"},{"location":"doc/admin/install/search_server/#ubuntu-linux","text":"Download the packages and modules you want to use Install the package and its dependencies: sudo apt install ./open-semantic-search-server*.deb After installation stops continue by type in a q to end showing a status message of Solr installation script","title":"Ubuntu Linux"},{"location":"doc/admin/install/search_server/#index-some-files","text":"Index some files (i.e. the documentation directory of Linux): opensemanticsearch-index-dir /usr/share/doc","title":"Index some files"},{"location":"doc/admin/install/search_server/#search","text":"Open your browser and call the new alias /search on your Apache webserver i.e.: http://localhost/search Enter a search query and enjoy the user interface for navigation, faceted search, exploratory search, interactive filters, analytics and visualizations .","title":"Search"},{"location":"doc/admin/install/search_server/#config","text":"Set config options by the web admin interface .","title":"Config"},{"location":"doc/admin/install/search_server/#secure-access","text":"If you don't secure the used Apache webserver or this web application, all who can access your computers IP have full access to the search engine and so access to all indexed contents, too. With its API and web apps in /search-apps it is possible to index data or tag documents. If you don't want to secure the whole Apache web server (for example limit access to localhost or add a password protection with htaccess), you should limit access to the search in /etc/solr-php-ui/apache.conf and limit access to the REST-API and web apps in /etc/opensemanticsearch-django-webapps/apache.conf ... The initial password for the Django admin interface (i.e. for adding tags that are usable for documents tagging) is live","title":"Secure access"},{"location":"doc/admin/install/search_server/#url-mapping","text":"If you want to setup a document search based on local files as a website with documents stored as local files in a local directory where users don't have access to your files and directories via the filesystem ( file:// ) you can setup a mapping to another protocoll (http://) and a directory where the files are available on your webserver. You can set the config option for mapping in /etc/opensemanticsearch/connector-files , where you can find the explaination how to do it.","title":"URL Mapping"},{"location":"doc/admin/install/search_server/#more-config-options","text":"You will find another flexible options in the config files of other modules in /etc/opensemanticsearch/ . Learn more ...","title":"More config options"},{"location":"doc/admin/optimize/","text":"Scaling and increasing performance by parallel processing, server clusters & buffers for writing How to index, search and explore big data faster by scaling and optimizing data processing This is a tutorial how to index very large document sets faster technically to enable faster search or analysis. So this tutorial is not about how to analyse, search, explore & filter big document collections . Indexing only filenames for (very fast) exploration and filtering by filenames First index only the filenames, which doesn't need much time. So you can explore, filter and find the most interesting paths or filenames without wating for completition of a full run maybe indexing long time many directories and files, that are not so interesting and for example start first processing of very interesting parts. Therefore first run indexing using only with the enhance_path plugin: `opensemanticsearch-index-file --plugins enhance_path dirname This will be very fast, because only indexing the paths and file names without extraction of other metadata or content. After you are able to search all existing filenames you can first or additionally (parallel) index very interesting paths or files. Doing expensive tasks like optical character recognition (OCR) later by disable expensive data enrichment plugins for first indexing run) Data enrichment with OCR needs very much time and CPU ressources for only some percent of additional content. So you might want to disable OCR and do a first full indexing without OCR to be able to search & analyze most parts of your data much earlier and meanwhile running a second forced indexing with OCR in the backround (after activating OCR again or based on an additional config file), later or at night. So you can do a first run or standard runs without OCR: Therefore disable OCR in /etc/opensemanticsearch/etl and in your Tika config (inside /usr/share/java/tika-server.jar) or just temporary deinstall tesseract-ocr. Second run with enabled OCR Enable OCR in your configs again. If you just deinstalled tesseract-ocr, dont forget to install this package and your additional OCR language packages (for example tesseract-ocr-deu for german) again. Force a second run with opensemanticsearch-index-dir --force *dirname* Or enrich yet indexed data with the OCR plugin from time to time or after indexing by running the data enrichment tool for only the OCR plugin, so other plugins have not to run again. Parallel processing of data extraction and data enrichment If there is more than one processor available, tasks like extracting text from documents and OCR will be calculated by parallel processing for multiple files simultanously so that all available processors will be used. So if working within a virtual machine (like for example Open Semantic Desktop Search), don't forget to set up more or all processors for the virtual machine settings (in the vertical tab System and horizontal tab Processor ) You can even use additional computers to a machine cluster, so multiple processors on multiple computers will be used. Parallel processing of indexing and search with server cluster for the index (Solr cluster) Too many or too slow searches? You can scale the Solr index from a single Solr index server and search server to a search cluster (Solr cloud) on multiple servers to share the load of many searches. ETL cluster for extracting text and OCR Too slow while indexing too many documents? With additional servers with installations of Open Semantic ETL for time intensive data processing or document processing like text extraction or OCR, if you give them access to the files and to your index server. Hint: preconfigurated Solr is open only from localhost, so open the Port for the other workers of your ETL cluster. Or if you use Hadoop, you can index files parallel using the whole Hadoop cluster with Map Reduce Indexer Tool . Donate You can donate to the open source project to get an optimized version and simple user interfaces for indexing faster (for example fast indexing without OCR with one click and doing OCR later while you are able to search in most data much sooner or easy to handle packages for setting up a cluster).`","title":"Scaling and increasing performance by parallel processing, server clusters & buffers for writing"},{"location":"doc/admin/optimize/#scaling-and-increasing-performance-by-parallel-processing-server-clusters-buffers-for-writing","text":"","title":"Scaling and increasing performance by parallel processing, server clusters &amp; buffers for writing"},{"location":"doc/admin/optimize/#how-to-index-search-and-explore-big-data-faster-by-scaling-and-optimizing-data-processing","text":"This is a tutorial how to index very large document sets faster technically to enable faster search or analysis. So this tutorial is not about how to analyse, search, explore & filter big document collections .","title":"How to index, search and explore big data faster by scaling and optimizing data processing"},{"location":"doc/admin/optimize/#indexing-only-filenames-for-very-fast-exploration-and-filtering-by-filenames","text":"First index only the filenames, which doesn't need much time. So you can explore, filter and find the most interesting paths or filenames without wating for completition of a full run maybe indexing long time many directories and files, that are not so interesting and for example start first processing of very interesting parts. Therefore first run indexing using only with the enhance_path plugin: `opensemanticsearch-index-file --plugins enhance_path dirname This will be very fast, because only indexing the paths and file names without extraction of other metadata or content. After you are able to search all existing filenames you can first or additionally (parallel) index very interesting paths or files.","title":"Indexing only filenames for (very fast) exploration and filtering by filenames"},{"location":"doc/admin/optimize/#doing-expensive-tasks-like-optical-character-recognition-ocr-later-by-disable-expensive-data-enrichment-plugins-for-first-indexing-run","text":"Data enrichment with OCR needs very much time and CPU ressources for only some percent of additional content. So you might want to disable OCR and do a first full indexing without OCR to be able to search & analyze most parts of your data much earlier and meanwhile running a second forced indexing with OCR in the backround (after activating OCR again or based on an additional config file), later or at night. So you can do a first run or standard runs without OCR: Therefore disable OCR in /etc/opensemanticsearch/etl and in your Tika config (inside /usr/share/java/tika-server.jar) or just temporary deinstall tesseract-ocr.","title":"Doing expensive tasks like optical character recognition (OCR) later by disable expensive data enrichment plugins for first indexing run)"},{"location":"doc/admin/optimize/#second-run-with-enabled-ocr","text":"Enable OCR in your configs again. If you just deinstalled tesseract-ocr, dont forget to install this package and your additional OCR language packages (for example tesseract-ocr-deu for german) again. Force a second run with opensemanticsearch-index-dir --force *dirname* Or enrich yet indexed data with the OCR plugin from time to time or after indexing by running the data enrichment tool for only the OCR plugin, so other plugins have not to run again.","title":"Second run with enabled OCR"},{"location":"doc/admin/optimize/#parallel-processing-of-data-extraction-and-data-enrichment","text":"If there is more than one processor available, tasks like extracting text from documents and OCR will be calculated by parallel processing for multiple files simultanously so that all available processors will be used. So if working within a virtual machine (like for example Open Semantic Desktop Search), don't forget to set up more or all processors for the virtual machine settings (in the vertical tab System and horizontal tab Processor ) You can even use additional computers to a machine cluster, so multiple processors on multiple computers will be used.","title":"Parallel processing of data extraction and data enrichment"},{"location":"doc/admin/optimize/#parallel-processing-of-indexing-and-search-with-server-cluster-for-the-index-solr-cluster","text":"Too many or too slow searches? You can scale the Solr index from a single Solr index server and search server to a search cluster (Solr cloud) on multiple servers to share the load of many searches.","title":"Parallel processing of indexing and search with server cluster for the index (Solr cluster)"},{"location":"doc/admin/optimize/#etl-cluster-for-extracting-text-and-ocr","text":"Too slow while indexing too many documents? With additional servers with installations of Open Semantic ETL for time intensive data processing or document processing like text extraction or OCR, if you give them access to the files and to your index server. Hint: preconfigurated Solr is open only from localhost, so open the Port for the other workers of your ETL cluster. Or if you use Hadoop, you can index files parallel using the whole Hadoop cluster with Map Reduce Indexer Tool .","title":"ETL cluster for extracting text and OCR"},{"location":"doc/admin/optimize/#donate","text":"You can donate to the open source project to get an optimized version and simple user interfaces for indexing faster (for example fast indexing without OCR with one click and doing OCR later while you are able to search in most data much sooner or easy to handle packages for setting up a cluster).`","title":"Donate"},{"location":"doc/admin/queue/","text":"Task queue management The task queue for the ETL workers is managed by Celery in a RabbitMQ queue. Command line interface (CLI) So you can use the queue management tools of RabbitMQ and queue management tools of Celery . Status of file imports in search user interface If this feature is not disabled by config, the status of file imports and OCR tasks is shown in the search user interface: While the file import is running, you can yet search by filenames and prioritize the import of certain files by click on \" Prioritize import \". Web user interface for task management (Flower web UI) You can monitor and manage the ETL task queue for document processing by Celery Flower web user interface (UI): A link to the Flower instance on your host (f.e. http://localhost/flower ) is available in the search user interface in the menu \" Datasources \" as \" Show running and open imports and analysis tasks (ETL tasks) \". Priorities By the config file /etc/opensemanticsearch/task_priorities you can set priorities by file extension types like .pdf . As default this option is set to process documents like PDF or DOC before images like JPG and files like .exe last.","title":"Task queue"},{"location":"doc/admin/queue/#task-queue-management","text":"The task queue for the ETL workers is managed by Celery in a RabbitMQ queue.","title":"Task queue management"},{"location":"doc/admin/queue/#command-line-interface-cli","text":"So you can use the queue management tools of RabbitMQ and queue management tools of Celery .","title":"Command line interface (CLI)"},{"location":"doc/admin/queue/#status-of-file-imports-in-search-user-interface","text":"If this feature is not disabled by config, the status of file imports and OCR tasks is shown in the search user interface: While the file import is running, you can yet search by filenames and prioritize the import of certain files by click on \" Prioritize import \".","title":"Status of file imports in search user interface"},{"location":"doc/admin/queue/#web-user-interface-for-task-management-flower-web-ui","text":"You can monitor and manage the ETL task queue for document processing by Celery Flower web user interface (UI): A link to the Flower instance on your host (f.e. http://localhost/flower ) is available in the search user interface in the menu \" Datasources \" as \" Show running and open imports and analysis tasks (ETL tasks) \".","title":"Web user interface for task management (Flower web UI)"},{"location":"doc/admin/queue/#priorities","text":"By the config file /etc/opensemanticsearch/task_priorities you can set priorities by file extension types like .pdf . As default this option is set to process documents like PDF or DOC before images like JPG and files like .exe last.","title":"Priorities"},{"location":"doc/admin/rest-api/","text":"REST API Application programming interface (API) available via generic HTTP waiting for another (web) service or software demanding for an action like crawling a directory or a webpage or indexing changed data (i.e. directly started after data change by a trigger of the CMS) and starting this actions. Crawling directories Crawl a directory and all subdirectories and index all files: http://127.0.0.1/search-apps/api/index-dir?uri=/usr/share/doc Index a file Index a single file: http://127.0.0.1:/search-apps/api/index-file?uri=/home/opensemanticsearch/readme.txt Index a web page Index a single web page: http://127.0.0.1:/search-apps/api/index-web?uri=http://www.opensemanticsearch.org/ Index a RSS-Feed Index each webpage from a RSS-Newsfeed: http://127.0.0.1:/search-apps/api/index-rss?uri=http://www.opensemanticsearch.org/feed Delete a file or document from index Delete a webpage or removed document file from index: http://127.0.0.1:/search-apps/api/delete?uri=file:///home/user/Documents/document.doc Search or read data We won't reinvent all wheels, so use the Rest-API of Solr for searching or / and getting data in XML or JSON format or use Solr Client APIs to get data with your favorite programming language.","title":"REST API"},{"location":"doc/admin/rest-api/#rest-api","text":"Application programming interface (API) available via generic HTTP waiting for another (web) service or software demanding for an action like crawling a directory or a webpage or indexing changed data (i.e. directly started after data change by a trigger of the CMS) and starting this actions.","title":"REST API"},{"location":"doc/admin/rest-api/#crawling-directories","text":"Crawl a directory and all subdirectories and index all files: http://127.0.0.1/search-apps/api/index-dir?uri=/usr/share/doc","title":"Crawling directories"},{"location":"doc/admin/rest-api/#index-a-file","text":"Index a single file: http://127.0.0.1:/search-apps/api/index-file?uri=/home/opensemanticsearch/readme.txt","title":"Index a file"},{"location":"doc/admin/rest-api/#index-a-web-page","text":"Index a single web page: http://127.0.0.1:/search-apps/api/index-web?uri=http://www.opensemanticsearch.org/","title":"Index a web page"},{"location":"doc/admin/rest-api/#index-a-rss-feed","text":"Index each webpage from a RSS-Newsfeed: http://127.0.0.1:/search-apps/api/index-rss?uri=http://www.opensemanticsearch.org/feed","title":"Index a RSS-Feed"},{"location":"doc/admin/rest-api/#delete-a-file-or-document-from-index","text":"Delete a webpage or removed document file from index: http://127.0.0.1:/search-apps/api/delete?uri=file:///home/user/Documents/document.doc","title":"Delete a file or document from index"},{"location":"doc/admin/rest-api/#search-or-read-data","text":"We won't reinvent all wheels, so use the Rest-API of Solr for searching or / and getting data in XML or JSON format or use Solr Client APIs to get data with your favorite programming language.","title":"Search or read data"},{"location":"doc/admin/scale/","text":"Scaling and increasing performance by parallel processing, server clusters & buffers for writing How to index, search and explore big data faster by scaling and optimizing data processing This is a tutorial how to index very large document sets faster technically to enable faster search or analysis. So this tutorial is not about how to analyse, search, explore & filter big document collections . Indexing only filenames for (very fast) exploration and filtering by filenames First index only the filenames, which doesn't need much time. So you can explore, filter and find the most interesting paths or filenames without wating for completition of a full run maybe indexing long time many directories and files, that are not so interesting and for example start first processing of very interesting parts. Therefore first run indexing using only with the enhance_path plugin: `opensemanticsearch-index-file --plugins enhance_path dirname This will be very fast, because only indexing the paths and file names without extraction of other metadata or content. After you are able to search all existing filenames you can first or additionally (parallel) index very interesting paths or files. Doing expensive tasks like optical character recognition (OCR) later by disable expensive data enrichment plugins for first indexing run) Data enrichment with OCR needs very much time and CPU ressources for only some percent of additional content. So you might want to disable OCR and do a first full indexing without OCR to be able to search & analyze most parts of your data much earlier and meanwhile running a second forced indexing with OCR in the backround (after activating OCR again or based on an additional config file), later or at night. So you can do a first run or standard runs without OCR: Therefore disable OCR in /etc/opensemanticsearch/etl and in your Tika config (inside /usr/share/java/tika-server.jar) or just temporary deinstall tesseract-ocr. Second run with enabled OCR Enable OCR in your configs again. If you just deinstalled tesseract-ocr, dont forget to install this package and your additional OCR language packages (for example tesseract-ocr-deu for german) again. Force a second run with opensemanticsearch-index-dir --force *dirname* Or enrich yet indexed data with the OCR plugin from time to time or after indexing by running the data enrichment tool for only the OCR plugin, so other plugins have not to run again. Parallel processing of data extraction and data enrichment If there is more than one processor available, tasks like extracting text from documents and OCR will be calculated by parallel processing for multiple files simultanously so that all available processors will be used. So if working within a virtual machine (like for example Open Semantic Desktop Search), don't forget to set up more or all processors for the virtual machine settings (in the vertical tab System and horizontal tab Processor ) You can even use additional computers to a machine cluster, so multiple processors on multiple computers will be used. Parallel processing of indexing and search with server cluster for the index (Solr cluster) Too many or too slow searches? You can scale the Solr index from a single Solr index server and search server to a search cluster (Solr cloud) on multiple servers to share the load of many searches. ETL cluster for extracting text and OCR Too slow while indexing too many documents? With additional servers with installations of Open Semantic ETL for time intensive data processing or document processing like text extraction or OCR, if you give them access to the files and to your index server. Hint: preconfigurated Solr is open only from localhost, so open the Port for the other workers of your ETL cluster. Or if you use Hadoop, you can index files parallel using the whole Hadoop cluster with Map Reduce Indexer Tool . Donate You can donate to the open source project to get an optimized version and simple user interfaces for indexing faster (for example fast indexing without OCR with one click and doing OCR later while you are able to search in most data much sooner or easy to handle packages for setting up a cluster).`","title":"Scaling and increasing performance by parallel processing, server clusters & buffers for writing"},{"location":"doc/admin/scale/#scaling-and-increasing-performance-by-parallel-processing-server-clusters-buffers-for-writing","text":"","title":"Scaling and increasing performance by parallel processing, server clusters &amp; buffers for writing"},{"location":"doc/admin/scale/#how-to-index-search-and-explore-big-data-faster-by-scaling-and-optimizing-data-processing","text":"This is a tutorial how to index very large document sets faster technically to enable faster search or analysis. So this tutorial is not about how to analyse, search, explore & filter big document collections .","title":"How to index, search and explore big data faster by scaling and optimizing data processing"},{"location":"doc/admin/scale/#indexing-only-filenames-for-very-fast-exploration-and-filtering-by-filenames","text":"First index only the filenames, which doesn't need much time. So you can explore, filter and find the most interesting paths or filenames without wating for completition of a full run maybe indexing long time many directories and files, that are not so interesting and for example start first processing of very interesting parts. Therefore first run indexing using only with the enhance_path plugin: `opensemanticsearch-index-file --plugins enhance_path dirname This will be very fast, because only indexing the paths and file names without extraction of other metadata or content. After you are able to search all existing filenames you can first or additionally (parallel) index very interesting paths or files.","title":"Indexing only filenames for (very fast) exploration and filtering by filenames"},{"location":"doc/admin/scale/#doing-expensive-tasks-like-optical-character-recognition-ocr-later-by-disable-expensive-data-enrichment-plugins-for-first-indexing-run","text":"Data enrichment with OCR needs very much time and CPU ressources for only some percent of additional content. So you might want to disable OCR and do a first full indexing without OCR to be able to search & analyze most parts of your data much earlier and meanwhile running a second forced indexing with OCR in the backround (after activating OCR again or based on an additional config file), later or at night. So you can do a first run or standard runs without OCR: Therefore disable OCR in /etc/opensemanticsearch/etl and in your Tika config (inside /usr/share/java/tika-server.jar) or just temporary deinstall tesseract-ocr.","title":"Doing expensive tasks like optical character recognition (OCR) later by disable expensive data enrichment plugins for first indexing run)"},{"location":"doc/admin/scale/#second-run-with-enabled-ocr","text":"Enable OCR in your configs again. If you just deinstalled tesseract-ocr, dont forget to install this package and your additional OCR language packages (for example tesseract-ocr-deu for german) again. Force a second run with opensemanticsearch-index-dir --force *dirname* Or enrich yet indexed data with the OCR plugin from time to time or after indexing by running the data enrichment tool for only the OCR plugin, so other plugins have not to run again.","title":"Second run with enabled OCR"},{"location":"doc/admin/scale/#parallel-processing-of-data-extraction-and-data-enrichment","text":"If there is more than one processor available, tasks like extracting text from documents and OCR will be calculated by parallel processing for multiple files simultanously so that all available processors will be used. So if working within a virtual machine (like for example Open Semantic Desktop Search), don't forget to set up more or all processors for the virtual machine settings (in the vertical tab System and horizontal tab Processor ) You can even use additional computers to a machine cluster, so multiple processors on multiple computers will be used.","title":"Parallel processing of data extraction and data enrichment"},{"location":"doc/admin/scale/#parallel-processing-of-indexing-and-search-with-server-cluster-for-the-index-solr-cluster","text":"Too many or too slow searches? You can scale the Solr index from a single Solr index server and search server to a search cluster (Solr cloud) on multiple servers to share the load of many searches.","title":"Parallel processing of indexing and search with server cluster for the index (Solr cluster)"},{"location":"doc/admin/scale/#etl-cluster-for-extracting-text-and-ocr","text":"Too slow while indexing too many documents? With additional servers with installations of Open Semantic ETL for time intensive data processing or document processing like text extraction or OCR, if you give them access to the files and to your index server. Hint: preconfigurated Solr is open only from localhost, so open the Port for the other workers of your ETL cluster. Or if you use Hadoop, you can index files parallel using the whole Hadoop cluster with Map Reduce Indexer Tool .","title":"ETL cluster for extracting text and OCR"},{"location":"doc/admin/scale/#donate","text":"You can donate to the open source project to get an optimized version and simple user interfaces for indexing faster (for example fast indexing without OCR with one click and doing OCR later while you are able to search in most data much sooner or easy to handle packages for setting up a cluster).`","title":"Donate"},{"location":"doc/admin/storage/","text":"Data storage locations In which paths and directories Open Semantic Search stores your data: Logfiles See separated documentation of logs . Database (Open Semantic Search Apps) The Django database of the search apps with your thesaurus, ontologies, facets config and your config by web (Admin UI) is stored in a SQLite DB in /var/opensemanticsearch/db/ Uploaded Ontologies files and Lists are stored in /var/opensemanticsearch/media/ontologies/ Search index (Solr) The Apache Solr search indexes are stored in /var/solr/data/ The document index is located in /var/solr/data/opensemanticsearch/ The entities index of Open Semantic Entity Search API is located in /var/solr/data/opensemanticsearch-entities/ Tasks queue (RabbitMQ) The RabbitMQ task queue is stored in the standard location of the Debian Package rabbitmq-server : /var/lib/rabbitmq/ OCR cache (Tesseract-OCR-Cache) The OCR cache containing the plain texts of images in documents that had been processed by Tesseract-OCR-Cache is stored in /var/cache/tesseract/","title":"Data storage locations"},{"location":"doc/admin/storage/#data-storage-locations","text":"In which paths and directories Open Semantic Search stores your data:","title":"Data storage locations"},{"location":"doc/admin/storage/#logfiles","text":"See separated documentation of logs .","title":"Logfiles"},{"location":"doc/admin/storage/#database-open-semantic-search-apps","text":"The Django database of the search apps with your thesaurus, ontologies, facets config and your config by web (Admin UI) is stored in a SQLite DB in /var/opensemanticsearch/db/ Uploaded Ontologies files and Lists are stored in /var/opensemanticsearch/media/ontologies/","title":"Database (Open Semantic Search Apps)"},{"location":"doc/admin/storage/#search-index-solr","text":"The Apache Solr search indexes are stored in /var/solr/data/ The document index is located in /var/solr/data/opensemanticsearch/ The entities index of Open Semantic Entity Search API is located in /var/solr/data/opensemanticsearch-entities/","title":"Search index (Solr)"},{"location":"doc/admin/storage/#tasks-queue-rabbitmq","text":"The RabbitMQ task queue is stored in the standard location of the Debian Package rabbitmq-server : /var/lib/rabbitmq/","title":"Tasks queue (RabbitMQ)"},{"location":"doc/admin/storage/#ocr-cache-tesseract-ocr-cache","text":"The OCR cache containing the plain texts of images in documents that had been processed by Tesseract-OCR-Cache is stored in /var/cache/tesseract/","title":"OCR cache (Tesseract-OCR-Cache)"},{"location":"doc/admin/twitter/","text":"title: Suchmaschine f\u00fcr Twitter: Tweets indexieren authors: - Markus Mandalka Suchmaschine f\u00fcr Twitter: Tweets indexieren Twitter indexieren Einzelne Abrufe von Twitter (per Cron auch zu gew\u00fcnschten Zeiten und / oder in gew\u00fcnschten Abst\u00e4nden durchf\u00fchrbar): Tweets von NutzeInnen indexieren, denen Sie folgen Indexiert Tweets aller Twitter-Accounts, denen gefolgt wird: solr\\_indextwitter Tweets eines Nutzers indexieren Indexiert alle Tweets des Accounts @opensemsearch (seine und von diesem retweetete) solr\\_indextwitter --timeline @opensemsearch Hashtag indexieren Sucht und indexiert Tweets mit dem Hashtag #opensemanticsearch , auch wenn den AutorInnen nicht gefolgt wird: solr\\_indextwitter --search #opensemanticsearch Tweets mit Suchwort indexieren Sucht und indexiert Tweets, in denen das Suchwort test enthalten ist, auch wenn den AutorInnen nicht gefolgt wird: solr\\_indextwitter --search test Twitter Stream indexieren Der durchg\u00e4ngig laufende Daemon fragt nicht einnmalig oder in bestimmten Zeitabst\u00e4nden regelm\u00e4ssig die neuesten Tweets bei Twitter ab, sondern schaltet mittels Twitter Stream API auf dauerhaften Empfang. Twitter sendet dann neue Tweets sobald sie ver\u00f6ffentlicht werden, so dass diese sofort indexiert werden. solr\\_indextwitter --stream So werden neben aller abonnierten Twitter-Accounts in der eigenen Timeline zus\u00e4tzlich weitere Suchworte monitored: solr\\_indextwitter --stream --search #hashtagX,#hashtagY,keywordX,keywordY,keywordZ Die Suchworte werden mit dem Parameter --search angegeben, ansonsten werden die Suchworte aus der Suchmaschinenkonfiguration (siehe Datenquellen) verwendet.","title":"Index"},{"location":"doc/admin/twitter/#suchmaschine-fur-twitter-tweets-indexieren","text":"","title":"Suchmaschine f\u00fcr Twitter: Tweets indexieren"},{"location":"doc/admin/twitter/#twitter-indexieren","text":"Einzelne Abrufe von Twitter (per Cron auch zu gew\u00fcnschten Zeiten und / oder in gew\u00fcnschten Abst\u00e4nden durchf\u00fchrbar):","title":"Twitter indexieren"},{"location":"doc/admin/twitter/#tweets-von-nutzeinnen-indexieren-denen-sie-folgen","text":"Indexiert Tweets aller Twitter-Accounts, denen gefolgt wird: solr\\_indextwitter","title":"Tweets von NutzeInnen indexieren, denen Sie folgen"},{"location":"doc/admin/twitter/#tweets-eines-nutzers-indexieren","text":"Indexiert alle Tweets des Accounts @opensemsearch (seine und von diesem retweetete) solr\\_indextwitter --timeline @opensemsearch","title":"Tweets eines Nutzers indexieren"},{"location":"doc/admin/twitter/#hashtag-indexieren","text":"Sucht und indexiert Tweets mit dem Hashtag #opensemanticsearch , auch wenn den AutorInnen nicht gefolgt wird: solr\\_indextwitter --search #opensemanticsearch","title":"Hashtag indexieren"},{"location":"doc/admin/twitter/#tweets-mit-suchwort-indexieren","text":"Sucht und indexiert Tweets, in denen das Suchwort test enthalten ist, auch wenn den AutorInnen nicht gefolgt wird: solr\\_indextwitter --search test","title":"Tweets mit Suchwort indexieren"},{"location":"doc/admin/twitter/#twitter-stream-indexieren","text":"Der durchg\u00e4ngig laufende Daemon fragt nicht einnmalig oder in bestimmten Zeitabst\u00e4nden regelm\u00e4ssig die neuesten Tweets bei Twitter ab, sondern schaltet mittels Twitter Stream API auf dauerhaften Empfang. Twitter sendet dann neue Tweets sobald sie ver\u00f6ffentlicht werden, so dass diese sofort indexiert werden. solr\\_indextwitter --stream So werden neben aller abonnierten Twitter-Accounts in der eigenen Timeline zus\u00e4tzlich weitere Suchworte monitored: solr\\_indextwitter --stream --search #hashtagX,#hashtagY,keywordX,keywordY,keywordZ Die Suchworte werden mit dem Parameter --search angegeben, ansonsten werden die Suchworte aus der Suchmaschinenkonfiguration (siehe Datenquellen) verwendet.","title":"Twitter Stream indexieren"},{"location":"doc/analytics/","text":"Analyze and explore (Analytics, Exploratory Search, Text Mining, Topic Modeling and Datavisualization) How to analyze, explore and datamine big document sets by Exploratory Search, Faceted Search, Entity Extraction and Text Mining (Datamining by Text Analysis, Natural Language Processing, Machine Learning and Clustering) There are user interfaces and views (just press the button/switch to the tab) for the following analysis and exploration for the results of a search query / search context or if no search query or filter is set, for all indexed documents: Aggregated overview (who, when, where, what, how often) Aggregated overview and named entities in the right sidebar shows you: Who? Which organizations? What? Where? When? In how many documents? Preview extracted text of document content The preview shows the extracted text of a document. Search context like the search query or active filters are marked within the content, so you can see fast whats important for you and why this document matched your query. Visualize locations occur in documents on Interactive map By the view / tab \"Map\" you can analyze visual which locations occur in the documents of your search results: . Networks, relations and connections (graph) The graph view generates data visualizations of networks, connections and relations between named entities like persons, organizations or tags from the content of your documents. Trend chart datavisualization (how many documents when) Trend chart : When how many results or documents? Table (all fields/columns/facets, minimum and maximum) The Tables view shows and can sort fields and get the maximum value and minimum value, the average and the median of each field/column Word list and word cloud (words) Word list and word cloud : Most used words, concept or names Text mining and topic modeling (content analysis) Text analysis, text mining, document mining and words : What is the content about? Similarity and differences (text comparision and overrepresented terms) Please donate: Show differences and focal points, core areas and key aspects by comparing word frequencies to find out what concepts or entities are overrepresented in documents in comparison to other documents or text corpus. Compare two texts / versions to show differences or same/copied passages. Connected words / Co-occurrences (N-Grams like Bigrams or Trigrams) Search with a whole document or text as a search query: Find similar text or documents about the same topics by klicking on \"more of that\".","title":"Analyze and explore (Analytics, Exploratory Search, Text Mining, Topic Modeling and Datavisualization)"},{"location":"doc/analytics/#analyze-and-explore-analytics-exploratory-search-text-mining-topic-modeling-and-datavisualization","text":"","title":"Analyze and explore (Analytics, Exploratory Search, Text Mining, Topic Modeling and Datavisualization)"},{"location":"doc/analytics/#how-to-analyze-explore-and-datamine-big-document-sets-by-exploratory-search-faceted-search-entity-extraction-and-text-mining-datamining-by-text-analysis-natural-language-processing-machine-learning-and-clustering","text":"There are user interfaces and views (just press the button/switch to the tab) for the following analysis and exploration for the results of a search query / search context or if no search query or filter is set, for all indexed documents:","title":"How to analyze, explore and datamine big document sets by Exploratory Search, Faceted Search, Entity Extraction and Text Mining (Datamining by Text Analysis, Natural Language Processing, Machine Learning and Clustering)"},{"location":"doc/analytics/#aggregated-overview-who-when-where-what-how-often","text":"Aggregated overview and named entities in the right sidebar shows you: Who? Which organizations? What? Where? When? In how many documents?","title":"Aggregated overview (who, when, where, what, how often)"},{"location":"doc/analytics/#preview-extracted-text-of-document-content","text":"The preview shows the extracted text of a document. Search context like the search query or active filters are marked within the content, so you can see fast whats important for you and why this document matched your query.","title":"Preview extracted text of document content"},{"location":"doc/analytics/#visualize-locations-occur-in-documents-on-interactive-map","text":"By the view / tab \"Map\" you can analyze visual which locations occur in the documents of your search results: .","title":"Visualize locations occur in documents on Interactive map"},{"location":"doc/analytics/#networks-relations-and-connections-graph","text":"The graph view generates data visualizations of networks, connections and relations between named entities like persons, organizations or tags from the content of your documents.","title":"Networks, relations and connections (graph)"},{"location":"doc/analytics/#trend-chart-datavisualization-how-many-documents-when","text":"Trend chart : When how many results or documents?","title":"Trend chart datavisualization (how many documents when)"},{"location":"doc/analytics/#table-all-fieldscolumnsfacets-minimum-and-maximum","text":"The Tables view shows and can sort fields and get the maximum value and minimum value, the average and the median of each field/column","title":"Table (all fields/columns/facets, minimum and maximum)"},{"location":"doc/analytics/#word-list-and-word-cloud-words","text":"Word list and word cloud : Most used words, concept or names","title":"Word list and word cloud (words)"},{"location":"doc/analytics/#text-mining-and-topic-modeling-content-analysis","text":"Text analysis, text mining, document mining and words : What is the content about?","title":"Text mining and topic modeling (content analysis)"},{"location":"doc/analytics/#similarity-and-differences-text-comparision-and-overrepresented-terms","text":"Please donate: Show differences and focal points, core areas and key aspects by comparing word frequencies to find out what concepts or entities are overrepresented in documents in comparison to other documents or text corpus. Compare two texts / versions to show differences or same/copied passages. Connected words / Co-occurrences (N-Grams like Bigrams or Trigrams) Search with a whole document or text as a search query: Find similar text or documents about the same topics by klicking on \"more of that\".","title":"Similarity and differences (text comparision and overrepresented terms)"},{"location":"doc/analytics/email/","text":"Emails and file attachments in Outlook mailbox files (PST) Extract mailbox files mail by mail The plugin Enhancer Outlook mailbox will index Microsoft Outlook mailboxes ( PST files) not only as one file but each included email for each (using the tool readpst ). How to index Microsoft Outlook mailbox files Just index the Microsoft Outlook file (.pst) like other files or documents: For example copy it to the documents directory of Open Semantic Desktop search and start \"Index documents\". Or use the \"Index for search\" in your file managers context menu. Or copy the file to a filemonitored directory of your file server. Or start a crawl of this file or its directory by Web user interface or command line tool. After some time of data analysis and indexing you can use fulltext search, filter and analyze included emails for each, not only the mailbox file itself as a whole. File attachments Mail attachments will be indexed, too. All data enrichment or data analysis plugins like OCR run for content of file attachments, too. So you can search, filter and analyse the content of mail attachments, too. Aggregated overview for analysis and Interactive filters Since the meta data like email-adresses will be available for each mail, you can use aggregated overviews and interactive filters (facets) like \"Message from\" or \"Message to\" for indexed emails to find or filter the email you search for faster. Network visualization for visual analysis So you can visualize the network with the graph view to overview who is connected and to see who wrote whom how often. Roadmap Next steps todo (please donate for further development ): Aggregated overview and interactive filter for domains So you can overview, analyze, filter and visualize which organization mailed with which how often, not only for special email-adresses or people.","title":"Emails and file attachments in Outlook mailbox files (PST)"},{"location":"doc/analytics/email/#emails-and-file-attachments-in-outlook-mailbox-files-pst","text":"","title":"Emails and file attachments in Outlook mailbox files (PST)"},{"location":"doc/analytics/email/#extract-mailbox-files-mail-by-mail","text":"The plugin Enhancer Outlook mailbox will index Microsoft Outlook mailboxes ( PST files) not only as one file but each included email for each (using the tool readpst ).","title":"Extract mailbox files mail by mail"},{"location":"doc/analytics/email/#how-to-index-microsoft-outlook-mailbox-files","text":"Just index the Microsoft Outlook file (.pst) like other files or documents: For example copy it to the documents directory of Open Semantic Desktop search and start \"Index documents\". Or use the \"Index for search\" in your file managers context menu. Or copy the file to a filemonitored directory of your file server. Or start a crawl of this file or its directory by Web user interface or command line tool. After some time of data analysis and indexing you can use fulltext search, filter and analyze included emails for each, not only the mailbox file itself as a whole.","title":"How to index Microsoft Outlook mailbox files"},{"location":"doc/analytics/email/#file-attachments","text":"Mail attachments will be indexed, too. All data enrichment or data analysis plugins like OCR run for content of file attachments, too. So you can search, filter and analyse the content of mail attachments, too.","title":"File attachments"},{"location":"doc/analytics/email/#aggregated-overview-for-analysis-and-interactive-filters","text":"Since the meta data like email-adresses will be available for each mail, you can use aggregated overviews and interactive filters (facets) like \"Message from\" or \"Message to\" for indexed emails to find or filter the email you search for faster. Network visualization for visual analysis So you can visualize the network with the graph view to overview who is connected and to see who wrote whom how often. Roadmap Next steps todo (please donate for further development ): Aggregated overview and interactive filter for domains So you can overview, analyze, filter and visualize which organization mailed with which how often, not only for special email-adresses or people.","title":"Aggregated overview for analysis and Interactive filters"},{"location":"doc/analytics/graph/","text":"title: Network analysis: Graph visualization for discovery and exploration of connections & relations authors: - Markus Mandalka Network analysis: Graph visualization for discovery and exploration of connections & relations Visualization of relations and connections between entities like persons or organizations within and across documents (co-occurrences of named entities) The graph/network analysis view shows you the direct and indirect relations, connections and networks between named entities like persons, organizations or main concepts which occur together (co-occurences) in your content, datasources and documents or are connected in your Linked Data Knowledge Graph. Color map and statistics of entity types/classes The statistics and color map in the bar shows you how many entities of each class are loaded to the graph visualization and which colors they have in the graph visualization. Size of entities/nodes: How many documents contain the entity The size of the entity circle / node visualizes how many documents contain this entity. Size of connections / edges: Co-occurences of connected entities in documents The edge / connection width is visualizing in how many documents both connected entities occur together. Extend: Load more entities & connections The graph shows you some first entities, which are extracted from documents matching your search query or connected to the entity you started to search for/from. On this base you can extend the graph by additional connections and entities: By a click on a entity or node more connections from / to this entity from / to other named entities (maybe yet not in the graph visualization) are loaded. Document based graph exploration by augmentation of connections with documents A click on a connection / edge with the connection type / property \"Documents (co-occurence)\" shows you in how many and which documents the connected entities occur together. In the tab \"List\" you see a list of documents in which both connected entities occur. In the tab \"Preview\" you can preview the full text and metadata of a single document. In the tab \"Entities\" you get an aggregated overview of other named entities within this documents or by other options in the sub menu \"Analyze\" you can analyze or filter this documents . Graph visualization and graph analysis by full-text search You can start a graph visualization from full text search (not only for entities, but for keywords in your documents, too) in the search user interface. Therefore do a full text search and click on the view \"Connections (Graph)\" in tab/menu analysis. There you can set which types (classes) of entities and connections (properties) to use for graph analysis and graph visualization and with how many of each entities to start with. By click on the button \"Visualize graph\" you open the graph explorer and can extend the graph by clicking an entity / node. Thesaurus or Ontology based entity graph exploration by concepts (SKOS & RDF) If you set up a thesaurus or an ontology, the linked concepts of your thesaurus or the selected ontology are shown in the graph, too, so additionally to occuring entities like people, organizations and locations you can explore by concepts from your thesaurus in the content, too. Limitations This is a first alpha release with some bugs and limitations. There is not an user interface control element for setting limits how many connections should be loaded at once, which can cause problems if you enable entities from Named Entity Recognition by machine learning (since that results in many false positives) or using very generic thesauri. In future releases there will be a sepraration of properties for Named Entitiy Recognition by Machine Learning and Named Entity Extraction by your thesaurus or tags and an user interface element for showing and setting custom limits for extending single entities / loading their connections. Automatic extraction of named entities from documents (Named Entity Extraction) Open Semantic Search or Open Semantic ETL extracts named entities defined in your thesaurus, ontologies or lists of names or recognized by Named Entity Recognition by machine learning automatically. Named entities like persons, organizations or places Extracted named entities like persons, organizations or locations (Named entity extraction) are used for structured navigation, aggregated overviews and interactive filters (faceted search) and to be able to get leads for connections and networks because you can analyze which persons, organizations or places occor together in how many documents. Automatic Named Entity Recognition by machine learning (ML) for automatic classification and annotation of text parts Additionally to known named entities in a thesaurus or imported ontologies other data analysis plugins integrate Named Entity Recognition (NER) by spaCy and/or Stanford Named Entities Recognizer (Stanford NER) . Named Entity Extraction of yet unknown entities or names So by integration of machine learning for analysing the structure of the text and classifying parts/words of the sentences to categories like person, location or organization, many yet unknown named entities can be extracted, which aren't configured or listed yet in the thesaurus or a list of names or ontology. Therefore it uses models trained with existing annotations of a large text corpus, so after that they can \"predict\" or better: guess by probability if a part of a sentence is a name of a person, a name of an organization, a verb or a place. Find more by combination with thesaurus and ontologies Since no machine learning algorithm and machine learning model is perfect, the search engine combines the analysis with other methods and data which is curated by human editors. Therefore you can add important names, aliases and alternate labels to the thesaurus , so the search engine will extract them even if the named entities recognition fails. You don't have to add each name yourself: By the ontologies manager you can import thousands of names from Open Data like Wikidata which offers an universal structured database with names of people like for example lists of names of politicians and members of parliament(s) . Improve OCR results Additional entities in the thesaurus are added to the OCR dictionary and so they are found better in scanned documents by the automatic OCR integration for example for images of scanned pages of legacy documents within PDF files. Manual tagging and annotation Since no automatic analysis and automatic tagging or annotation is perfect you can tag manually documents by the semantic tagger or annotate visual parts/words/names/paragraphs/senteces within documents by Hypothesis annotator . Open Source tool, web app and web user interface (UI) for discovery, exploration and visualization of a graph The integrated Open Source software Open Semantic Visual Linked Data Knowledge Graph Explorer is a web app providing user interfaces (UI) to discover, explore and visualize linked data in a graph for visualization and exploration of direct and indirect connections between entities like people, organizations and locations in your Linked Data Knowledge Graph. Neo4J browser Alternate you can use the Neo4j browser by Cypher graph query language: Therefore enable the Open Source ETL plugin for integration with the Neo4j graph database in the config.","title":"Index"},{"location":"doc/analytics/graph/#network-analysis-graph-visualization-for-discovery-and-exploration-of-connections-relations","text":"","title":"Network analysis: Graph visualization for discovery and exploration of connections &amp; relations"},{"location":"doc/analytics/graph/#visualization-of-relations-and-connections-between-entities-like-persons-or-organizations-within-and-across-documents-co-occurrences-of-named-entities","text":"The graph/network analysis view shows you the direct and indirect relations, connections and networks between named entities like persons, organizations or main concepts which occur together (co-occurences) in your content, datasources and documents or are connected in your Linked Data Knowledge Graph.","title":"Visualization of relations and connections between entities like persons or organizations within and across documents (co-occurrences of named entities)"},{"location":"doc/analytics/graph/#color-map-and-statistics-of-entity-typesclasses","text":"The statistics and color map in the bar shows you how many entities of each class are loaded to the graph visualization and which colors they have in the graph visualization.","title":"Color map and statistics of entity types/classes"},{"location":"doc/analytics/graph/#size-of-entitiesnodes-how-many-documents-contain-the-entity","text":"The size of the entity circle / node visualizes how many documents contain this entity.","title":"Size of entities/nodes: How many documents contain the entity"},{"location":"doc/analytics/graph/#size-of-connections-edges-co-occurences-of-connected-entities-in-documents","text":"The edge / connection width is visualizing in how many documents both connected entities occur together.","title":"Size of connections / edges: Co-occurences of connected entities in documents"},{"location":"doc/analytics/graph/#extend-load-more-entities-connections","text":"The graph shows you some first entities, which are extracted from documents matching your search query or connected to the entity you started to search for/from. On this base you can extend the graph by additional connections and entities: By a click on a entity or node more connections from / to this entity from / to other named entities (maybe yet not in the graph visualization) are loaded.","title":"Extend: Load more entities &amp; connections"},{"location":"doc/analytics/graph/#document-based-graph-exploration-by-augmentation-of-connections-with-documents","text":"A click on a connection / edge with the connection type / property \"Documents (co-occurence)\" shows you in how many and which documents the connected entities occur together. In the tab \"List\" you see a list of documents in which both connected entities occur. In the tab \"Preview\" you can preview the full text and metadata of a single document. In the tab \"Entities\" you get an aggregated overview of other named entities within this documents or by other options in the sub menu \"Analyze\" you can analyze or filter this documents .","title":"Document based graph exploration by augmentation of connections with documents"},{"location":"doc/analytics/graph/#graph-visualization-and-graph-analysis-by-full-text-search","text":"You can start a graph visualization from full text search (not only for entities, but for keywords in your documents, too) in the search user interface. Therefore do a full text search and click on the view \"Connections (Graph)\" in tab/menu analysis. There you can set which types (classes) of entities and connections (properties) to use for graph analysis and graph visualization and with how many of each entities to start with. By click on the button \"Visualize graph\" you open the graph explorer and can extend the graph by clicking an entity / node.","title":"Graph visualization and graph analysis by full-text search"},{"location":"doc/analytics/graph/#thesaurus-or-ontology-based-entity-graph-exploration-by-concepts-skos-rdf","text":"If you set up a thesaurus or an ontology, the linked concepts of your thesaurus or the selected ontology are shown in the graph, too, so additionally to occuring entities like people, organizations and locations you can explore by concepts from your thesaurus in the content, too.","title":"Thesaurus or Ontology based entity graph exploration by concepts (SKOS &amp; RDF)"},{"location":"doc/analytics/graph/#limitations","text":"This is a first alpha release with some bugs and limitations. There is not an user interface control element for setting limits how many connections should be loaded at once, which can cause problems if you enable entities from Named Entity Recognition by machine learning (since that results in many false positives) or using very generic thesauri. In future releases there will be a sepraration of properties for Named Entitiy Recognition by Machine Learning and Named Entity Extraction by your thesaurus or tags and an user interface element for showing and setting custom limits for extending single entities / loading their connections.","title":"Limitations"},{"location":"doc/analytics/graph/#automatic-extraction-of-named-entities-from-documents-named-entity-extraction","text":"Open Semantic Search or Open Semantic ETL extracts named entities defined in your thesaurus, ontologies or lists of names or recognized by Named Entity Recognition by machine learning automatically.","title":"Automatic extraction of named entities from documents (Named Entity Extraction)"},{"location":"doc/analytics/graph/#named-entities-like-persons-organizations-or-places","text":"Extracted named entities like persons, organizations or locations (Named entity extraction) are used for structured navigation, aggregated overviews and interactive filters (faceted search) and to be able to get leads for connections and networks because you can analyze which persons, organizations or places occor together in how many documents.","title":"Named entities like persons, organizations or places"},{"location":"doc/analytics/graph/#automatic-named-entity-recognition-by-machine-learning-ml-for-automatic-classification-and-annotation-of-text-parts","text":"Additionally to known named entities in a thesaurus or imported ontologies other data analysis plugins integrate Named Entity Recognition (NER) by spaCy and/or Stanford Named Entities Recognizer (Stanford NER) .","title":"Automatic Named Entity Recognition by machine learning (ML) for automatic classification and annotation of text parts"},{"location":"doc/analytics/graph/#named-entity-extraction-of-yet-unknown-entities-or-names","text":"So by integration of machine learning for analysing the structure of the text and classifying parts/words of the sentences to categories like person, location or organization, many yet unknown named entities can be extracted, which aren't configured or listed yet in the thesaurus or a list of names or ontology. Therefore it uses models trained with existing annotations of a large text corpus, so after that they can \"predict\" or better: guess by probability if a part of a sentence is a name of a person, a name of an organization, a verb or a place.","title":"Named Entity Extraction of yet unknown entities or names"},{"location":"doc/analytics/graph/#find-more-by-combination-with-thesaurus-and-ontologies","text":"Since no machine learning algorithm and machine learning model is perfect, the search engine combines the analysis with other methods and data which is curated by human editors. Therefore you can add important names, aliases and alternate labels to the thesaurus , so the search engine will extract them even if the named entities recognition fails. You don't have to add each name yourself: By the ontologies manager you can import thousands of names from Open Data like Wikidata which offers an universal structured database with names of people like for example lists of names of politicians and members of parliament(s) .","title":"Find more by combination with thesaurus and ontologies"},{"location":"doc/analytics/graph/#improve-ocr-results","text":"Additional entities in the thesaurus are added to the OCR dictionary and so they are found better in scanned documents by the automatic OCR integration for example for images of scanned pages of legacy documents within PDF files.","title":"Improve OCR results"},{"location":"doc/analytics/graph/#manual-tagging-and-annotation","text":"Since no automatic analysis and automatic tagging or annotation is perfect you can tag manually documents by the semantic tagger or annotate visual parts/words/names/paragraphs/senteces within documents by Hypothesis annotator .","title":"Manual tagging and annotation"},{"location":"doc/analytics/graph/#open-source-tool-web-app-and-web-user-interface-ui-for-discovery-exploration-and-visualization-of-a-graph","text":"The integrated Open Source software Open Semantic Visual Linked Data Knowledge Graph Explorer is a web app providing user interfaces (UI) to discover, explore and visualize linked data in a graph for visualization and exploration of direct and indirect connections between entities like people, organizations and locations in your Linked Data Knowledge Graph.","title":"Open Source tool, web app and web user interface (UI) for discovery, exploration and visualization of a graph"},{"location":"doc/analytics/graph/#neo4j-browser","text":"Alternate you can use the Neo4j browser by Cypher graph query language: Therefore enable the Open Source ETL plugin for integration with the Neo4j graph database in the config.","title":"Neo4J browser"},{"location":"doc/analytics/law/","text":"Automatic extraction of law codes and law clauses How to extract, analyze, sort, structure and filter your documents and large document collections by legal codes and law clauses with free Open Source natural language processing (NLP) tools and Open Data legal thesaurus extracted and imported from Wikidata knowledge graph . Aggregated overviews and interactive filters (Faceted search) for your documents by legal codes and law clauses Open Semantic Search provides law code extraction, analysis, aggregated overviews and interactive filters (Faceted search) for your documents by extracted law codes and law clauses out of the box. This is based on analysis results of specialized Open Source ETL plugin for analysis of law clauses by extraction of text patterns by regular expressions (regex) and law codes by dictionary based named entity extraction and entity linking of an integrated legal ontology of law codes, a domain knowledge SKOS thesaurus extracted from the open knowledge graph / ontology Wikidata (open data): Open Legal tech: How to extract legal codes and law clauses by Open Source NLP tools and Open Data from Wikidata This documentation describes how this legal codes and law clause extraction works in detail and how you can blacklist custom law code labels or aliases, which are too ambiguous in your documents context. Open Source ETL plugin for textmining of structured legal data from full text of documents The Open Semantic ETL (extracts documents full text to more structured data and search index) open source plugin \"extract law\" is based on and combines multiple natural language processing (NLP) and textmining methods to extract, aggregate and normalize law codes and law clauses in multiple variants from the full texts of your documents: Extract law clauses by Regex (text pattern based extraction) One main step is to extract law clauses like \u00a7 123 from the document text with a Regular expression for the Python Regex library re . Extract law codes by thesaurus (dictionary based named entity extraction) Another main step is to extract law code labels (f.e. \"Civil code\", \"Strafgesetzbuch\" or \"B\u00fcrgerliches Gesetzbuch\" or their aliases like \"STGB\" or \"BGB\") by our integrated law codes thesaurus. This Simple Knowledge Organization System (SKOS) standard based thesaurus of law codes is an export of results of a Wikidata SPARQL query for labels, synonyms and alternate labels of law codes extracted from the Wikidata knowledge graph / ontology. Since this SKOS thesaurus of law codes is preconfigured in Ontology Manager (Python/Django UI for import data from SPARQL/RDF to Solr Texttagger) of the semantic search engine, this step was yet done by the ETL plugin for dictionary/thesaurus/ontology based extraction of labels of entities from full text by OpenRefine Reconciliation API Standard based Open Semantic Entity Search API , Solr providing a REST-API for managing this entities with labels and entity IDs/URIs in a Solr index and Solr text tagger , a Finite State Transducer (FST) based dictionary matcher REST-API to extract this indexed labels / substrings from full text. Open data from Wikidata knowledge graph For getting the open data list / thesaurus of law code labels and their alternate labels/aliases we query, extract and import Open Data of entities from Wikipedia from the the free universal knowledge graph Wikidata by the following SPARQL query returning RDFS labels and SKOS thesaurus alternate labels: PREFIX skos: <http://www.w3.org/2004/02/skos/core#> CONSTRUCT { ?uri rdfs:label ?label ; skos:prefLabel ?prefLabel ; skos:altLabel ?altLabel . } WHERE { ?uri p:P31/ps:P31/wdt:P279* wd:Q922203. OPTIONAL { ?uri rdfs:label ?label . } OPTIONAL { ?uri skos:prefLabel ?prefLabel . } OPTIONAL { ?uri skos:altLabel ?altLabel . } } So we get relevant law code labels like \"Strafgesetzbuch\" (German penal code) and aliases or alternate labels like \"STGB\" which is an abbreviation for \"Strafgesetzbuch\" (German penal code). Combine law codes and law clauses to law code clauses As last main step both approaches are combined: The law plugin checks if there are combinations of extracted law codes with extracted law clauses, so we get law code clauses like \"\u00a7123 civil code\" for the facet \"Law code clause\". Normalization of law codes Alternate names are normalized to the preferred label of the entity / law code, so f.e. \u00a7123 STGB in a text will be added to the interactive filter in a normalized form \"\u00a7123 Strafgesetzbuch\", so if you overview or filter by the facet/interactive filter like \"Law code\", you will get documents with all variants. Reduce false positives Some alternate names of law codes are too general or too ambiguous. For example the alias \"CC\" of the law code \"Civil code\" is used with another meaning in many other contexts (f.e. in many emails as abbreviation of \"carbon copy (CC)\" function). So this alias CC is listed in the blacklist /etc/opensemanticsearch/blacklist/enhance_extract_law/blacklist-lawcode-if-no-clause so the law code \"Civil code\" will be only added to the facet \"Law codes\" if the occuring alias CC is used combined with a law clause like \u00a7 123 CC or CC \u00a7 123 but not if only the alias \"CC\" is in the text, while if the preferred label \"Civil code\" is in the text without such a law clause, it will be added to the facet \"Law codes\", because this label is not blacklisted.","title":"Legal codes and law clauses"},{"location":"doc/analytics/law/#automatic-extraction-of-law-codes-and-law-clauses","text":"How to extract, analyze, sort, structure and filter your documents and large document collections by legal codes and law clauses with free Open Source natural language processing (NLP) tools and Open Data legal thesaurus extracted and imported from Wikidata knowledge graph .","title":"Automatic extraction of law codes and law clauses"},{"location":"doc/analytics/law/#aggregated-overviews-and-interactive-filters-faceted-search-for-your-documents-by-legal-codes-and-law-clauses","text":"Open Semantic Search provides law code extraction, analysis, aggregated overviews and interactive filters (Faceted search) for your documents by extracted law codes and law clauses out of the box. This is based on analysis results of specialized Open Source ETL plugin for analysis of law clauses by extraction of text patterns by regular expressions (regex) and law codes by dictionary based named entity extraction and entity linking of an integrated legal ontology of law codes, a domain knowledge SKOS thesaurus extracted from the open knowledge graph / ontology Wikidata (open data):","title":"Aggregated overviews and interactive filters (Faceted search) for your documents by legal codes and law clauses"},{"location":"doc/analytics/law/#open-legal-tech-how-to-extract-legal-codes-and-law-clauses-by-open-source-nlp-tools-and-open-data-from-wikidata","text":"This documentation describes how this legal codes and law clause extraction works in detail and how you can blacklist custom law code labels or aliases, which are too ambiguous in your documents context.","title":"Open Legal tech: How to extract legal codes and law clauses by Open Source NLP tools and Open Data from Wikidata"},{"location":"doc/analytics/law/#open-source-etl-plugin-for-textmining-of-structured-legal-data-from-full-text-of-documents","text":"The Open Semantic ETL (extracts documents full text to more structured data and search index) open source plugin \"extract law\" is based on and combines multiple natural language processing (NLP) and textmining methods to extract, aggregate and normalize law codes and law clauses in multiple variants from the full texts of your documents:","title":"Open Source ETL plugin for textmining of structured legal data from full text of documents"},{"location":"doc/analytics/law/#extract-law-clauses-by-regex-text-pattern-based-extraction","text":"One main step is to extract law clauses like \u00a7 123 from the document text with a Regular expression for the Python Regex library re .","title":"Extract law clauses by Regex (text pattern based extraction)"},{"location":"doc/analytics/law/#extract-law-codes-by-thesaurus-dictionary-based-named-entity-extraction","text":"Another main step is to extract law code labels (f.e. \"Civil code\", \"Strafgesetzbuch\" or \"B\u00fcrgerliches Gesetzbuch\" or their aliases like \"STGB\" or \"BGB\") by our integrated law codes thesaurus. This Simple Knowledge Organization System (SKOS) standard based thesaurus of law codes is an export of results of a Wikidata SPARQL query for labels, synonyms and alternate labels of law codes extracted from the Wikidata knowledge graph / ontology. Since this SKOS thesaurus of law codes is preconfigured in Ontology Manager (Python/Django UI for import data from SPARQL/RDF to Solr Texttagger) of the semantic search engine, this step was yet done by the ETL plugin for dictionary/thesaurus/ontology based extraction of labels of entities from full text by OpenRefine Reconciliation API Standard based Open Semantic Entity Search API , Solr providing a REST-API for managing this entities with labels and entity IDs/URIs in a Solr index and Solr text tagger , a Finite State Transducer (FST) based dictionary matcher REST-API to extract this indexed labels / substrings from full text.","title":"Extract law codes by thesaurus (dictionary based named entity extraction)"},{"location":"doc/analytics/law/#open-data-from-wikidata-knowledge-graph","text":"For getting the open data list / thesaurus of law code labels and their alternate labels/aliases we query, extract and import Open Data of entities from Wikipedia from the the free universal knowledge graph Wikidata by the following SPARQL query returning RDFS labels and SKOS thesaurus alternate labels: PREFIX skos: <http://www.w3.org/2004/02/skos/core#> CONSTRUCT { ?uri rdfs:label ?label ; skos:prefLabel ?prefLabel ; skos:altLabel ?altLabel . } WHERE { ?uri p:P31/ps:P31/wdt:P279* wd:Q922203. OPTIONAL { ?uri rdfs:label ?label . } OPTIONAL { ?uri skos:prefLabel ?prefLabel . } OPTIONAL { ?uri skos:altLabel ?altLabel . } } So we get relevant law code labels like \"Strafgesetzbuch\" (German penal code) and aliases or alternate labels like \"STGB\" which is an abbreviation for \"Strafgesetzbuch\" (German penal code).","title":"Open data from Wikidata knowledge graph"},{"location":"doc/analytics/law/#combine-law-codes-and-law-clauses-to-law-code-clauses","text":"As last main step both approaches are combined: The law plugin checks if there are combinations of extracted law codes with extracted law clauses, so we get law code clauses like \"\u00a7123 civil code\" for the facet \"Law code clause\".","title":"Combine law codes and law clauses to law code clauses"},{"location":"doc/analytics/law/#normalization-of-law-codes","text":"Alternate names are normalized to the preferred label of the entity / law code, so f.e. \u00a7123 STGB in a text will be added to the interactive filter in a normalized form \"\u00a7123 Strafgesetzbuch\", so if you overview or filter by the facet/interactive filter like \"Law code\", you will get documents with all variants.","title":"Normalization of law codes"},{"location":"doc/analytics/law/#reduce-false-positives","text":"Some alternate names of law codes are too general or too ambiguous. For example the alias \"CC\" of the law code \"Civil code\" is used with another meaning in many other contexts (f.e. in many emails as abbreviation of \"carbon copy (CC)\" function). So this alias CC is listed in the blacklist /etc/opensemanticsearch/blacklist/enhance_extract_law/blacklist-lawcode-if-no-clause so the law code \"Civil code\" will be only added to the facet \"Law codes\" if the occuring alias CC is used combined with a law clause like \u00a7 123 CC or CC \u00a7 123 but not if only the alias \"CC\" is in the text, while if the preferred label \"Civil code\" is in the text without such a law clause, it will be added to the facet \"Law codes\", because this label is not blacklisted.","title":"Reduce false positives"},{"location":"doc/analytics/map/","text":"Interactive map of documents or search results Visualize locations occur in documents on Interactive map By the view / tab \"Map\" you can analyze visual which locations occur in the documents of your search results: Openstreetmap Open Data from Openstreetmap is used to show the map. The extraction and mapping of documents to locations (data enrichment of your documents by geodata) is done on your local computer, not by a cloud service. Add geodata to Thesaurus Therefore there will be an UI adding geodata to your locations of interest that you can manage with the thesaurus manager. Open Data for Geodata by ontologies with geo coordinates You don't have to manage each town of your country yourself: Thanks to open standards for the Semantic Web you can import open data ontologies like for example Wikidata location data like the towns of your country with geo coordinates to your linked data knowledge graph by ontologies manager . Open Source user interfaces out of the box Thanks to a client ordering this features as open source software, there will be an easy UI for setup that after the Open Semantic Entity Search API is released as open source. If you are interested to alpha & beta test such features before that, please write an email.","title":"Interactive map of documents or search results"},{"location":"doc/analytics/map/#interactive-map-of-documents-or-search-results","text":"","title":"Interactive map of documents or search results"},{"location":"doc/analytics/map/#visualize-locations-occur-in-documents-on-interactive-map","text":"By the view / tab \"Map\" you can analyze visual which locations occur in the documents of your search results:","title":"Visualize locations occur in documents on Interactive map"},{"location":"doc/analytics/map/#openstreetmap","text":"Open Data from Openstreetmap is used to show the map. The extraction and mapping of documents to locations (data enrichment of your documents by geodata) is done on your local computer, not by a cloud service.","title":"Openstreetmap"},{"location":"doc/analytics/map/#add-geodata-to-thesaurus","text":"Therefore there will be an UI adding geodata to your locations of interest that you can manage with the thesaurus manager.","title":"Add geodata to Thesaurus"},{"location":"doc/analytics/map/#open-data-for-geodata-by-ontologies-with-geo-coordinates","text":"You don't have to manage each town of your country yourself: Thanks to open standards for the Semantic Web you can import open data ontologies like for example Wikidata location data like the towns of your country with geo coordinates to your linked data knowledge graph by ontologies manager .","title":"Open Data for Geodata by ontologies with geo coordinates"},{"location":"doc/analytics/map/#open-source-user-interfaces-out-of-the-box","text":"Thanks to a client ordering this features as open source software, there will be an easy UI for setup that after the Open Semantic Entity Search API is released as open source. If you are interested to alpha & beta test such features before that, please write an email.","title":"Open Source user interfaces out of the box"},{"location":"doc/analytics/money/","text":"Automatic extraction of money and currencies Open Semantic Search extracts amounts of money from text to the facet \"Money\" and currencies to the facet \"Currencies\". For example text parts of your documents like \" 100 dollars \", \" 1,000$ \", \" $ 100.50 \" or \" 100 USD \" are extracted to the facet \"Money\". So you can overview (facets) and drill down / filter your search results to documents concerning money. Open Source ETL money extraction plugin For that purpose the Open Semantic ETL (extracts documents full text to more structured data and search index) open source plugin \"extract money\" is based on and combines multiple natural language processing (NLP) and textmining methods like ontology / SKOS thesaurus / dictionary based entity extraction and regular expressions (regex) for extraction of numbers with (fully integrated) open data from the free knowledge graph Wikidata to extract money amounts and currencies in multiple variants from the full texts of your documents. So by this dictionary- and text-pattern (Regex)-based approach it often extracts additional structured information concerning money where the machine learning approach by Named Entity Recognition (NER) failed on prediction of the NER class money . Currency signs The plugin is configured with currency signs like for example the Dollar currency sign $ or the Euro currency sign \u20ac . Currency labels Another main approach is the extraction of currency labels (f.e. \"Dollar\", \"Euro\" and their aliases like \"US Dollar\", \"USD\" or \"EUR\") by our integrated currencies thesaurus. This Simple Knowledge Organization System (SKOS) standard based thesaurus of currencies is an export of results of a Wikidata SPARQL query for labels, synonyms and alternate labels of currencies extracted from the Wikidata knowledge graph / ontology. Extract currency labels and aliases by thesaurus (dictionary based named entity extraction) Since this SKOS thesaurus of currencies is preconfigured in Ontology Manager (Python/Django UI for import data from SPARQL/RDF to Solr Texttagger) of the semantic search engine, this step was yet done by the ETL plugin for dictionary/thesaurus/ontology based extraction of labels of entities from full text by OpenRefine Reconciliation API Standard based Open Semantic Entity Search API , Solr providing a REST-API for managing these entities with labels and entity IDs/URIs in a Solr index and Solr text tagger , a Finite State Transducer (FST) based dictionary matcher REST-API to extract this indexed labels / substrings from full text. Knowledge Graph based on Open Data from Wikidata The used currency ontology includes different currency labels and aliases like \"US dollar\", \"EUR\" or \"Euros\": For getting the open data list / thesaurus of currency labels and their alternate labels/aliases we query, extract and import Open Data of entities from Wikipedia from the the free universal knowledge graph Wikidata by the following SPARQL query returning RDFS labels and SKOS thesaurus alternate labels: SPARQL query Get labels from all instances of type currency (Wikidata Entity Q8142 ) PREFIX skos: <http://www.w3.org/2004/02/skos/core#> CONSTRUCT { ?uri rdfs:label ?label ; skos:prefLabel ?prefLabel ; skos:altLabel ?altLabel . } WHERE { ?uri p:P31/ps:P31/wdt:P279* wd:Q8142. OPTIONAL { ?uri rdfs:label ?label . } OPTIONAL { ?uri skos:prefLabel ?prefLabel . } OPTIONAL { ?uri skos:altLabel ?altLabel . } } Extraction combines numbers with currency signs or currency labels to facet money As last step all approaches are combined by a Regular expression which matches numbers which are combined with extracted currency label before or after a number (text pattern based extraction). So from this Regex matches we get money like \"100 dollars\" for the facet \"Money\".","title":"Automatic extraction of law codes and law clauses"},{"location":"doc/analytics/money/#automatic-extraction-of-money-and-currencies","text":"Open Semantic Search extracts amounts of money from text to the facet \"Money\" and currencies to the facet \"Currencies\". For example text parts of your documents like \" 100 dollars \", \" 1,000$ \", \" $ 100.50 \" or \" 100 USD \" are extracted to the facet \"Money\". So you can overview (facets) and drill down / filter your search results to documents concerning money.","title":"Automatic extraction of money and currencies"},{"location":"doc/analytics/money/#open-source-etl-money-extraction-plugin","text":"For that purpose the Open Semantic ETL (extracts documents full text to more structured data and search index) open source plugin \"extract money\" is based on and combines multiple natural language processing (NLP) and textmining methods like ontology / SKOS thesaurus / dictionary based entity extraction and regular expressions (regex) for extraction of numbers with (fully integrated) open data from the free knowledge graph Wikidata to extract money amounts and currencies in multiple variants from the full texts of your documents. So by this dictionary- and text-pattern (Regex)-based approach it often extracts additional structured information concerning money where the machine learning approach by Named Entity Recognition (NER) failed on prediction of the NER class money .","title":"Open Source ETL money extraction plugin"},{"location":"doc/analytics/money/#currency-signs","text":"The plugin is configured with currency signs like for example the Dollar currency sign $ or the Euro currency sign \u20ac .","title":"Currency signs"},{"location":"doc/analytics/money/#currency-labels","text":"Another main approach is the extraction of currency labels (f.e. \"Dollar\", \"Euro\" and their aliases like \"US Dollar\", \"USD\" or \"EUR\") by our integrated currencies thesaurus. This Simple Knowledge Organization System (SKOS) standard based thesaurus of currencies is an export of results of a Wikidata SPARQL query for labels, synonyms and alternate labels of currencies extracted from the Wikidata knowledge graph / ontology.","title":"Currency labels"},{"location":"doc/analytics/money/#extract-currency-labels-and-aliases-by-thesaurus-dictionary-based-named-entity-extraction","text":"Since this SKOS thesaurus of currencies is preconfigured in Ontology Manager (Python/Django UI for import data from SPARQL/RDF to Solr Texttagger) of the semantic search engine, this step was yet done by the ETL plugin for dictionary/thesaurus/ontology based extraction of labels of entities from full text by OpenRefine Reconciliation API Standard based Open Semantic Entity Search API , Solr providing a REST-API for managing these entities with labels and entity IDs/URIs in a Solr index and Solr text tagger , a Finite State Transducer (FST) based dictionary matcher REST-API to extract this indexed labels / substrings from full text.","title":"Extract currency labels and aliases by thesaurus (dictionary based named entity extraction)"},{"location":"doc/analytics/money/#knowledge-graph-based-on-open-data-from-wikidata","text":"The used currency ontology includes different currency labels and aliases like \"US dollar\", \"EUR\" or \"Euros\": For getting the open data list / thesaurus of currency labels and their alternate labels/aliases we query, extract and import Open Data of entities from Wikipedia from the the free universal knowledge graph Wikidata by the following SPARQL query returning RDFS labels and SKOS thesaurus alternate labels:","title":"Knowledge Graph based on Open Data from Wikidata"},{"location":"doc/analytics/money/#sparql-query","text":"Get labels from all instances of type currency (Wikidata Entity Q8142 ) PREFIX skos: <http://www.w3.org/2004/02/skos/core#> CONSTRUCT { ?uri rdfs:label ?label ; skos:prefLabel ?prefLabel ; skos:altLabel ?altLabel . } WHERE { ?uri p:P31/ps:P31/wdt:P279* wd:Q8142. OPTIONAL { ?uri rdfs:label ?label . } OPTIONAL { ?uri skos:prefLabel ?prefLabel . } OPTIONAL { ?uri skos:altLabel ?altLabel . } }","title":"SPARQL query"},{"location":"doc/analytics/money/#extraction-combines-numbers-with-currency-signs-or-currency-labels-to-facet-money","text":"As last step all approaches are combined by a Regular expression which matches numbers which are combined with extracted currency label before or after a number (text pattern based extraction). So from this Regex matches we get money like \"100 dollars\" for the facet \"Money\".","title":"Extraction combines numbers with currency signs or currency labels to facet money"},{"location":"doc/analytics/textmining/","text":"Text mining, text analytics and content analysis Text data mining (TDM) by text analysis, information extraction, document mining, text comparison, text visualization and topic modelling The search engine extracts automatically texts of different file formats and uses grammar rules (stemming) to index and find different word forms. On this base and index you can search, review, filter, analyze and mine content with different text mining, analysis, extraction, data mining and clustering methods. So you can use the search engine not only for information retrieval by full text search to search and find known issues or to get structured data from unstructured data sources or texts by information extraction. It can be used as integrated text mining toolbox for text datamining (TDM) for semi-automated or automated text analysis, document mining, text comparision, text visualization and topic modelling to get useful analysis results even of unknown data sources. Search and filter the interesting documents If you don't want to analyze all indexed documents, you can search and filter the context you want to mine and analyze. Words: Word list and word cloud The view Words (option of the tab/button Analyze ) shows you the words which are contained in the most documents of the results of your search context (documents matching your search query and filters). If you do not enter a search query and don't use a filters it shows the words which are contained in the most of all indexed documents. The number shows you how many documents (matching your search query and filters or if no search query or filter of all documents) use this word. If you click on a word, this word will be added as an additional filter. The words are visualiszed as a word cloud. The more documents containing the word, the bigger it is in the visualization Aggregated overviews of extracted structured informations, named entities and concepts for exploratory search (thesaurus based, ontonologies based and machine learning for automatic classification based faceted search) With the faceted search you can see an aggregated overview for the different facets like paths, concepts, persons, locations or organzations showing, how many documents matching the named entities. This structure will be generated and facets/fields are valued with data from the following analysis: Lists of Named Entities: Listed known named entities like organizations, persons, locations or concepts. They can be [managed in plaintext lists, databases, ontologies, thesauri or in the [thesaurus user interface](../../datamanagement/thesaurus) for dictionary based or thesaurus based text mining and thesaurus based faceted search](../../datamanagement/ontologies) Annotation & Tagging: Tags from (collaborative) annotations and tagging Text patterns (Regular Expressions): Extraction of structured data or data enrichment with text patterns (regular expressions) can extract informations like email-adresses or amounts of money. They are added to facets like Email adresses, From:, To: or money. Named entity extraction or Named entity recognition (NER) of even yet unknown entities like persons, organizations or locations by automatic classification of this text parts by machine learning on an annotated training corpus model . Topic modelling (clustering and differences) Please donate so we can implement this sooner): Topic modelling (clusters of topics what about documents are) What are the contents about? What are the most common topics in the whole, selected or filtered document set? Coocuration (Connected words): Which words occure together (Bigrams/Trigrams/N-Grams)? What is special in comparision with another text or document set ? See \"Compare text or part of the corpus with other text or part of corpus\". Similarity (\"more like this\") Please donate so we can implement this sooner): Search with a whole document or text as a search query: If not yet, index your document which should be used as search query. Search for that document (i.e. by filename). Find similar text or documents about the same topics by clicking on \"more like this\". Direct text comparision: Differences of two text versions (visualization of added, deleted or copy pasted parts) Compare two texts / versions to show differences or same/copied passages or deleted or added words. Please donate so we can implement this sooner: Document set comparision (show differences like overrepresented terms) Please donate so we can implement this sooner: Special focus of a text or document set (text corpora) by comparision with other text or document set (text corpora). Show differences and focal points, core areas and key aspects by comparing word frequencies to find out what concepts or entities are overrepresented in documents in comparison to other documents or text corpus. Extract text patterns with Regular Expressions (RegEx) You can extract some structured data i.e. for aggregated overviews, interactive navigation and interactive filters (faceted search), data analysis and data visualization from unstructured text by extraction of the interesting text parts to structured flields, properties or facets by defining text patterns with regular expressions (RegEx) or own regular expressions based enhancer plugins Advanced text analysis, text mining, document mining and text visualizations Advanced features like clustering and network analysis and advanced visualizations need more CPU load, more parameters and knowledge and specialized tools for different analysis, so you have to start them manually for your documents or for special search context. But many advanced text mining tools support only few document formats and data formats and do not optical character recognition (OCR) automatically. Since this free software is interoperable open source software and uses open standards you are free to integrate additional data enrichment or data analysis plugins or to use other specialized tools additionally and based on the (exportable) text extraction, data enrichment, search and filter results of the search engine. How to explore and analyse a document collection with external text mining tools? After automatic extracting, indexing, analysis (i.e. optical character recognition by OCR engines) and enriching (i.e. with Named Entities or extraction of email-addresses) you can do an advanced text analysis, text mining and document mining with this special tools based on an export of all data or an export of search results or filtered results: Search and filter/drill down the interesting document set (or do not, if you want to analyze all documents) Export this search results to a CSV file. Select the interesting fields like id , title , persons, organzations and mainly the fields content and ocr_t Import the CSV in other open source text mining tools and use the extracted text data with natural language processing (NLP) or machine learning (ML), named entities recognition (NER) or classification libraries until some of its advances machine learning methods for text mining are integrated into the user interfaces Use their advanced features and views, for example different views from Jigsaw Free Software and Open Source text analytics and text mining toolkits and platforms or text mining solutions Alternate Free Software and Open Source text analytics and text mining toolkits or text mining platforms: Text mining platforms Gate - General architecture for text engineering Open source components for natural language processing (NLP), clustering and classification (machine learning) Open source frameworks & programming libraries or APIs for natural language processing (NLP), clustering and classification (machine learning): * Apache Solr (Java based REST-API) * Elastic search * Apache UIMA - Unstructured Information Management Architecture for information extraction * DKPro - Text mining framework (Java and UIMA) * OpenNLP - Command line tools and Java library * Python Natural Language Toolkit (NLTK) - Natural language processing library (Python) * Gensim - Topic modelling programming library (Python) * Mallet (Java) * Apache Mahout (Java) * Apache Spark (Java, but APIs for Pyton, too) * Apache Stanbol More: Text Analysis Portal for Research or in Wikipedia list of text mining software","title":"Text mining, text analytics and content analysis"},{"location":"doc/analytics/textmining/#text-mining-text-analytics-and-content-analysis","text":"","title":"Text mining, text analytics and content analysis"},{"location":"doc/analytics/textmining/#text-data-mining-tdm-by-text-analysis-information-extraction-document-mining-text-comparison-text-visualization-and-topic-modelling","text":"The search engine extracts automatically texts of different file formats and uses grammar rules (stemming) to index and find different word forms. On this base and index you can search, review, filter, analyze and mine content with different text mining, analysis, extraction, data mining and clustering methods. So you can use the search engine not only for information retrieval by full text search to search and find known issues or to get structured data from unstructured data sources or texts by information extraction. It can be used as integrated text mining toolbox for text datamining (TDM) for semi-automated or automated text analysis, document mining, text comparision, text visualization and topic modelling to get useful analysis results even of unknown data sources.","title":"Text data mining (TDM) by text analysis, information extraction, document mining, text comparison, text visualization and topic modelling"},{"location":"doc/analytics/textmining/#search-and-filter-the-interesting-documents","text":"If you don't want to analyze all indexed documents, you can search and filter the context you want to mine and analyze.","title":"Search and filter the interesting documents"},{"location":"doc/analytics/textmining/#words-word-list-and-word-cloud","text":"The view Words (option of the tab/button Analyze ) shows you the words which are contained in the most documents of the results of your search context (documents matching your search query and filters). If you do not enter a search query and don't use a filters it shows the words which are contained in the most of all indexed documents. The number shows you how many documents (matching your search query and filters or if no search query or filter of all documents) use this word. If you click on a word, this word will be added as an additional filter. The words are visualiszed as a word cloud. The more documents containing the word, the bigger it is in the visualization","title":"Words: Word list and word cloud"},{"location":"doc/analytics/textmining/#aggregated-overviews-of-extracted-structured-informations-named-entities-and-concepts-for-exploratory-search-thesaurus-based-ontonologies-based-and-machine-learning-for-automatic-classification-based-faceted-search","text":"With the faceted search you can see an aggregated overview for the different facets like paths, concepts, persons, locations or organzations showing, how many documents matching the named entities. This structure will be generated and facets/fields are valued with data from the following analysis: Lists of Named Entities: Listed known named entities like organizations, persons, locations or concepts. They can be [managed in plaintext lists, databases, ontologies, thesauri or in the [thesaurus user interface](../../datamanagement/thesaurus) for dictionary based or thesaurus based text mining and thesaurus based faceted search](../../datamanagement/ontologies) Annotation & Tagging: Tags from (collaborative) annotations and tagging Text patterns (Regular Expressions): Extraction of structured data or data enrichment with text patterns (regular expressions) can extract informations like email-adresses or amounts of money. They are added to facets like Email adresses, From:, To: or money. Named entity extraction or Named entity recognition (NER) of even yet unknown entities like persons, organizations or locations by automatic classification of this text parts by machine learning on an annotated training corpus model .","title":"Aggregated overviews of extracted structured informations, named entities and concepts for exploratory search (thesaurus based, ontonologies based and machine learning for automatic classification based faceted search)"},{"location":"doc/analytics/textmining/#topic-modelling-clustering-and-differences","text":"Please donate so we can implement this sooner): Topic modelling (clusters of topics what about documents are) What are the contents about? What are the most common topics in the whole, selected or filtered document set? Coocuration (Connected words): Which words occure together (Bigrams/Trigrams/N-Grams)? What is special in comparision with another text or document set ? See \"Compare text or part of the corpus with other text or part of corpus\".","title":"Topic modelling (clustering and differences)"},{"location":"doc/analytics/textmining/#similarity-more-like-this","text":"Please donate so we can implement this sooner): Search with a whole document or text as a search query: If not yet, index your document which should be used as search query. Search for that document (i.e. by filename). Find similar text or documents about the same topics by clicking on \"more like this\".","title":"Similarity (\"more like this\")"},{"location":"doc/analytics/textmining/#direct-text-comparision-differences-of-two-text-versions-visualization-of-added-deleted-or-copy-pasted-parts","text":"Compare two texts / versions to show differences or same/copied passages or deleted or added words. Please donate so we can implement this sooner:","title":"Direct text comparision: Differences of two text versions (visualization of added, deleted or copy pasted parts)"},{"location":"doc/analytics/textmining/#document-set-comparision-show-differences-like-overrepresented-terms","text":"Please donate so we can implement this sooner: Special focus of a text or document set (text corpora) by comparision with other text or document set (text corpora). Show differences and focal points, core areas and key aspects by comparing word frequencies to find out what concepts or entities are overrepresented in documents in comparison to other documents or text corpus.","title":"Document set comparision (show differences like overrepresented terms)"},{"location":"doc/analytics/textmining/#extract-text-patterns-with-regular-expressions-regex","text":"You can extract some structured data i.e. for aggregated overviews, interactive navigation and interactive filters (faceted search), data analysis and data visualization from unstructured text by extraction of the interesting text parts to structured flields, properties or facets by defining text patterns with regular expressions (RegEx) or own regular expressions based enhancer plugins","title":"Extract text patterns with Regular Expressions (RegEx)"},{"location":"doc/analytics/textmining/#advanced-text-analysis-text-mining-document-mining-and-text-visualizations","text":"Advanced features like clustering and network analysis and advanced visualizations need more CPU load, more parameters and knowledge and specialized tools for different analysis, so you have to start them manually for your documents or for special search context. But many advanced text mining tools support only few document formats and data formats and do not optical character recognition (OCR) automatically. Since this free software is interoperable open source software and uses open standards you are free to integrate additional data enrichment or data analysis plugins or to use other specialized tools additionally and based on the (exportable) text extraction, data enrichment, search and filter results of the search engine.","title":"Advanced text analysis, text mining, document mining and text visualizations"},{"location":"doc/analytics/textmining/#how-to-explore-and-analyse-a-document-collection-with-external-text-mining-tools","text":"After automatic extracting, indexing, analysis (i.e. optical character recognition by OCR engines) and enriching (i.e. with Named Entities or extraction of email-addresses) you can do an advanced text analysis, text mining and document mining with this special tools based on an export of all data or an export of search results or filtered results: Search and filter/drill down the interesting document set (or do not, if you want to analyze all documents) Export this search results to a CSV file. Select the interesting fields like id , title , persons, organzations and mainly the fields content and ocr_t Import the CSV in other open source text mining tools and use the extracted text data with natural language processing (NLP) or machine learning (ML), named entities recognition (NER) or classification libraries until some of its advances machine learning methods for text mining are integrated into the user interfaces Use their advanced features and views, for example different views from Jigsaw","title":"How to explore and analyse a document collection with external text mining tools?"},{"location":"doc/analytics/textmining/#free-software-and-open-source-text-analytics-and-text-mining-toolkits-and-platforms-or-text-mining-solutions","text":"Alternate Free Software and Open Source text analytics and text mining toolkits or text mining platforms:","title":"Free Software and Open Source text analytics and text mining toolkits and platforms or text mining solutions"},{"location":"doc/analytics/textmining/#text-mining-platforms","text":"Gate - General architecture for text engineering","title":"Text mining platforms"},{"location":"doc/analytics/textmining/#open-source-components-for-natural-language-processing-nlp-clustering-and-classification-machine-learning","text":"Open source frameworks & programming libraries or APIs for natural language processing (NLP), clustering and classification (machine learning): * Apache Solr (Java based REST-API) * Elastic search * Apache UIMA - Unstructured Information Management Architecture for information extraction * DKPro - Text mining framework (Java and UIMA) * OpenNLP - Command line tools and Java library * Python Natural Language Toolkit (NLTK) - Natural language processing library (Python) * Gensim - Topic modelling programming library (Python) * Mallet (Java) * Apache Mahout (Java) * Apache Spark (Java, but APIs for Pyton, too) * Apache Stanbol More: Text Analysis Portal for Research or in Wikipedia list of text mining software","title":"Open source components for natural language processing (NLP), clustering and classification (machine learning)"},{"location":"doc/analytics/trend/","text":"Trend chart Trend chart will show you many of the search results or matching documents occure when on which date like year, month, day or hour. Just click on the button Trend chart in the tab/button Analyze .","title":"Trend chart"},{"location":"doc/analytics/trend/#trend-chart","text":"Trend chart will show you many of the search results or matching documents occure when on which date like year, month, day or hour. Just click on the button Trend chart in the tab/button Analyze .","title":"Trend chart"},{"location":"doc/analytics/words/","text":"Word cloud and word list Words: Word list and word cloud The view Words (option of the tab/button Analyze ) shows you the words which are contained in the most documents of the results of your search context: The number shows you how many documents (matching your search query and filters or if no search query or filter of all documents) use this word. If you click on a word, this word will be added as an additional filter. The words are visualiszed as a word cloud. The more documents containing the word, the bigger it is in the visualization","title":"Word cloud and word list"},{"location":"doc/analytics/words/#word-cloud-and-word-list","text":"","title":"Word cloud and word list"},{"location":"doc/analytics/words/#words-word-list-and-word-cloud","text":"The view Words (option of the tab/button Analyze ) shows you the words which are contained in the most documents of the results of your search context: The number shows you how many documents (matching your search query and filters or if no search query or filter of all documents) use this word. If you click on a word, this word will be added as an additional filter. The words are visualiszed as a word cloud. The more documents containing the word, the bigger it is in the visualization","title":"Words: Word list and word cloud"},{"location":"doc/data_enrichment/","text":"title: Data enrichment & content enrichment: Merge data & combine data analysis & document analysis tools (Enhancer) authors: - Markus Mandalka Data enrichment & content enrichment: Merge data & combine data analysis & document analysis tools (Enhancer) Combining and merging multiple data sources & analytics of multiple content analysis and document analysis tools Data enrichment is done by modular document analysis, content analysis or data enrichment plugins managed with our lightweight, flexible, extendable and interoperable open source ETL (extract, transform, load) and data enrichment framework and toolset for document processing, automated content analysis and media analysis, merge and data enrichment pipelines . Document processing, data integration, content analysis, content enrichment and data enrichment pipeline (Enhancement chain) The document processing pipeline or chain is a list of data enrichment plugins (enhancer), which will be runned for each document to enrich, analyse or link them with additional data or analysis. A part of the default document processing pipeline configuration is for example: * Extract text (enhance-text) * OCR images (enhance-ocr) * Adding annotations and tags (enhance-rdf) * Indexing to database and/or search index (f.e. Solr or Elastic Search) Semantic data enrichment plugins Such modular data enrichment plugins (enhancer) will enhance or enrich the content with additional meta data or analytics. For example the plugins for named entities recognition or automatic text recognition (OCR) for image files . You can analyze your data with internal webservices (or if you don't need privacy with external webservices or \"the cloud\") or with your favorite programming language in a custom data enrichment plugin . Configuration of a custom document processing, content analysis and data enrichment pipeline The pipeline or the enabled plugins are configurated for all data sources in /etc/opensemanticsearch/connector or can be overwritten or extended for each data source or connector in their specialized configs like connector-files The analysis chain is running in order, since some plugins depend on data analysis of other plugins. You can add additional or new data enrichment plugins to config['plugins'] . Or you overwrite this config option to define a custom data enrichment pipeline with only a few needed plugins. Enrich parts, enrich later, add additional enrichments, update data enrichments or distributed data enrichment The tool opensemanticsearch-enrich can run data enrichment parts or plugins which are not enabled in your default document processing pipe later or from time to time. For example sometimes its better to index all documents without OCR in short time and after that to do the OCR of the documents with images which will need long time. So the users are able to search in most documents and text, not having to wait until only few parts and only for a few documents like some text in images are recognized in a long time process first before other documents after them were indexed, which takes only very few time, because there are no images. Or you can do expensive data enrichment like OCR at night or on low server load or distribute this work on different processors (parallel processing) or servers (cluster) or web services (cloud). Another possibility is to enrich with tools or webservices that imporoved or updated their results because of better analytics quality or more available data from time to time to integrate newer data or analytcs results. Or to enrich later with a additional webservice, without to have to run the full document processing chain again. Or if a webservice was not available while indexing to enrich data with its analytics later. You can run the tool from REST-API or on the command line: opensemanticsearch-enrich --plugins *pluginname* Optional you can add a search query or filter query, so only the interesting or important data or document(s) will get enriched: opensemanticsearch-enrich --plugins *pluginname* *query* Integration of other frameworks for data integration for data warehouse or for extraction, transformation and load (ETL) There are powerful open source ETL frameworks (extraction, transformation and load) for data integration, mapping, filtering and transformation for data warehousing with powerful features and graphical user interfaces (GUI). Linked Data with open Semantic Web Standards Modern Semantic Web and Linked Data standard formats help you to integrate linked open data to improve data analysis, navigation and filtering . Enrich your data with free databases and Open Data So you can enrich your data with many free knowledge bases like WikiData (the structured database of Wikipedia) to use its lists of names or structures for analytics or import a Wiktionary as a thesaurus to find more verb forms especially of irregular verbs or synonyms. Data integration and data enrichment frameworks Learn more about Semantic Web, Linked Data, data integration, data analysis and data enrichment frameworks .","title":"Index"},{"location":"doc/data_enrichment/#data-enrichment-content-enrichment-merge-data-combine-data-analysis-document-analysis-tools-enhancer","text":"","title":"Data enrichment &amp; content enrichment: Merge data &amp; combine data analysis &amp; document analysis tools (Enhancer)"},{"location":"doc/data_enrichment/#combining-and-merging-multiple-data-sources-analytics-of-multiple-content-analysis-and-document-analysis-tools","text":"Data enrichment is done by modular document analysis, content analysis or data enrichment plugins managed with our lightweight, flexible, extendable and interoperable open source ETL (extract, transform, load) and data enrichment framework and toolset for document processing, automated content analysis and media analysis, merge and data enrichment pipelines .","title":"Combining and merging multiple data sources &amp; analytics of multiple content analysis and document analysis tools"},{"location":"doc/data_enrichment/#document-processing-data-integration-content-analysis-content-enrichment-and-data-enrichment-pipeline-enhancement-chain","text":"The document processing pipeline or chain is a list of data enrichment plugins (enhancer), which will be runned for each document to enrich, analyse or link them with additional data or analysis. A part of the default document processing pipeline configuration is for example: * Extract text (enhance-text) * OCR images (enhance-ocr) * Adding annotations and tags (enhance-rdf) * Indexing to database and/or search index (f.e. Solr or Elastic Search)","title":"Document processing, data integration, content analysis, content enrichment and data enrichment pipeline (Enhancement chain)"},{"location":"doc/data_enrichment/#semantic-data-enrichment-plugins","text":"Such modular data enrichment plugins (enhancer) will enhance or enrich the content with additional meta data or analytics. For example the plugins for named entities recognition or automatic text recognition (OCR) for image files . You can analyze your data with internal webservices (or if you don't need privacy with external webservices or \"the cloud\") or with your favorite programming language in a custom data enrichment plugin .","title":"Semantic data enrichment plugins"},{"location":"doc/data_enrichment/#configuration-of-a-custom-document-processing-content-analysis-and-data-enrichment-pipeline","text":"The pipeline or the enabled plugins are configurated for all data sources in /etc/opensemanticsearch/connector or can be overwritten or extended for each data source or connector in their specialized configs like connector-files The analysis chain is running in order, since some plugins depend on data analysis of other plugins. You can add additional or new data enrichment plugins to config['plugins'] . Or you overwrite this config option to define a custom data enrichment pipeline with only a few needed plugins.","title":"Configuration of a custom document processing, content analysis and data enrichment pipeline"},{"location":"doc/data_enrichment/#enrich-parts-enrich-later-add-additional-enrichments-update-data-enrichments-or-distributed-data-enrichment","text":"The tool opensemanticsearch-enrich can run data enrichment parts or plugins which are not enabled in your default document processing pipe later or from time to time. For example sometimes its better to index all documents without OCR in short time and after that to do the OCR of the documents with images which will need long time. So the users are able to search in most documents and text, not having to wait until only few parts and only for a few documents like some text in images are recognized in a long time process first before other documents after them were indexed, which takes only very few time, because there are no images. Or you can do expensive data enrichment like OCR at night or on low server load or distribute this work on different processors (parallel processing) or servers (cluster) or web services (cloud). Another possibility is to enrich with tools or webservices that imporoved or updated their results because of better analytics quality or more available data from time to time to integrate newer data or analytcs results. Or to enrich later with a additional webservice, without to have to run the full document processing chain again. Or if a webservice was not available while indexing to enrich data with its analytics later. You can run the tool from REST-API or on the command line: opensemanticsearch-enrich --plugins *pluginname* Optional you can add a search query or filter query, so only the interesting or important data or document(s) will get enriched: opensemanticsearch-enrich --plugins *pluginname* *query*","title":"Enrich parts, enrich later, add additional enrichments, update data enrichments or distributed data enrichment"},{"location":"doc/data_enrichment/#integration-of-other-frameworks-for-data-integration-for-data-warehouse-or-for-extraction-transformation-and-load-etl","text":"There are powerful open source ETL frameworks (extraction, transformation and load) for data integration, mapping, filtering and transformation for data warehousing with powerful features and graphical user interfaces (GUI).","title":"Integration of other frameworks for data integration for data warehouse or for extraction, transformation and load (ETL)"},{"location":"doc/data_enrichment/#linked-data-with-open-semantic-web-standards","text":"Modern Semantic Web and Linked Data standard formats help you to integrate linked open data to improve data analysis, navigation and filtering .","title":"Linked Data with open Semantic Web Standards"},{"location":"doc/data_enrichment/#enrich-your-data-with-free-databases-and-open-data","text":"So you can enrich your data with many free knowledge bases like WikiData (the structured database of Wikipedia) to use its lists of names or structures for analytics or import a Wiktionary as a thesaurus to find more verb forms especially of irregular verbs or synonyms.","title":"Enrich your data with free databases and Open Data"},{"location":"doc/data_enrichment/#data-integration-and-data-enrichment-frameworks","text":"Learn more about Semantic Web, Linked Data, data integration, data analysis and data enrichment frameworks .","title":"Data integration and data enrichment frameworks"},{"location":"doc/datamanagement/","text":"title: Structuring data and documents: Metadata management of Names & Concepts by Lists, Vocabulary, Thesaurus and Ontologies and Named Entity Recognition authors: - Markus Mandalka Structuring data and documents: Metadata management of Names & Concepts by Lists, Vocabulary, Thesaurus and Ontologies and Named Entity Recognition Structure data, filters, navigation and aggregated overviews by Lists, Dictionaries, Vocabularies, Taxonomies, Mindmaps, Thesaurus (SKOS) and Graphs (Ontologies) for filtering, clustering and aggregating your documents You can structure, cluster and filter your data by different methods and structured data like lists of names or concepts (named entities) or ontologies based on named entities: Named Entities: Names of people, organizations, places or concepts Named Entities are for example of people of interests, organizations like companies, places like town names or important concepts or words. You can manage Named Entities (Names, Concepts, Persons, Places, Locations) name by name in the Thesaurus . Named entities recognition adds some unknown entities by machine learning. Container formats for lists of named entities, vocabularies or ontologies Multiple such named entities can be stored and organized in Dictionaries, Vocabularies, Databases, Lists or Ontologies So you can import external data sources with many named entities by the Lists, Vocabularies & Ontologies manager . Structure Based on such named entities or categories you can structure your documents with such names by the following methods: Categories, groups or lists (Classification / tagging / categorizing / classifying by being on a list or ontology , Tagging by rules and queries or clustering by machine learning) Hierarchies, Trees or Mindmaps (Taxonomies) Network or graph of concepts / words (Connected words and concepts in Thesaurus) Open standard format: Simple Knowledge Organization System (SKOS) Example data: Custom domain thesaurus or linked open data from Wiktionary Other domain specific or private Ontologies Open standard format: RDFS or OWL Manual tagging and annotation Tagging by machine learning (Automatic classification or clustering) Links (Connections) or Networks Rules Grammar rules and grammar heuristics (Stemming)","title":"Index"},{"location":"doc/datamanagement/#structuring-data-and-documents-metadata-management-of-names-concepts-by-lists-vocabulary-thesaurus-and-ontologies-and-named-entity-recognition","text":"","title":"Structuring data and documents: Metadata management of Names &amp; Concepts by Lists, Vocabulary, Thesaurus and Ontologies and Named Entity Recognition"},{"location":"doc/datamanagement/#structure-data-filters-navigation-and-aggregated-overviews-by-lists-dictionaries-vocabularies-taxonomies-mindmaps-thesaurus-skos-and-graphs-ontologies-for-filtering-clustering-and-aggregating-your-documents","text":"You can structure, cluster and filter your data by different methods and structured data like lists of names or concepts (named entities) or ontologies based on named entities:","title":"Structure data, filters, navigation and aggregated overviews by Lists, Dictionaries, Vocabularies, Taxonomies, Mindmaps, Thesaurus (SKOS) and Graphs (Ontologies) for filtering, clustering and aggregating your documents"},{"location":"doc/datamanagement/#named-entities-names-of-people-organizations-places-or-concepts","text":"Named Entities are for example of people of interests, organizations like companies, places like town names or important concepts or words. You can manage Named Entities (Names, Concepts, Persons, Places, Locations) name by name in the Thesaurus . Named entities recognition adds some unknown entities by machine learning.","title":"Named Entities: Names of people, organizations, places or concepts"},{"location":"doc/datamanagement/#container-formats-for-lists-of-named-entities-vocabularies-or-ontologies","text":"Multiple such named entities can be stored and organized in Dictionaries, Vocabularies, Databases, Lists or Ontologies So you can import external data sources with many named entities by the Lists, Vocabularies & Ontologies manager .","title":"Container formats for lists of named entities, vocabularies or ontologies"},{"location":"doc/datamanagement/#structure","text":"Based on such named entities or categories you can structure your documents with such names by the following methods: Categories, groups or lists (Classification / tagging / categorizing / classifying by being on a list or ontology , Tagging by rules and queries or clustering by machine learning) Hierarchies, Trees or Mindmaps (Taxonomies) Network or graph of concepts / words (Connected words and concepts in Thesaurus) Open standard format: Simple Knowledge Organization System (SKOS) Example data: Custom domain thesaurus or linked open data from Wiktionary Other domain specific or private Ontologies Open standard format: RDFS or OWL Manual tagging and annotation Tagging by machine learning (Automatic classification or clustering) Links (Connections) or Networks Rules Grammar rules and grammar heuristics (Stemming)","title":"Structure"},{"location":"doc/datamanagement/annotation/","text":"Annotation and tagging If a name and no of its aliases or a word and no of its synonyms is not contained in the document, that doesn't mean, that the document is not important for this topic. If you tag a document with a tag (f.e. a name of a person, organization, location or a concept or word), this document will be found on further queries with this word(s) even if this word(s) is/are not contained in the document or would be not recognized by automatic text analysis. Why (collaborative) tagging and (collaborative) annotations by (teams of) human editors? Even if text mining and text analytics technology like automatic named entity recognition of persons, organizations or places and interactive filters and automatic topic detection by thesaurus for content analysis is very helpful to automate much of the editors work, no automatic text analysis, OCR or machine learning model is perfect. Human knowledge better than artificial intelligence, OCR and automatic ranking algorithms So as specialists, domain experts or thinking human you and your team can add words, interpretations, qualitative values or structure that can not be analysed automatically because there is no data enrichment or automatic analysis plugin for that issue or its automatic analysis is not good enough, since computers are dump and can not understand or decide every context. Find, analyze and filter tagged documents by faceted search Additionally, tags will be used for faceted search and exploratory search like like aggregated overviews and interactive filters for navigation , too, so you get an overview and you can find and filter or drill down tagged documents faster and with more comfort. Your research team or users will find yet analysed and tagged documents despite codes, euphemism and lies If a name and no of its aliases or a word and no of its synonyms is not contained in the document, it doesn't mean, that the document is not important for the topic. For example corrupt companies or politicians wont write the word corruption into their documents, even if the document is a evidence of corruption. If you tag a document with a tag (f.e. a name of a person, organization, location or a concept or word), this document will be found on further queries with this word(s) even if this word(s) is/are not contained in the document. Connect knowledge Maybe you search for a connection of two special topics, for example for racism in an organization. Your search would find anything. Maybe a specialist tags a document with the tag racism because he knows, that a special code is for racism. Maybe another specialist has an idea about members of the organization and tags the document with the organization. Now you will find the document if you search for that organization in context with topic, even if the specialists added their knowledge for only one topic or domain without deeper knowledge about the other one. How to tag or annotate a document? Just click \"Tagging and annotation\" for this document in the search results to annotate this document. * Add tags (f.e. concepts, persons, organizations or locations) or write some free text into the notes field. * Save the annotation. This will trigger to index the document again, so some seconds later this document is findable and filterable in the tagged context(s), too. Visual annotations If you want to annotate visual and only parts within documents or web pages, you can use Hypothesis annotation tools for visual annotations and tagging . Automated tagging and automated annotation Such manually tagging by human editors, teams or groups is for documents that doesn't contain the tag (or another word or query or aliases) within the content, so that a human has to classify it to this additional context(s). If you want to tag documents i.e. with Tags like organizations, persons, locations or aliases that are in the text for sure without potential technical problems (works only if for sure if without OCR errors, typos or alternate and/or yet unknown labels or codes), you can add this tag or name to the Thesaurus or Named Entities Manager which will tag all documents containing the name, concept or query automatically and for new documents, too. Alternate annotation tools for semantic annotations You can integrate enhanced annotation tools for semantic annotations, which can not only tag whole documents but even parts of them: * Neonion * Pundit * Hypothesis * AnnotatorJS","title":"Tagging and annotation"},{"location":"doc/datamanagement/annotation/#annotation-and-tagging","text":"If a name and no of its aliases or a word and no of its synonyms is not contained in the document, that doesn't mean, that the document is not important for this topic. If you tag a document with a tag (f.e. a name of a person, organization, location or a concept or word), this document will be found on further queries with this word(s) even if this word(s) is/are not contained in the document or would be not recognized by automatic text analysis.","title":"Annotation and tagging"},{"location":"doc/datamanagement/annotation/#why-collaborative-tagging-and-collaborative-annotations-by-teams-of-human-editors","text":"Even if text mining and text analytics technology like automatic named entity recognition of persons, organizations or places and interactive filters and automatic topic detection by thesaurus for content analysis is very helpful to automate much of the editors work, no automatic text analysis, OCR or machine learning model is perfect.","title":"Why (collaborative) tagging and (collaborative) annotations by (teams of) human editors?"},{"location":"doc/datamanagement/annotation/#human-knowledge-better-than-artificial-intelligence-ocr-and-automatic-ranking-algorithms","text":"So as specialists, domain experts or thinking human you and your team can add words, interpretations, qualitative values or structure that can not be analysed automatically because there is no data enrichment or automatic analysis plugin for that issue or its automatic analysis is not good enough, since computers are dump and can not understand or decide every context.","title":"Human knowledge better than artificial intelligence, OCR and automatic ranking algorithms"},{"location":"doc/datamanagement/annotation/#find-analyze-and-filter-tagged-documents-by-faceted-search","text":"Additionally, tags will be used for faceted search and exploratory search like like aggregated overviews and interactive filters for navigation , too, so you get an overview and you can find and filter or drill down tagged documents faster and with more comfort.","title":"Find, analyze and filter tagged documents by faceted search"},{"location":"doc/datamanagement/annotation/#your-research-team-or-users-will-find-yet-analysed-and-tagged-documents-despite-codes-euphemism-and-lies","text":"If a name and no of its aliases or a word and no of its synonyms is not contained in the document, it doesn't mean, that the document is not important for the topic. For example corrupt companies or politicians wont write the word corruption into their documents, even if the document is a evidence of corruption. If you tag a document with a tag (f.e. a name of a person, organization, location or a concept or word), this document will be found on further queries with this word(s) even if this word(s) is/are not contained in the document.","title":"Your research team or users will find yet analysed and tagged documents despite codes, euphemism and lies"},{"location":"doc/datamanagement/annotation/#connect-knowledge","text":"Maybe you search for a connection of two special topics, for example for racism in an organization. Your search would find anything. Maybe a specialist tags a document with the tag racism because he knows, that a special code is for racism. Maybe another specialist has an idea about members of the organization and tags the document with the organization. Now you will find the document if you search for that organization in context with topic, even if the specialists added their knowledge for only one topic or domain without deeper knowledge about the other one.","title":"Connect knowledge"},{"location":"doc/datamanagement/annotation/#how-to-tag-or-annotate-a-document","text":"Just click \"Tagging and annotation\" for this document in the search results to annotate this document. * Add tags (f.e. concepts, persons, organizations or locations) or write some free text into the notes field. * Save the annotation. This will trigger to index the document again, so some seconds later this document is findable and filterable in the tagged context(s), too.","title":"How to tag or annotate a document?"},{"location":"doc/datamanagement/annotation/#visual-annotations","text":"If you want to annotate visual and only parts within documents or web pages, you can use Hypothesis annotation tools for visual annotations and tagging .","title":"Visual annotations"},{"location":"doc/datamanagement/annotation/#automated-tagging-and-automated-annotation","text":"Such manually tagging by human editors, teams or groups is for documents that doesn't contain the tag (or another word or query or aliases) within the content, so that a human has to classify it to this additional context(s). If you want to tag documents i.e. with Tags like organizations, persons, locations or aliases that are in the text for sure without potential technical problems (works only if for sure if without OCR errors, typos or alternate and/or yet unknown labels or codes), you can add this tag or name to the Thesaurus or Named Entities Manager which will tag all documents containing the name, concept or query automatically and for new documents, too.","title":"Automated tagging and automated annotation"},{"location":"doc/datamanagement/annotation/#alternate-annotation-tools-for-semantic-annotations","text":"You can integrate enhanced annotation tools for semantic annotations, which can not only tag whole documents but even parts of them: * Neonion * Pundit * Hypothesis * AnnotatorJS","title":"Alternate annotation tools for semantic annotations"},{"location":"doc/datamanagement/annotation/hypothesis/","text":"Visual Annotations, Web Annotation and Tagging Integration with Open Source annotation tool Hypothesis The integrated Open Source visual annotation tool Hypothesis provides an powerful visual user interface for (collaborative) web annotation and tagging by human editors, teams and groups that supports not only to tag documents and add page notes, but allows you to annotate documents and web pages within the text , even for single words, names, parts of sentences, sentences or paragraphs. To improve search, textanalytics & textmining and connect knowledge by annotations of human editors , the open source search engine offers an easy to use web user interface for setup an import and indexing of hypothesis annotations, tags and documents . Why (collaborative) tagging and (collaborative) annotations by (teams of) human editors? If a name and no of its aliases or a word and no of its synonyms is not contained in the document content or metadata it can not be found. But that doesn't mean, that the document or web page is not important for this topic. Even if text mining and text analytics technology like automatic named entity recognition of persons, organizations or places and interactive filters and automatic topic detection by thesaurus for automatic content analysis is very helpful to automate much of the researcher(s) and editor(s) work, no automatic text analysis, OCR , automatic tagging of topics or concepts by thesaurus and no machine learning mode is perfect and computers are dumb or/and often have to few data about all context to judge about ambigous data or meaning of texts. So additionally to automatic text analysis, good search results needs manual annotations by human editors or together in teams to improve of search results, find important and relevant documents, for easier filtering , verification and classification and together connect different knowledge of (interdisciplinary) teams . Search finds more by combining / data enrichment of content with tags and annotations If you tag a document or web page with a tag (f.e. a name of a person, organization, location or a concept or word), this document will be found on further queries with this word(s) even if this word(s) is/are not contained in the document or would be not recognized by automatic text analysis.","title":"Visual Annotations, Web Annotation and Tagging"},{"location":"doc/datamanagement/annotation/hypothesis/#visual-annotations-web-annotation-and-tagging","text":"","title":"Visual Annotations, Web Annotation and Tagging"},{"location":"doc/datamanagement/annotation/hypothesis/#integration-with-open-source-annotation-tool-hypothesis","text":"The integrated Open Source visual annotation tool Hypothesis provides an powerful visual user interface for (collaborative) web annotation and tagging by human editors, teams and groups that supports not only to tag documents and add page notes, but allows you to annotate documents and web pages within the text , even for single words, names, parts of sentences, sentences or paragraphs. To improve search, textanalytics & textmining and connect knowledge by annotations of human editors , the open source search engine offers an easy to use web user interface for setup an import and indexing of hypothesis annotations, tags and documents .","title":"Integration with Open Source annotation tool Hypothesis"},{"location":"doc/datamanagement/annotation/hypothesis/#why-collaborative-tagging-and-collaborative-annotations-by-teams-of-human-editors","text":"If a name and no of its aliases or a word and no of its synonyms is not contained in the document content or metadata it can not be found. But that doesn't mean, that the document or web page is not important for this topic. Even if text mining and text analytics technology like automatic named entity recognition of persons, organizations or places and interactive filters and automatic topic detection by thesaurus for automatic content analysis is very helpful to automate much of the researcher(s) and editor(s) work, no automatic text analysis, OCR , automatic tagging of topics or concepts by thesaurus and no machine learning mode is perfect and computers are dumb or/and often have to few data about all context to judge about ambigous data or meaning of texts. So additionally to automatic text analysis, good search results needs manual annotations by human editors or together in teams to improve of search results, find important and relevant documents, for easier filtering , verification and classification and together connect different knowledge of (interdisciplinary) teams .","title":"Why (collaborative) tagging and (collaborative) annotations by (teams of) human editors?"},{"location":"doc/datamanagement/annotation/hypothesis/#search-finds-more-by-combining-data-enrichment-of-content-with-tags-and-annotations","text":"If you tag a document or web page with a tag (f.e. a name of a person, organization, location or a concept or word), this document will be found on further queries with this word(s) even if this word(s) is/are not contained in the document or would be not recognized by automatic text analysis.","title":"Search finds more by combining / data enrichment of content with tags and annotations"},{"location":"doc/datamanagement/db/","text":"Named Entities in Relational Databases (SQL DB) Please donate: How to setup a relational database with fields as lists for Named Enties for faceted search and named entities extraction.","title":"Named Entities in Relational Databases (SQL DB)"},{"location":"doc/datamanagement/db/#named-entities-in-relational-databases-sql-db","text":"Please donate: How to setup a relational database with fields as lists for Named Enties for faceted search and named entities extraction.","title":"Named Entities in Relational Databases (SQL DB)"},{"location":"doc/datamanagement/named_entity_recognition/","text":"Automatic extraction of named entities like persons, organizations or locations by named entity recognition How to automatically extract named entities like persons, organizations or locations by machine learning for named entity recognition Extracted named entities like persons, organizations or locations (Named entity extraction) are used for structured navigation, aggregated overviews and interactive filters (faceted search) and to be able to get leads for connections and networks because you can analyze which persons, organizations or places occor together in how many documents. Automatic Named Entity Recognition by machine learning (ML) for automatic classification and annotation of text parts Additionally to known named entities in a thesaurus or imported ontologies other data analysis plugins integrate Named Entity Recognition (NER) by spaCy and/or Stanford Named Entities Recognizer (Stanford NER) . Named Entity Extraction of yet unknown entities or names So by integration of machine learning for analysing the structure of the text and classifying parts/words of the sentences to categories like person, location or organization, many yet unknown named entities can be extracted, which aren't configured or listed yet in the thesaurus or a list of names or ontology. Therefore it uses models trained with existing annotations of a large text corpus, so after that they can \"predict\" or better: guess by probability if a part of a sentence is a name of a person, a name of an organization, a verb or a place. Find more by combination with thesaurus and ontologies Since no machine learning algorithm and machine learning model is perfect, the search engine combines the analysis with other methods and data which is curated by human editors. Therefore you can add important names to the thesaurus , so the search engine will extract them even if the named entities recognition fails. You don't have to add each name yourself: By the ontologies manager you can import thousands of names from Open Data like Wikidata which offers an universal structured database with names of people like for example lists of names of politicians and members of parliament(s) . Configuration of the Spacy NER plugin The SpaCy Named Entity Recognition NER plugin is enabled by default, if not disabled because not needed or because of performance issues. The NER plugin is configured with language specific classifiers for English, German, Spanish, Portuguese, French, Dutch and Italian. Configuration of the Stanford NER plugin The Stanford Named Entitiy Recognizer (NER) plugin can be enabled instead or additional and is configured with language specific classifiers for English, German and Spanish.","title":"Automatic extraction of named entities like persons, organizations or locations by named entity recognition"},{"location":"doc/datamanagement/named_entity_recognition/#automatic-extraction-of-named-entities-like-persons-organizations-or-locations-by-named-entity-recognition","text":"","title":"Automatic extraction of named entities like persons, organizations or locations by named entity recognition"},{"location":"doc/datamanagement/named_entity_recognition/#how-to-automatically-extract-named-entities-like-persons-organizations-or-locations-by-machine-learning-for-named-entity-recognition","text":"Extracted named entities like persons, organizations or locations (Named entity extraction) are used for structured navigation, aggregated overviews and interactive filters (faceted search) and to be able to get leads for connections and networks because you can analyze which persons, organizations or places occor together in how many documents.","title":"How to automatically extract named entities like persons, organizations or locations by machine learning for named entity recognition"},{"location":"doc/datamanagement/named_entity_recognition/#automatic-named-entity-recognition-by-machine-learning-ml-for-automatic-classification-and-annotation-of-text-parts","text":"Additionally to known named entities in a thesaurus or imported ontologies other data analysis plugins integrate Named Entity Recognition (NER) by spaCy and/or Stanford Named Entities Recognizer (Stanford NER) .","title":"Automatic Named Entity Recognition by machine learning (ML) for automatic classification and annotation of text parts"},{"location":"doc/datamanagement/named_entity_recognition/#named-entity-extraction-of-yet-unknown-entities-or-names","text":"So by integration of machine learning for analysing the structure of the text and classifying parts/words of the sentences to categories like person, location or organization, many yet unknown named entities can be extracted, which aren't configured or listed yet in the thesaurus or a list of names or ontology. Therefore it uses models trained with existing annotations of a large text corpus, so after that they can \"predict\" or better: guess by probability if a part of a sentence is a name of a person, a name of an organization, a verb or a place.","title":"Named Entity Extraction of yet unknown entities or names"},{"location":"doc/datamanagement/named_entity_recognition/#find-more-by-combination-with-thesaurus-and-ontologies","text":"Since no machine learning algorithm and machine learning model is perfect, the search engine combines the analysis with other methods and data which is curated by human editors. Therefore you can add important names to the thesaurus , so the search engine will extract them even if the named entities recognition fails. You don't have to add each name yourself: By the ontologies manager you can import thousands of names from Open Data like Wikidata which offers an universal structured database with names of people like for example lists of names of politicians and members of parliament(s) .","title":"Find more by combination with thesaurus and ontologies"},{"location":"doc/datamanagement/named_entity_recognition/#configuration-of-the-spacy-ner-plugin","text":"The SpaCy Named Entity Recognition NER plugin is enabled by default, if not disabled because not needed or because of performance issues. The NER plugin is configured with language specific classifiers for English, German, Spanish, Portuguese, French, Dutch and Italian.","title":"Configuration of the Spacy NER plugin"},{"location":"doc/datamanagement/named_entity_recognition/#configuration-of-the-stanford-ner-plugin","text":"The Stanford Named Entitiy Recognizer (NER) plugin can be enabled instead or additional and is configured with language specific classifiers for English, German and Spanish.","title":"Configuration of the Stanford NER plugin"},{"location":"doc/datamanagement/ocr/","text":"How to optimize and improve Optical Character Recognition results (OCR) Automatic text recognition in images or scanned documents by Optical Character Recognition (OCR) Text stored in image formats like JPG, PNG, TIFF or GIF (i.e. scans, photos or screenshots) can not be found by standard full text search. So this enhancer enriches meta data of images like filename, format and size with results from automatic text recognition or optical character recognition (OCR) by free open source OCR software like Tesseract . Since many information is not searchable by full text search because its in graphical formats embedded in PDF documents or Powerpoint presentations (i.e. screenshots instead of text format), the enhancer extracts images from PDF files for automatic text recognition (OCR), too. Enable OCR OCR is enabled by default in the Debian and Ubuntu packages and the virtual machine packages like Open Semantic Desktop Search or Open Semantic Search Appliance. If you build your search engine from the open source code enable OCR by installing the open source software Tesseract OCR . If you disabled/enabled OCR, you should disable/enable OCR for images within PDF files, too, since many PDF files are scans and do contain much text data only as graphics. How to optimize OCR settings to improve OCR results You can optimize OCR to find more by different ways, which you can combine for optimal OCR results with fewer OCR failures: Scanning resolution If scanning documents yourself, scan or store the images with a higher resolution, so the OCR can analyse more details of the characters. Language of dictionary Since OCR uses a language specific dictionary, set the OCR language to your language or to multiple languages , which are used in your documents. Additional custom OCR dictionary entries from Thesaurus and Ontologies Fully integrated out of the box in next open source release: Concepts, words and names of named entities like organizations, places, locations or persons that are important for you so you added them to your thesaurus or which are included in lists of names or ontologies (for example lists of names of relevant persons from internal meta data sources or from open data sources like Wikidata ) you defined for faceted search/interactive filters and/or analytics/aggregated overviews are recognized better by OCR of scanned documents, too. Therefore your additional domain knowledge / vocabulary from thesaurus, lists and ontologies is used additional to the OCR language specific dictionary of Tesseract OCR. Since in many scanned legacy files on paper names are fully written in uppercase, this autogenerated custom OCR dictionary / OCR wordlist includes the uppercase variant of each word, too. So you should consider to rebuild your index / reindex important files by force (so they are analyzed again & reindexed, even if yet in index) after adding very important words or names to the thesaurus or provided by (new or changed) ontologies. Disable usage of dictionaries and word lists OCR results for unknown names which are not in such managed lists or dictionaries are better, if using dictionaries and lists is switched off. If you have enough CPU resources while indexing, you can combine results of both settings to index more names or words from scanned documents in correct spelling, since the search engine will find all results of both different and technically opponent OCR strategies. Automatic deskewing and rotation of low quality scans before OCR Many documents are scanned skew. Automatic deskewing of such low quality scans by Scantailor before OCR can improve the OCR results. Therefore the Scantailor plugin is enabled, if not disabled because of performance isssues. Training characters and fonts You can train the OCR engine with the special fonts used in your documents to improve the machine learning model for recognition of characters of this fonts. How to manage OCR errors and fails Despite all that optimization, automatic character recognition can fail and OCR errors like wrong recognized words or names occur. Therefore there are integrated tools for manual handling and management of OCR failures on document level for single documents or on meta level for all documents: Handle OCR failures by collaborative tagging and annotation For single documents with OCR failures you can add annotations or tags with the words that were recognized wrong by the OCR engine, so the search engine can find them despite this OCR errors because of the tags or annotations written correct. Manage OCR failures by thesaurus (Hidden labels) Manage common OCR failures for all documents by Thesaurus entries for management of OCR errors (Hidden labels) Since you handle this OCR failure on meta level, the correction can be applied automatically to new documents with the same wrong recognized words or names. The recommender can analyse the corpus for typos/OCR errors of a thesaurus entry and recommends such misspellings for adding to the thesaurus as hidden label by one click. Combining OCR results of multiple OCR tools No OCR engine is perfect. So in some projects we used for example Abby Finereader to OCR the images in PDFs and additionally to the integrated Tesseract OCR. Each of them recognized words or names, on which the other OCR software failed. Because that combined and indexed both OCR results for the same document, we could find many documents more. The Open Semantic ETL framework is able to combine or unify and index analysis results of multiple analysis or OCR tools or OCR parameters for the same document or image. More information about improving OCR quality Improving the quality of the output of Tesseract OCR Koordinierungsprojekt zur Weiterentwicklung von OCR Verfahren","title":"How to optimize and improve Optical Character Recognition results (OCR)"},{"location":"doc/datamanagement/ocr/#how-to-optimize-and-improve-optical-character-recognition-results-ocr","text":"","title":"How to optimize and improve Optical Character Recognition results (OCR)"},{"location":"doc/datamanagement/ocr/#automatic-text-recognition-in-images-or-scanned-documents-by-optical-character-recognition-ocr","text":"Text stored in image formats like JPG, PNG, TIFF or GIF (i.e. scans, photos or screenshots) can not be found by standard full text search. So this enhancer enriches meta data of images like filename, format and size with results from automatic text recognition or optical character recognition (OCR) by free open source OCR software like Tesseract . Since many information is not searchable by full text search because its in graphical formats embedded in PDF documents or Powerpoint presentations (i.e. screenshots instead of text format), the enhancer extracts images from PDF files for automatic text recognition (OCR), too.","title":"Automatic text recognition in images or scanned documents by Optical Character Recognition (OCR)"},{"location":"doc/datamanagement/ocr/#enable-ocr","text":"OCR is enabled by default in the Debian and Ubuntu packages and the virtual machine packages like Open Semantic Desktop Search or Open Semantic Search Appliance. If you build your search engine from the open source code enable OCR by installing the open source software Tesseract OCR . If you disabled/enabled OCR, you should disable/enable OCR for images within PDF files, too, since many PDF files are scans and do contain much text data only as graphics.","title":"Enable OCR"},{"location":"doc/datamanagement/ocr/#how-to-optimize-ocr-settings-to-improve-ocr-results","text":"You can optimize OCR to find more by different ways, which you can combine for optimal OCR results with fewer OCR failures:","title":"How to optimize OCR settings to improve OCR results"},{"location":"doc/datamanagement/ocr/#scanning-resolution","text":"If scanning documents yourself, scan or store the images with a higher resolution, so the OCR can analyse more details of the characters.","title":"Scanning resolution"},{"location":"doc/datamanagement/ocr/#language-of-dictionary","text":"Since OCR uses a language specific dictionary, set the OCR language to your language or to multiple languages , which are used in your documents.","title":"Language of dictionary"},{"location":"doc/datamanagement/ocr/#additional-custom-ocr-dictionary-entries-from-thesaurus-and-ontologies","text":"Fully integrated out of the box in next open source release: Concepts, words and names of named entities like organizations, places, locations or persons that are important for you so you added them to your thesaurus or which are included in lists of names or ontologies (for example lists of names of relevant persons from internal meta data sources or from open data sources like Wikidata ) you defined for faceted search/interactive filters and/or analytics/aggregated overviews are recognized better by OCR of scanned documents, too. Therefore your additional domain knowledge / vocabulary from thesaurus, lists and ontologies is used additional to the OCR language specific dictionary of Tesseract OCR. Since in many scanned legacy files on paper names are fully written in uppercase, this autogenerated custom OCR dictionary / OCR wordlist includes the uppercase variant of each word, too. So you should consider to rebuild your index / reindex important files by force (so they are analyzed again & reindexed, even if yet in index) after adding very important words or names to the thesaurus or provided by (new or changed) ontologies.","title":"Additional custom OCR dictionary entries from Thesaurus and Ontologies"},{"location":"doc/datamanagement/ocr/#disable-usage-of-dictionaries-and-word-lists","text":"OCR results for unknown names which are not in such managed lists or dictionaries are better, if using dictionaries and lists is switched off. If you have enough CPU resources while indexing, you can combine results of both settings to index more names or words from scanned documents in correct spelling, since the search engine will find all results of both different and technically opponent OCR strategies.","title":"Disable usage of dictionaries and word lists"},{"location":"doc/datamanagement/ocr/#automatic-deskewing-and-rotation-of-low-quality-scans-before-ocr","text":"Many documents are scanned skew. Automatic deskewing of such low quality scans by Scantailor before OCR can improve the OCR results. Therefore the Scantailor plugin is enabled, if not disabled because of performance isssues.","title":"Automatic deskewing and rotation of low quality scans before OCR"},{"location":"doc/datamanagement/ocr/#training-characters-and-fonts","text":"You can train the OCR engine with the special fonts used in your documents to improve the machine learning model for recognition of characters of this fonts.","title":"Training characters and fonts"},{"location":"doc/datamanagement/ocr/#how-to-manage-ocr-errors-and-fails","text":"Despite all that optimization, automatic character recognition can fail and OCR errors like wrong recognized words or names occur. Therefore there are integrated tools for manual handling and management of OCR failures on document level for single documents or on meta level for all documents:","title":"How to manage OCR errors and fails"},{"location":"doc/datamanagement/ocr/#handle-ocr-failures-by-collaborative-tagging-and-annotation","text":"For single documents with OCR failures you can add annotations or tags with the words that were recognized wrong by the OCR engine, so the search engine can find them despite this OCR errors because of the tags or annotations written correct.","title":"Handle OCR failures by collaborative tagging and annotation"},{"location":"doc/datamanagement/ocr/#manage-ocr-failures-by-thesaurus-hidden-labels","text":"Manage common OCR failures for all documents by Thesaurus entries for management of OCR errors (Hidden labels) Since you handle this OCR failure on meta level, the correction can be applied automatically to new documents with the same wrong recognized words or names. The recommender can analyse the corpus for typos/OCR errors of a thesaurus entry and recommends such misspellings for adding to the thesaurus as hidden label by one click.","title":"Manage OCR failures by thesaurus (Hidden labels)"},{"location":"doc/datamanagement/ocr/#combining-ocr-results-of-multiple-ocr-tools","text":"No OCR engine is perfect. So in some projects we used for example Abby Finereader to OCR the images in PDFs and additionally to the integrated Tesseract OCR. Each of them recognized words or names, on which the other OCR software failed. Because that combined and indexed both OCR results for the same document, we could find many documents more. The Open Semantic ETL framework is able to combine or unify and index analysis results of multiple analysis or OCR tools or OCR parameters for the same document or image.","title":"Combining OCR results of multiple OCR tools"},{"location":"doc/datamanagement/ocr/#more-information-about-improving-ocr-quality","text":"Improving the quality of the output of Tesseract OCR Koordinierungsprojekt zur Weiterentwicklung von OCR Verfahren","title":"More information about improving OCR quality"},{"location":"doc/datamanagement/ontologies/","text":"Manage Lists, Dictionaries, Vocabularies & Thesauri (Ontologies) Management of Lists of names or concepts, dictionaries, vocabularies, thesauri or ontologies for faceted search and ontology based text mining & document analysis If there is an existing list, dictionary, vocabulary or thesaurus or if the entries are available in other structured format like an semantic web ontology yet i.e. from other tools or some open data sources , you don't need to enter or manage manually each named entity like persons, organizations, locations or important words or concepts to your own thesaurus to be able to use them for analytics, navigation, aggregated overview and interactive filters . You can integrate or import additional, existing, shared or external list of names, thesauri, dictionaries, vocabularies or ontologies: Import external lists of names, thesaurus or ontologies Just click on \"Import List of Names, Thesaurus or Ontology\" and upload an file or reference to an file or web resource. After klicking on \"save\" new documents will be analyzed by this thesaurus, ontology or list. After klicking on \"Apply\" all old documents will be tagged with this ontology, too. Interactive filters and analytics by aggregated overview (Faceted search) Use the search user interface and see the new search facet with the ontology or list name where you can see which names or concepts occur in how much documents and click on the name or concept to filter the results to documents where this name or concept occurs. Learn more about faceted search. Formats The formats will be autodetected: Plain text lists This list can be a plain text file with one name in each line. The charset / encoding should be autodetected. In cases the autodetection doesn't work correct for some old standards or local charsets, convert/export your data with the universal charset UTF-8 first. Ontologies and Thesauri Or it can be an ontology in the open standards Resource Description Format (RDF), Simple Knowledge Organisation System (SKOS) or OWL. Turtle format will follow (until then you can convert that to RDF format using a standard triplestore). If you want only to import one language of a multilingual thesaurus or ontology, you can use a standard triplestore to filter the thesaurus using a SPARQL query to select only labels in your favorite language and use/import only that part of the graph. Spreadsheets and Tables (Excel or Open Office or Libreoffice Calc) You can use data from tables or spreadsheets like Excel or Open Office Calc or Libre Office Calc or even columns from databases, too when there maybe will be an automatic import plugin for lists in this format, too (please donate). Meanwhile you have to convert tables or databases which are not in RDF format or plain text format yet to plain text lists manually: * Open the data with Excel or OpenOffice or Libreoffice Calc. * Select the column with the names * Copy the column with the names (click Edit > Copy in the main menu or press CTRL & c) * Open a new table * Paste that column with the names (click Edit > Paste in the main menu or press CTRL & v) * Save the new table with only this column in CSV format. Since the table has only one column, you will get a plain text list file. * Upload or reference this export file like described for open semantic web formats or plaintext lists.","title":"Lists, Dictionaries, Vocabularies and Thesauri (Ontologies)"},{"location":"doc/datamanagement/ontologies/#manage-lists-dictionaries-vocabularies-thesauri-ontologies","text":"","title":"Manage Lists, Dictionaries, Vocabularies &amp; Thesauri (Ontologies)"},{"location":"doc/datamanagement/ontologies/#management-of-lists-of-names-or-concepts-dictionaries-vocabularies-thesauri-or-ontologies-for-faceted-search-and-ontology-based-text-mining-document-analysis","text":"If there is an existing list, dictionary, vocabulary or thesaurus or if the entries are available in other structured format like an semantic web ontology yet i.e. from other tools or some open data sources , you don't need to enter or manage manually each named entity like persons, organizations, locations or important words or concepts to your own thesaurus to be able to use them for analytics, navigation, aggregated overview and interactive filters . You can integrate or import additional, existing, shared or external list of names, thesauri, dictionaries, vocabularies or ontologies:","title":"Management of Lists of names or concepts, dictionaries, vocabularies, thesauri or ontologies for faceted search and ontology based text mining &amp; document analysis"},{"location":"doc/datamanagement/ontologies/#import-external-lists-of-names-thesaurus-or-ontologies","text":"Just click on \"Import List of Names, Thesaurus or Ontology\" and upload an file or reference to an file or web resource. After klicking on \"save\" new documents will be analyzed by this thesaurus, ontology or list. After klicking on \"Apply\" all old documents will be tagged with this ontology, too.","title":"Import external lists of names, thesaurus or ontologies"},{"location":"doc/datamanagement/ontologies/#interactive-filters-and-analytics-by-aggregated-overview-faceted-search","text":"Use the search user interface and see the new search facet with the ontology or list name where you can see which names or concepts occur in how much documents and click on the name or concept to filter the results to documents where this name or concept occurs. Learn more about faceted search.","title":"Interactive filters and analytics by aggregated overview (Faceted search)"},{"location":"doc/datamanagement/ontologies/#formats","text":"The formats will be autodetected:","title":"Formats"},{"location":"doc/datamanagement/ontologies/#plain-text-lists","text":"This list can be a plain text file with one name in each line. The charset / encoding should be autodetected. In cases the autodetection doesn't work correct for some old standards or local charsets, convert/export your data with the universal charset UTF-8 first.","title":"Plain text lists"},{"location":"doc/datamanagement/ontologies/#ontologies-and-thesauri","text":"Or it can be an ontology in the open standards Resource Description Format (RDF), Simple Knowledge Organisation System (SKOS) or OWL. Turtle format will follow (until then you can convert that to RDF format using a standard triplestore). If you want only to import one language of a multilingual thesaurus or ontology, you can use a standard triplestore to filter the thesaurus using a SPARQL query to select only labels in your favorite language and use/import only that part of the graph.","title":"Ontologies and Thesauri"},{"location":"doc/datamanagement/ontologies/#spreadsheets-and-tables-excel-or-open-office-or-libreoffice-calc","text":"You can use data from tables or spreadsheets like Excel or Open Office Calc or Libre Office Calc or even columns from databases, too when there maybe will be an automatic import plugin for lists in this format, too (please donate). Meanwhile you have to convert tables or databases which are not in RDF format or plain text format yet to plain text lists manually: * Open the data with Excel or OpenOffice or Libreoffice Calc. * Select the column with the names * Copy the column with the names (click Edit > Copy in the main menu or press CTRL & c) * Open a new table * Paste that column with the names (click Edit > Paste in the main menu or press CTRL & v) * Save the new table with only this column in CSV format. Since the table has only one column, you will get a plain text list file. * Upload or reference this export file like described for open semantic web formats or plaintext lists.","title":"Spreadsheets and Tables (Excel or Open Office or Libreoffice Calc)"},{"location":"doc/datamanagement/opendata/","text":"How to integrate Open Data to enhance semantic search, data analysis, text mining & document analysis How data enrichment with Linked Open Data (LOD) from Open Access databases helps to improve search, find more results, analyse document sets and data sets and how monitoring with Open Data watchlists generates leads for investigative research Open Data from Open Access databases like WikiData (structured data from Wikipedia) can be used to improve analysis and navigation of your private data. It can generate leads for important documents where important concepts or people occur . Or your private data or documents can be enriched with additional data or structures from open data. So the integration of Open Data with Open Source research tools enables easier search, more and better search results, analytics and interactive filters (faceted search), watchlists and other useful and powerful features by a combination of textmining and data analysis methods like entity matching and entity tagging , analytics or automatic classification by machine learning . Computing all this with open source software on your own computer or server all that is possible without giving away sensitive informations because of privacy issues with any cloud services . Sharing data and Open Data If there is an existing list, dictionary, vocabulary or thesaurus or if the entries are available in other structured format like an semantic web ontology yet i.e. from other tools or some open data sources , you don't need to enter manually each named entity like persons, organizations, locations or important words or concepts to the thesaurus to use them for analytics, navigation, aggregated overview and interactive filters . You can import existing, shared or external list of names, thesauri, dictionaries, vocabularies or ontologies : Tagging named entities like people or organizations Using this list of important names or ontology the named entities tagger can tag documents in which this names occur. Leads for research by matching watchlists So you get leads for potential relevant documents where important concepts or people on such a watchlist or ontology occur, i.e. if analysing a large unknown document set or dataset or after getting new documents or news. Analytics, aggregated overviews and exploration By the faceted search you get an aggregated overview for the different facets like concepts, persons, locations or organizations showing, how many documents matching this entity. Interactive filters (faceted search) You can use this overviews or named entities as interactive filter to narrow down search results. So a click to a facet (i.e. an organization) will drill down the search results to fewer documents, matching this additional facet/filter, too. Free open data source (Linked Open Data from WikiData) One of the biggest and an universal Open Data source is WikiData , the structured multilingual database from Wikipedia. From this structured datasource you can select and download lists of names for example of important people like politicians for analysis of your document sets, navigation and interactive filters, enable semantic search to find aliases or names in other languages, watchlists, data for data enrichment or data for machine learning. Finds more by thesauri of synonyms and irregular verbs (Wiktionary) Another usage of Open Data is to use a dictionary and thesaurus like the Wikipedia Wiktionary or an Libre Office or Open Office hunspell dictionary to extend search queries or indexes so you are able to find synonyms and irregular verbs, too. Automatic classification or disambiguation (machine learning) Another use of open data from WikiData or Wikipedia is to get good training data for classification and disambiguation. So we get for example the possibility to guess which meaning names or words with more than one meaning (homonyms) by comparing the context with big open data like Wikipedia by machine learning algorithms for automatic classification to disambiguate the named entity. Privacy and data protection: Investigative Research with Open Source Software With our free Open Source software and research tools you can use Open Data from open data sources on the internet even for investigative research in sensitive documents or other sensitive data with enhanced privacy needs like for example needed for investigative journalism: Since there is no cloud but only other peoples or companies computers, we want to compute our data ourselves with Free Software. So after download and import of an open dataset our open source search engine computes all matching and watching for this Open Data list or ontology only on your own laptop or server. There is no upload of documents content fotr analysis nor upload of matching names to a cloud service. Big Data analysis by Open Data This is even possible for many named entities or list entries and very much data or and huge document collections. In some research, text mining and data analysis projects we cross referenced documents with lists and ontologies with hundreds of thousands of names like politicians, species, materials or other important concepts to look for. In such cases you should import the list to thesaurus editor or ontologies manager for entity tagging before indexing the documents so most entities can be extracted more performant while and before indexing instead of later searches and document updates for each entity, which will need some time in such cases.","title":"How to integrate Open Data to enhance semantic search, data analysis, text mining & document analysis"},{"location":"doc/datamanagement/opendata/#how-to-integrate-open-data-to-enhance-semantic-search-data-analysis-text-mining-document-analysis","text":"","title":"How to integrate Open Data to enhance semantic search, data analysis, text mining &amp; document analysis"},{"location":"doc/datamanagement/opendata/#how-data-enrichment-with-linked-open-data-lod-from-open-access-databases-helps-to-improve-search-find-more-results-analyse-document-sets-and-data-sets-and-how-monitoring-with-open-data-watchlists-generates-leads-for-investigative-research","text":"Open Data from Open Access databases like WikiData (structured data from Wikipedia) can be used to improve analysis and navigation of your private data. It can generate leads for important documents where important concepts or people occur . Or your private data or documents can be enriched with additional data or structures from open data. So the integration of Open Data with Open Source research tools enables easier search, more and better search results, analytics and interactive filters (faceted search), watchlists and other useful and powerful features by a combination of textmining and data analysis methods like entity matching and entity tagging , analytics or automatic classification by machine learning . Computing all this with open source software on your own computer or server all that is possible without giving away sensitive informations because of privacy issues with any cloud services .","title":"How data enrichment with Linked Open Data (LOD) from Open Access databases helps to improve search, find more results, analyse document sets and data sets and how monitoring with Open Data watchlists generates leads for investigative research"},{"location":"doc/datamanagement/opendata/#sharing-data-and-open-data","text":"If there is an existing list, dictionary, vocabulary or thesaurus or if the entries are available in other structured format like an semantic web ontology yet i.e. from other tools or some open data sources , you don't need to enter manually each named entity like persons, organizations, locations or important words or concepts to the thesaurus to use them for analytics, navigation, aggregated overview and interactive filters . You can import existing, shared or external list of names, thesauri, dictionaries, vocabularies or ontologies :","title":"Sharing data and Open Data"},{"location":"doc/datamanagement/opendata/#tagging-named-entities-like-people-or-organizations","text":"Using this list of important names or ontology the named entities tagger can tag documents in which this names occur.","title":"Tagging named entities like people or organizations"},{"location":"doc/datamanagement/opendata/#leads-for-research-by-matching-watchlists","text":"So you get leads for potential relevant documents where important concepts or people on such a watchlist or ontology occur, i.e. if analysing a large unknown document set or dataset or after getting new documents or news.","title":"Leads for research by matching watchlists"},{"location":"doc/datamanagement/opendata/#analytics-aggregated-overviews-and-exploration","text":"By the faceted search you get an aggregated overview for the different facets like concepts, persons, locations or organizations showing, how many documents matching this entity.","title":"Analytics, aggregated overviews and exploration"},{"location":"doc/datamanagement/opendata/#interactive-filters-faceted-search","text":"You can use this overviews or named entities as interactive filter to narrow down search results. So a click to a facet (i.e. an organization) will drill down the search results to fewer documents, matching this additional facet/filter, too.","title":"Interactive filters (faceted search)"},{"location":"doc/datamanagement/opendata/#free-open-data-source-linked-open-data-from-wikidata","text":"One of the biggest and an universal Open Data source is WikiData , the structured multilingual database from Wikipedia. From this structured datasource you can select and download lists of names for example of important people like politicians for analysis of your document sets, navigation and interactive filters, enable semantic search to find aliases or names in other languages, watchlists, data for data enrichment or data for machine learning.","title":"Free open data source (Linked Open Data from WikiData)"},{"location":"doc/datamanagement/opendata/#finds-more-by-thesauri-of-synonyms-and-irregular-verbs-wiktionary","text":"Another usage of Open Data is to use a dictionary and thesaurus like the Wikipedia Wiktionary or an Libre Office or Open Office hunspell dictionary to extend search queries or indexes so you are able to find synonyms and irregular verbs, too.","title":"Finds more by thesauri of synonyms and irregular verbs (Wiktionary)"},{"location":"doc/datamanagement/opendata/#automatic-classification-or-disambiguation-machine-learning","text":"Another use of open data from WikiData or Wikipedia is to get good training data for classification and disambiguation. So we get for example the possibility to guess which meaning names or words with more than one meaning (homonyms) by comparing the context with big open data like Wikipedia by machine learning algorithms for automatic classification to disambiguate the named entity.","title":"Automatic classification or disambiguation (machine learning)"},{"location":"doc/datamanagement/opendata/#privacy-and-data-protection-investigative-research-with-open-source-software","text":"With our free Open Source software and research tools you can use Open Data from open data sources on the internet even for investigative research in sensitive documents or other sensitive data with enhanced privacy needs like for example needed for investigative journalism: Since there is no cloud but only other peoples or companies computers, we want to compute our data ourselves with Free Software. So after download and import of an open dataset our open source search engine computes all matching and watching for this Open Data list or ontology only on your own laptop or server. There is no upload of documents content fotr analysis nor upload of matching names to a cloud service.","title":"Privacy and data protection: Investigative Research with Open Source Software"},{"location":"doc/datamanagement/opendata/#big-data-analysis-by-open-data","text":"This is even possible for many named entities or list entries and very much data or and huge document collections. In some research, text mining and data analysis projects we cross referenced documents with lists and ontologies with hundreds of thousands of names like politicians, species, materials or other important concepts to look for. In such cases you should import the list to thesaurus editor or ontologies manager for entity tagging before indexing the documents so most entities can be extracted more performant while and before indexing instead of later searches and document updates for each entity, which will need some time in such cases.","title":"Big Data analysis by Open Data"},{"location":"doc/datamanagement/opendata/wikidata/","text":"Import lists of names from Wikidata knowledge graph Import lists of names from Wikidata for analytics of your document sets, structure of navigation, interactive filters and aggregated overviews Open Data from Open Access databases like WikiData (structured data and universal knowledge graph from Wikipedia in open standards for linked data and semantic web) can be used to improve search, analysis, filtering and navigation of your private documents, news and data . This tutorial describes how to import multilingual lists of names from Wikidata as universal ontology, knowledge graph or thesaurus to * see / overview which of these names occur in your document sets or search results (aggregated overview) * get interactive filters for your documents by this names or entities like persons (for example all politicians of your parliament ), organizations or locations * automatically find aliases or the same names written in other languages, too * configure alerts, feeds or leads for new documents where important concepts or people occur by search or filter for/of that lists/facet Wikidata as open data database and knowledge graph for fine granular structured multilingual linked data If there is an existing list, dictionary, vocabulary or thesaurus or if the entries are available in other structured format like an semantic web ontology yet from open data sources like Wikidata, you don't need to enter manually each named entity like persons, organizations, locations or important words or concepts to the thesaurus to use them for analytics, navigation, aggregated overview and interactive filters . One of the biggest and an universal Open Data source is WikiData , the multilingual structured database from Wikipedia. From this multilingual and fine granular structured datasource you can select and download lists of names for example of people like politicians for your analysis of document sets & news, interactive filters (faceted search) or alerts. Automatic tagging with named entities in list like names of people or organizations from Wikidata Using this list of important names or ontology the named entities tagger is tagging documents in which this names occur. Leads for research by matching watchlists So you get leads for potential relevant documents where important concepts or people on such a watchlist or ontology occur, i.e. if analysing a large unknown document set or dataset or after getting new documents or news. Analytics, aggregated overviews and exploration By the faceted search you get an aggregated overview for the different facets like concepts, persons, locations or organizations showing, how many of the found or all documents matching this entity. Interactive filters (faceted search) You can use this overviews or named entities as interactive filter to narrow down search results. So a click to a facet (i.e. an organization) will drill down the search results to fewer documents, matching this additional facet/filter, too. Multilingual labels, alternate labels and aliases for Semantic Search Since the used open standards for Semantic Web and Linked Data labels like names can be multilingual, so you will find the names in different languages. So for example after import of names of politicians from Wikidata the semantic search for Angela Merkel will find documents which contain Angela Dorothea Kasner , too, which is birth name of Angela Merkel before she married. Additionally documents which contain Angela Merkelowa or \u0410\u043d\u0433\u0435\u043b\u0430 \u041c\u0435\u0440\u043a\u0435\u043b\u044c will be found, too, which is Angela Merkel written in russian language. Open Standard for linked data and interoperability Since Wikidata is using open standards for Linked Data and interoperability of the Semantic Web, the structured data in XML/RDF format and linked data by RDFS and SKOS thesaurus standard is interoperable and can be used for configuration of interactive filters and analytics of your document set in Open Semantic Search out of the box. Import list or ontology or define Wikidata query So you can import existing, shared or external list of names, thesauri, dictionaries, vocabularies or ontologies : SPARQL query and SPARQL endpoint (triplestore) Instead of uploading or referencing an yet downloaded graph or list of names, you can configure a SPARQL query on a triplestore (SPARQL endpoint) in the datasources user interface for ontololgies in the tab \"SPARQL\": Therefore you have to use SPARQL queries with CONSTRUCT (like you can see in our SPARQL example queries ) or DESCRIBE (warning: much more overhead if the knowledge graph has more properties and linked data than only properties and data of a thesaurus) so the SPARQL results includes the following standard properties that are used for analytics: rdfs:label and/or skos:altLabel and/or skos:hiddenLabel Plain text lists If you work only with one single language you can export the results of a SELECT query for only a single field/property in CSV format (only one field, so you get a plain text list instead of a table) and import/upload/reference this plain text list of names to the Lists and Ontologies Manager, which is much more performant for this case, because you save download bandwidth and CPU resources while import. Privacy & data protection Open Semantic Search sends only the queries of the defined lists to the internet or linked data cloud and downloads the lists of names. Different to common cloud services for named entity recognition, all other computing is done by open source software on your own computer or server without giving away sensitive informations like document content, search queries and how often and what has been found. If even this list queries are sensitive information you can do downloads manually over Tor and import the data from the saved local files/graphs/lists by referencing or uploading them to the ontologies manager. Wikidata SPARQL queries examples SPARQL Endpoint: https://query.wikidata.org/sparql Politicians Germany (german politicians) Members of german parialment Deutscher Bundestag (Bundestagsabgeordnete) Current and former members of German parliament (Deutscher Bundestag) (>4000 names): PREFIX skos: <http://www.w3.org/2004/02/skos/core#> CONSTRUCT { ?uri rdfs:label ?label ; skos:prefLabel ?prefLabel ; skos:altLabel ?altLabel . } WHERE { ?uri wdt:P39 wd:Q1939555 . OPTIONAL { ?uri rdfs:label ?label . } OPTIONAL { ?uri skos:prefLabel ?prefLabel . } OPTIONAL { ?uri skos:altLabel ?altLabel . } } The following SPARQL alternate using DESCRIBE would work, too, but is not recommended and would cause much more overhead if the knowledge graph has more properties and linked data than only thesaurus properties and data (like it is the case with Wikidata, which is a universal knowledge graph with many different properties, not \"only\" a thesaurus with labels and only a few types of relations): ~~ DESCRIBE ?uri WHERE { ?uri wdt:P39 wd:Q1939555 . } ~~ Landtag (Landtagsabgeordnete) Baden W\u00fcrttemberg Current and former members of Landtag Baden-W\u00fcrttemberg: PREFIX skos: <http://www.w3.org/2004/02/skos/core#> CONSTRUCT { ?uri rdfs:label ?label ; skos:prefLabel ?prefLabel ; skos:altLabel ?altLabel . } WHERE { ?uri wdt:P39 wd:Q17481175 . OPTIONAL { ?uri rdfs:label ?label . } OPTIONAL { ?uri skos:prefLabel ?prefLabel . } OPTIONAL { ?uri skos:altLabel ?altLabel . } } List of politicians of other countries or parliaments To find out more about queries for politicians of your country or your countries politicians are not yet available for download, you can help to add them to Wikidata in the working group Every Politician .","title":"Import lists of names from Wikidata knowledge graph"},{"location":"doc/datamanagement/opendata/wikidata/#import-lists-of-names-from-wikidata-knowledge-graph","text":"","title":"Import lists of names from Wikidata knowledge graph"},{"location":"doc/datamanagement/opendata/wikidata/#import-lists-of-names-from-wikidata-for-analytics-of-your-document-sets-structure-of-navigation-interactive-filters-and-aggregated-overviews","text":"Open Data from Open Access databases like WikiData (structured data and universal knowledge graph from Wikipedia in open standards for linked data and semantic web) can be used to improve search, analysis, filtering and navigation of your private documents, news and data . This tutorial describes how to import multilingual lists of names from Wikidata as universal ontology, knowledge graph or thesaurus to * see / overview which of these names occur in your document sets or search results (aggregated overview) * get interactive filters for your documents by this names or entities like persons (for example all politicians of your parliament ), organizations or locations * automatically find aliases or the same names written in other languages, too * configure alerts, feeds or leads for new documents where important concepts or people occur by search or filter for/of that lists/facet","title":"Import lists of names from Wikidata for analytics of your document sets, structure of navigation, interactive filters and aggregated overviews"},{"location":"doc/datamanagement/opendata/wikidata/#wikidata-as-open-data-database-and-knowledge-graph-for-fine-granular-structured-multilingual-linked-data","text":"If there is an existing list, dictionary, vocabulary or thesaurus or if the entries are available in other structured format like an semantic web ontology yet from open data sources like Wikidata, you don't need to enter manually each named entity like persons, organizations, locations or important words or concepts to the thesaurus to use them for analytics, navigation, aggregated overview and interactive filters . One of the biggest and an universal Open Data source is WikiData , the multilingual structured database from Wikipedia. From this multilingual and fine granular structured datasource you can select and download lists of names for example of people like politicians for your analysis of document sets & news, interactive filters (faceted search) or alerts.","title":"Wikidata as open data database and knowledge graph for fine granular structured multilingual linked data"},{"location":"doc/datamanagement/opendata/wikidata/#automatic-tagging-with-named-entities-in-list-like-names-of-people-or-organizations-from-wikidata","text":"Using this list of important names or ontology the named entities tagger is tagging documents in which this names occur.","title":"Automatic tagging with named entities in list like names of people or organizations from Wikidata"},{"location":"doc/datamanagement/opendata/wikidata/#leads-for-research-by-matching-watchlists","text":"So you get leads for potential relevant documents where important concepts or people on such a watchlist or ontology occur, i.e. if analysing a large unknown document set or dataset or after getting new documents or news.","title":"Leads for research by matching watchlists"},{"location":"doc/datamanagement/opendata/wikidata/#analytics-aggregated-overviews-and-exploration","text":"By the faceted search you get an aggregated overview for the different facets like concepts, persons, locations or organizations showing, how many of the found or all documents matching this entity.","title":"Analytics, aggregated overviews and exploration"},{"location":"doc/datamanagement/opendata/wikidata/#interactive-filters-faceted-search","text":"You can use this overviews or named entities as interactive filter to narrow down search results. So a click to a facet (i.e. an organization) will drill down the search results to fewer documents, matching this additional facet/filter, too.","title":"Interactive filters (faceted search)"},{"location":"doc/datamanagement/opendata/wikidata/#multilingual-labels-alternate-labels-and-aliases-for-semantic-search","text":"Since the used open standards for Semantic Web and Linked Data labels like names can be multilingual, so you will find the names in different languages. So for example after import of names of politicians from Wikidata the semantic search for Angela Merkel will find documents which contain Angela Dorothea Kasner , too, which is birth name of Angela Merkel before she married. Additionally documents which contain Angela Merkelowa or \u0410\u043d\u0433\u0435\u043b\u0430 \u041c\u0435\u0440\u043a\u0435\u043b\u044c will be found, too, which is Angela Merkel written in russian language.","title":"Multilingual labels, alternate labels and aliases for Semantic Search"},{"location":"doc/datamanagement/opendata/wikidata/#open-standard-for-linked-data-and-interoperability","text":"Since Wikidata is using open standards for Linked Data and interoperability of the Semantic Web, the structured data in XML/RDF format and linked data by RDFS and SKOS thesaurus standard is interoperable and can be used for configuration of interactive filters and analytics of your document set in Open Semantic Search out of the box.","title":"Open Standard for linked data and interoperability"},{"location":"doc/datamanagement/opendata/wikidata/#import-list-or-ontology-or-define-wikidata-query","text":"So you can import existing, shared or external list of names, thesauri, dictionaries, vocabularies or ontologies :","title":"Import list or ontology or define Wikidata query"},{"location":"doc/datamanagement/opendata/wikidata/#sparql-query-and-sparql-endpoint-triplestore","text":"Instead of uploading or referencing an yet downloaded graph or list of names, you can configure a SPARQL query on a triplestore (SPARQL endpoint) in the datasources user interface for ontololgies in the tab \"SPARQL\": Therefore you have to use SPARQL queries with CONSTRUCT (like you can see in our SPARQL example queries ) or DESCRIBE (warning: much more overhead if the knowledge graph has more properties and linked data than only properties and data of a thesaurus) so the SPARQL results includes the following standard properties that are used for analytics: rdfs:label and/or skos:altLabel and/or skos:hiddenLabel","title":"SPARQL query and SPARQL endpoint (triplestore)"},{"location":"doc/datamanagement/opendata/wikidata/#plain-text-lists","text":"If you work only with one single language you can export the results of a SELECT query for only a single field/property in CSV format (only one field, so you get a plain text list instead of a table) and import/upload/reference this plain text list of names to the Lists and Ontologies Manager, which is much more performant for this case, because you save download bandwidth and CPU resources while import.","title":"Plain text lists"},{"location":"doc/datamanagement/opendata/wikidata/#privacy-data-protection","text":"Open Semantic Search sends only the queries of the defined lists to the internet or linked data cloud and downloads the lists of names. Different to common cloud services for named entity recognition, all other computing is done by open source software on your own computer or server without giving away sensitive informations like document content, search queries and how often and what has been found. If even this list queries are sensitive information you can do downloads manually over Tor and import the data from the saved local files/graphs/lists by referencing or uploading them to the ontologies manager.","title":"Privacy &amp; data protection"},{"location":"doc/datamanagement/opendata/wikidata/#wikidata-sparql-queries-examples","text":"SPARQL Endpoint: https://query.wikidata.org/sparql","title":"Wikidata SPARQL queries examples"},{"location":"doc/datamanagement/opendata/wikidata/#politicians","text":"","title":"Politicians"},{"location":"doc/datamanagement/opendata/wikidata/#germany-german-politicians","text":"","title":"Germany (german politicians)"},{"location":"doc/datamanagement/opendata/wikidata/#members-of-german-parialment","text":"","title":"Members of german parialment"},{"location":"doc/datamanagement/opendata/wikidata/#deutscher-bundestag-bundestagsabgeordnete","text":"Current and former members of German parliament (Deutscher Bundestag) (>4000 names): PREFIX skos: <http://www.w3.org/2004/02/skos/core#> CONSTRUCT { ?uri rdfs:label ?label ; skos:prefLabel ?prefLabel ; skos:altLabel ?altLabel . } WHERE { ?uri wdt:P39 wd:Q1939555 . OPTIONAL { ?uri rdfs:label ?label . } OPTIONAL { ?uri skos:prefLabel ?prefLabel . } OPTIONAL { ?uri skos:altLabel ?altLabel . } } The following SPARQL alternate using DESCRIBE would work, too, but is not recommended and would cause much more overhead if the knowledge graph has more properties and linked data than only thesaurus properties and data (like it is the case with Wikidata, which is a universal knowledge graph with many different properties, not \"only\" a thesaurus with labels and only a few types of relations): ~~ DESCRIBE ?uri WHERE { ?uri wdt:P39 wd:Q1939555 . } ~~","title":"Deutscher Bundestag (Bundestagsabgeordnete)"},{"location":"doc/datamanagement/opendata/wikidata/#landtag-landtagsabgeordnete","text":"","title":"Landtag (Landtagsabgeordnete)"},{"location":"doc/datamanagement/opendata/wikidata/#baden-wurttemberg","text":"Current and former members of Landtag Baden-W\u00fcrttemberg: PREFIX skos: <http://www.w3.org/2004/02/skos/core#> CONSTRUCT { ?uri rdfs:label ?label ; skos:prefLabel ?prefLabel ; skos:altLabel ?altLabel . } WHERE { ?uri wdt:P39 wd:Q17481175 . OPTIONAL { ?uri rdfs:label ?label . } OPTIONAL { ?uri skos:prefLabel ?prefLabel . } OPTIONAL { ?uri skos:altLabel ?altLabel . } }","title":"Baden W\u00fcrttemberg"},{"location":"doc/datamanagement/opendata/wikidata/#list-of-politicians-of-other-countries-or-parliaments","text":"To find out more about queries for politicians of your country or your countries politicians are not yet available for download, you can help to add them to Wikidata in the working group Every Politician .","title":"List of politicians of other countries or parliaments"},{"location":"doc/datamanagement/rss/","text":"RSS-Feed Manager Managing imports of RSS newsfeeds RSS-Feed manager is a web app and user interface for managing imports of RSS-Newsfeeds. Usage Login Login to the admin interface. Choose the application \"RSS-Feed manager\". Add new Feeds Add a RSS-Feed or more Newsfeeds. Set the uri of the RSS-Feed. The title and description is optional and only for easier managing many feeds for humans. How often to import: Delta time If you dont want to import a feed on every cron run, set a delta time for the feed. Example: A delta time of 60 minutes will import the feed (and new news) only one time per hour or set to 0 minutes to import the feed on every cron run. Installation See the admin documentation . Roadmap Plans for the feature ( please donate ): We want to add tagging functionality to the RSS importer, so you can add metadata or tags to all news imported with this feed, i.e. a special topic). At the moment you can only setup tagging with rules because of the datasource URL (i.e. domain of a newspaper) or topics (i.e. keywords) but not additional tags only because the source of the articles was a special RSS-Newsfeed (i.e. a feed only for one topic, we would not find out the other ways and we can not find out because of URL of the article, because the domain has more then one topic). Additionally want to add tagging for RSS-Feed management, so you can organize many newsfeeds in the management interface with tags.","title":"RSS-Feed Manager"},{"location":"doc/datamanagement/rss/#rss-feed-manager","text":"","title":"RSS-Feed Manager"},{"location":"doc/datamanagement/rss/#managing-imports-of-rss-newsfeeds","text":"RSS-Feed manager is a web app and user interface for managing imports of RSS-Newsfeeds.","title":"Managing imports of RSS newsfeeds"},{"location":"doc/datamanagement/rss/#usage","text":"","title":"Usage"},{"location":"doc/datamanagement/rss/#login","text":"Login to the admin interface. Choose the application \"RSS-Feed manager\".","title":"Login"},{"location":"doc/datamanagement/rss/#add-new-feeds","text":"Add a RSS-Feed or more Newsfeeds. Set the uri of the RSS-Feed. The title and description is optional and only for easier managing many feeds for humans.","title":"Add new Feeds"},{"location":"doc/datamanagement/rss/#how-often-to-import-delta-time","text":"If you dont want to import a feed on every cron run, set a delta time for the feed. Example: A delta time of 60 minutes will import the feed (and new news) only one time per hour or set to 0 minutes to import the feed on every cron run.","title":"How often to import: Delta time"},{"location":"doc/datamanagement/rss/#installation","text":"See the admin documentation .","title":"Installation"},{"location":"doc/datamanagement/rss/#roadmap","text":"Plans for the feature ( please donate ): We want to add tagging functionality to the RSS importer, so you can add metadata or tags to all news imported with this feed, i.e. a special topic). At the moment you can only setup tagging with rules because of the datasource URL (i.e. domain of a newspaper) or topics (i.e. keywords) but not additional tags only because the source of the articles was a special RSS-Newsfeed (i.e. a feed only for one topic, we would not find out the other ways and we can not find out because of URL of the article, because the domain has more then one topic). Additionally want to add tagging for RSS-Feed management, so you can organize many newsfeeds in the management interface with tags.","title":"Roadmap"},{"location":"doc/datamanagement/rules/","text":"title: Rules: Rule based automatic tagging and automatic classification authors: - Markus Mandalka Rules: Rule based automatic tagging and automatic classification Setting search query based rules for automatic classification Since the Thesaurus editor and named entity manager allows not only to set keywords or names to extract known named entities like locations or organisations but flexible and powerful search operators for simple or complex queries you can use it for defining rules for automatic classification of new content or for easy and flexible definitions of news pipes or channels for example for alerts, automatically filtered newsfeeds for different channels, for further reading or automatic publishing. Classification or tagging with categories for navigation and interactive filters Since this flexibility you can use the query based tagging even for better navigation and overview trough exploratory search by additional search facets or flexible rule based aggregated overviews of differenced and / or grouped aspects, known as pivot tables. Tagging for alerts or newsfeeds For example you can setup to tag all new articles with a keywords or search queries like myCompanyname or with more words like myTown murder or more complex queries like myTown AND shooting AND NOT \"shooting star\" AND NOT \"photo shooting\" with the tag maybe urgent and/or important and subscribe only this tag as alert or newsfeed instead managing subscriptions or alerting of many queries. Groups In the admin interface you can add some or many of such rules to groups, i.e. to the group (or since groups can be part of a parent group even a taxonomy) \"Violence\". So you can manage many alerts and search queries with the admin interface and your users have only to subscribe one tag or group instead of subscribing each of the (new) queries. Alerts if new or changed number too small or too big Or if you indexing data sources like datasets or tables with columns with numbers you can subscribe alerts for new rows that are or get bigger or smaller than the value you set in your search query. Qualifying with automatic classification For example you can set rules like if source is a certain domain (with a search query like field URI like https://www.trusted-investigative-journalism-organisation.org/* ) then tag the article to the category \"proven source\". The other way you can classify content from advertising or boulevard media with the tag \"not a reliable source\" in the facet \"information quality\" News pipes So you can easy define newspipes: For example if there was an important alert for a special topic and a colleague who is specialist for that toppic verified that and tagged the new article with the tag \"verified\" mark it for a channel where more people will read it or are subscribed to or mark it for a news pipe for further verifying for other colleagues or further automatic filtering. Custom facets / fields / properties Rules can tag their values to the standard facet like Tags or Author or you can choose a custom facet to have a faceted search and navigation with different facets like \"Locations\" or \"Organisations\" or other custom facets.","title":"Index"},{"location":"doc/datamanagement/rules/#rules-rule-based-automatic-tagging-and-automatic-classification","text":"","title":"Rules: Rule based automatic tagging and automatic classification"},{"location":"doc/datamanagement/rules/#setting-search-query-based-rules-for-automatic-classification","text":"Since the Thesaurus editor and named entity manager allows not only to set keywords or names to extract known named entities like locations or organisations but flexible and powerful search operators for simple or complex queries you can use it for defining rules for automatic classification of new content or for easy and flexible definitions of news pipes or channels for example for alerts, automatically filtered newsfeeds for different channels, for further reading or automatic publishing.","title":"Setting search query based rules for automatic classification"},{"location":"doc/datamanagement/rules/#classification-or-tagging-with-categories-for-navigation-and-interactive-filters","text":"Since this flexibility you can use the query based tagging even for better navigation and overview trough exploratory search by additional search facets or flexible rule based aggregated overviews of differenced and / or grouped aspects, known as pivot tables.","title":"Classification or tagging with categories for navigation and interactive filters"},{"location":"doc/datamanagement/rules/#tagging-for-alerts-or-newsfeeds","text":"For example you can setup to tag all new articles with a keywords or search queries like myCompanyname or with more words like myTown murder or more complex queries like myTown AND shooting AND NOT \"shooting star\" AND NOT \"photo shooting\" with the tag maybe urgent and/or important and subscribe only this tag as alert or newsfeed instead managing subscriptions or alerting of many queries.","title":"Tagging for alerts or newsfeeds"},{"location":"doc/datamanagement/rules/#groups","text":"In the admin interface you can add some or many of such rules to groups, i.e. to the group (or since groups can be part of a parent group even a taxonomy) \"Violence\". So you can manage many alerts and search queries with the admin interface and your users have only to subscribe one tag or group instead of subscribing each of the (new) queries.","title":"Groups"},{"location":"doc/datamanagement/rules/#alerts-if-new-or-changed-number-too-small-or-too-big","text":"Or if you indexing data sources like datasets or tables with columns with numbers you can subscribe alerts for new rows that are or get bigger or smaller than the value you set in your search query.","title":"Alerts if new or changed number too small or too big"},{"location":"doc/datamanagement/rules/#qualifying-with-automatic-classification","text":"For example you can set rules like if source is a certain domain (with a search query like field URI like https://www.trusted-investigative-journalism-organisation.org/* ) then tag the article to the category \"proven source\". The other way you can classify content from advertising or boulevard media with the tag \"not a reliable source\" in the facet \"information quality\"","title":"Qualifying with automatic classification"},{"location":"doc/datamanagement/rules/#news-pipes","text":"So you can easy define newspipes: For example if there was an important alert for a special topic and a colleague who is specialist for that toppic verified that and tagged the new article with the tag \"verified\" mark it for a channel where more people will read it or are subscribed to or mark it for a news pipe for further verifying for other colleagues or further automatic filtering.","title":"News pipes"},{"location":"doc/datamanagement/rules/#custom-facets-fields-properties","text":"Rules can tag their values to the standard facet like Tags or Author or you can choose a custom facet to have a faceted search and navigation with different facets like \"Locations\" or \"Organisations\" or other custom facets.","title":"Custom facets / fields / properties"},{"location":"doc/datamanagement/tagging_results_of_search_query/","text":"Tagging by search query You don't have to tag each document of a search result manually i.e. by the tagger app to be able to use the interactive filter (faceted search) for the tag(s). How to tag all results of a search query automatically The module Multi-Tagger or Query-Tagger app searches for all results of a given search query and tags all found documents or files with the given tag in the given facet (i.e. \"Tags\", \"Organization\" or \"Location\") So you can tag many documents at once with easy or with complex search queries. Usage Insert a query (this can be a name or a complex search query ). Select the facet to which the tag should be added. Optionally insert a value or tag. Press submit and wait for a status how many documents were tagged. Use the user interface for creating search queries If you don't want to type in a complex query, use the standard search user interface with its comfortable web interface for exploratory search and interactive filters (facets) and press \"tag all results\" there or select the view \"show query\" if you want to copy paste it to change or add some parts. Thesaurus for management of automatic tagging or rules based classification This tool is for quick and dirty one time tagging, i.e. to structure an small one time investigation where no new documents will be indexed and where you don't have to store or manage this query. If you want to store, manage and reuse such tagging queries and tag automatically and continuesly for all new data or documents matching a search query you might want to use the thesaurus for named entities like organizations, persons, locations or concepts , doing the same but automatically.","title":"Tagging by search query"},{"location":"doc/datamanagement/tagging_results_of_search_query/#tagging-by-search-query","text":"You don't have to tag each document of a search result manually i.e. by the tagger app to be able to use the interactive filter (faceted search) for the tag(s).","title":"Tagging by search query"},{"location":"doc/datamanagement/tagging_results_of_search_query/#how-to-tag-all-results-of-a-search-query-automatically","text":"The module Multi-Tagger or Query-Tagger app searches for all results of a given search query and tags all found documents or files with the given tag in the given facet (i.e. \"Tags\", \"Organization\" or \"Location\") So you can tag many documents at once with easy or with complex search queries.","title":"How to tag all results of a search query automatically"},{"location":"doc/datamanagement/tagging_results_of_search_query/#usage","text":"Insert a query (this can be a name or a complex search query ). Select the facet to which the tag should be added. Optionally insert a value or tag. Press submit and wait for a status how many documents were tagged.","title":"Usage"},{"location":"doc/datamanagement/tagging_results_of_search_query/#use-the-user-interface-for-creating-search-queries","text":"If you don't want to type in a complex query, use the standard search user interface with its comfortable web interface for exploratory search and interactive filters (facets) and press \"tag all results\" there or select the view \"show query\" if you want to copy paste it to change or add some parts.","title":"Use the user interface for creating search queries"},{"location":"doc/datamanagement/tagging_results_of_search_query/#thesaurus-for-management-of-automatic-tagging-or-rules-based-classification","text":"This tool is for quick and dirty one time tagging, i.e. to structure an small one time investigation where no new documents will be indexed and where you don't have to store or manage this query. If you want to store, manage and reuse such tagging queries and tag automatically and continuesly for all new data or documents matching a search query you might want to use the thesaurus for named entities like organizations, persons, locations or concepts , doing the same but automatically.","title":"Thesaurus for management of automatic tagging or rules based classification"},{"location":"doc/datamanagement/thesaurus/","text":"Thesaurus editor for editing, structuring & linking vocabulary or dictionary of topics, concepts & names User interface for editing, linking and managing vocabulary (words, topics, terms or concepts) and named entities (i.e. persons or organizations) of a custom thesaurus for structuring semantic search & document analysis by domain knowledge Open Semantic Thesaurus Editor and Thesaurus Manager is the integrated Django (Python) based open source web app as user interfaces for editing, linking, management and structuring of a controlled vocabulary or domain knowledge. So you can manage important terms, words or concepts, names, topics, persons, organizations or places in a custom thesaurus for editing names, entities or concepts , their alternate labels like aliases (i.e. nickname), synonyms (a car is a vehicle, too) or typos and misspellings and its structure or relations like hyponyms (even if not every car is a truck, every truck is a vehicle, too). The structured data will be used for structuring faceted search and analysis or text mining of indexed documents and to be able to find more by semantic search by considering alternate spellings or synonyms: How to manage structure like names, organizations, locations, places, persons of interest or important words or concepts Therefore open the webapp clicking on \"Thesaurus\" in the topbar of the search interface and use its web user interface to insert or edit entities and to manage their aliases or groups. With the thesaurus user interfaces you can manage relevant named entities like organizations or company names, persons of interest, locations or concepts for automatic tagging (autotagging) and for automatic analysis for exploratory search: Easy adding of new entities: Just add a name, a word or a query Just add a new entity, set a name or word and save it: Just click on the \"Add new entity\" button (in your entity list or in the topbar of the entities manager), set a name like a company name or a concept like an interessting or important word into the field \"Name\" and click the \"Save\" button. All other more complex but very powerful features like alternate labels for synonyms, hidden labels for typos or relations to other concepts are optional. Structures search navigation and document analysis like overviews, interactive filters, semantic search (Faceted search by Thesaurus based Named Entity Extraction & Named Entity Linking) The entries of the thesaurus are used for automatic tagging for additional structure for analysis and named entity extraction or named entity linking for exploratory search or as tags for news pipes or alerts: Based on the thesaurus entries the named entity tagger or named entity extraction can find the name or label, alternate labels like synonyms and misspellings and add the name (prefered label) to the configured facets for aggregated overview, interactive filters and analytics. For example if you add the entity \"Open Semantic Search\" with/to the facet \"Software\", you will be able to use this entity or name as interactive filter and will get an aggregated overview of the count of documents matching this entity while searching for other queries. Additionally using the alternate labels, aliases or synonyms, the semantic search can not only find exactly the terms the user search for, but the search engine will find documents with alternate terms like synonyms, too. Alternate labels and synonyms Alternate spellings, aliases or synonyms of a concept can be added to \"Alternate labels\". Different fuzzy search methods are integrated to recommend alternate Labels . Manage misspellings or typos and errors by automatic textrecognition (OCR) Since no automatic textrecognition (OCR) is perfect and there are typos in documents you didn't write yourself and can not change or since your users will write terms wrong in their queries, you can manage typos and OCR errors: For each concept or name you can define hidden labels or misspellings i.e. with misspelled variants of the name or concept, so the name or concept will be found, too, even if it is misspelled in the documents. Recommender for misspelled names or OCR errors The thesaurus recommender can generate automatic recommendations of such hidden labels or misspelled names from variants of misspelled text inside your documents (corpus). Broader concept You can link to broader concepts up in the hierarchy or taxonomy. Narrower concept You can link to narrower concepts down in the hierarchy or taxonomy. You can use the recommender for compounded words to automatically find some recommendations for narrower terms you can add with one click. Related terms You can link to related terms which are not on higher level or lower level of the hierarchy or taxonomy. Recommendation of related terms Please donate for automatic recommendations of other related terms (often co-ocurences) which can be found by topic modelling (machine learning by LDA) Search query (optional) Mostly you need only to set a name of a entity to tag it for exploratory search, aggregated overviews and interactive filters (faceted search) . If there is no (optional) search query in the tab query, this name will be used as the search query per default. But optional you can set a little bit different (i.e. including wildcards) or more complex queries than the name in the tab \"Query\". For example \"shooting in my town\" AND NOT \"photo shooting in my town\" Custom tags (optional) - i.e. for alerts or newsfeeds or for classification For example you can setup to tag all new articles with a keywords or search queries like myCompanyname or with more words like myTown murder or more complex queries like myTown AND shooting AND NOT \"shooting star\" AND NOT \"photo shooting\" with the tag maybe urgent and or important and subscribe this tag as alert. Groups / Lists (optional) You can add named entities to groups. For example if you add the entity \"shooting\" to the group \"violence\", documents with \"shooting\" will be tagged with \"violence\", too, even if not containig the word violence. If you search or filter with the group \"violance\", documents with the named entity \"shooting\" (which was assigned to this group/list) and all other named entities assigned to this group will be found, too. So you get an aggregated overview by groups/lists or you are able to filter or search not only by single entities but by whole groups/lists. Taxonomies: Navigate and filter with groups, lists or collections and find and navigate subtopics (optional) Since you can set groups to a parent group, you can build a hierarchy or taxonomy to be able to find, overview or navigate subtopics. For example if you search or filter for the facet violence all terms of the group like \"shooting\" or \"murder\" will be found too and you get an aggregated overview over all forms together and over the different forms. Or for example if you add the entity or product \"Open Semantic Search\" with the alias \"@OpenSemSearch\" to the group \"Solr\" which has the parent group \"Search engine\" which has the parent group \"Open Source Software\" the documents with the content \"Open Semantic Search\" or tweets from @OpenSemSearch will be tagged and found with \"Open source\" and with \"search engine\" and \"Solr\" even if not containing this words. Import external lists of names, thesaurus or ontologies You don't need to enter each entity like persons, organizations, locations or concepts, if there is an exisiting list or if the entries are available in other structured format yet. You can import existing, shared or external list of names, thesauri, dictionaries, vocabularies or semantic web ontologies with the lists and ontologies manager : Learn more about how to import a lists, a dictionary, a vocabulary, a thesaurus or an ontology . Interoperability by open standard for a Simple Knowledge Organization System (SKOS) For interoperability reasons the thesaurus can be imported from and exported to the open standard formats Resource Description Framework (RDF) and Simple Knowledge Organization System (SKOS). Integrate and import thesauri from other RDF or SKOS compatible tools or open data sources Since the ontologies manager for named entities extraction is able to import the open semantic web standard Resource Description Framework (RDF) and Simple Knowledge Organization System (SKOS) you can import or integrate (additional) thesauri from other thesaurus editors or thesaurus management tools by importing their thesauri. Open thesauri and Open Data vocabularies You don't have to create an own thesaurus for common concepts or topics. In many cases it is less effort to integrate or import public thesauri from the linked open data (LOD) cloud. Export thesaurus to RDF and SKOS The thesaurus will be exportable to the open standard format Resource Description Framework (RDF) and Simple Knowledge Organization System (SKOS) , so you can use it in other software and knowledge management tools or share or publish it. Methods for managing a vocabulary for semantic search Thesaurus for linking concepts and managing aliases and synonyms Simple Knowledge Organization System (SKOS): Open standard for thesauri from W3C Managing Named Entities like names, persons, organizations, places, terms and concepts as database or knowledge base for Named Entity Linking Synonyms Hyponyms (narrower terms) Taxonomy (defining hierarchy or tree by links to narrower or broader terms) for taxonomy based search navigation Alternate free software and open source tools for thesaurus editing Alternate Simple Knowledge Organization System (SKOS) standard compatible free software open source tools and for editing and managing thesauri: * Skosmos (PHP & SPARQL) * SkosJS (Javascript & SPARQL) * Unilexicon * Vocbench * TemaTres * OpenSKOS * iQvoc (Ruby on rails) More in W3C Wiki ...","title":"Vocabulary and thesaurus (dictionary of names or concepts)"},{"location":"doc/datamanagement/thesaurus/#thesaurus-editor-for-editing-structuring-linking-vocabulary-or-dictionary-of-topics-concepts-names","text":"","title":"Thesaurus editor for editing, structuring &amp; linking vocabulary or dictionary of topics, concepts &amp; names"},{"location":"doc/datamanagement/thesaurus/#user-interface-for-editing-linking-and-managing-vocabulary-words-topics-terms-or-concepts-and-named-entities-ie-persons-or-organizations-of-a-custom-thesaurus-for-structuring-semantic-search-document-analysis-by-domain-knowledge","text":"Open Semantic Thesaurus Editor and Thesaurus Manager is the integrated Django (Python) based open source web app as user interfaces for editing, linking, management and structuring of a controlled vocabulary or domain knowledge. So you can manage important terms, words or concepts, names, topics, persons, organizations or places in a custom thesaurus for editing names, entities or concepts , their alternate labels like aliases (i.e. nickname), synonyms (a car is a vehicle, too) or typos and misspellings and its structure or relations like hyponyms (even if not every car is a truck, every truck is a vehicle, too). The structured data will be used for structuring faceted search and analysis or text mining of indexed documents and to be able to find more by semantic search by considering alternate spellings or synonyms:","title":"User interface for editing, linking and managing vocabulary (words, topics, terms or concepts) and named entities (i.e. persons or organizations) of a custom thesaurus for structuring semantic search &amp; document analysis by domain knowledge"},{"location":"doc/datamanagement/thesaurus/#how-to-manage-structure-like-names-organizations-locations-places-persons-of-interest-or-important-words-or-concepts","text":"Therefore open the webapp clicking on \"Thesaurus\" in the topbar of the search interface and use its web user interface to insert or edit entities and to manage their aliases or groups. With the thesaurus user interfaces you can manage relevant named entities like organizations or company names, persons of interest, locations or concepts for automatic tagging (autotagging) and for automatic analysis for exploratory search:","title":"How to manage structure like names, organizations, locations, places, persons of interest or important words or concepts"},{"location":"doc/datamanagement/thesaurus/#easy-adding-of-new-entities-just-add-a-name-a-word-or-a-query","text":"Just add a new entity, set a name or word and save it: Just click on the \"Add new entity\" button (in your entity list or in the topbar of the entities manager), set a name like a company name or a concept like an interessting or important word into the field \"Name\" and click the \"Save\" button. All other more complex but very powerful features like alternate labels for synonyms, hidden labels for typos or relations to other concepts are optional.","title":"Easy adding of new entities: Just add a name, a word or a query"},{"location":"doc/datamanagement/thesaurus/#structures-search-navigation-and-document-analysis-like-overviews-interactive-filters-semantic-search-faceted-search-by-thesaurus-based-named-entity-extraction-named-entity-linking","text":"The entries of the thesaurus are used for automatic tagging for additional structure for analysis and named entity extraction or named entity linking for exploratory search or as tags for news pipes or alerts: Based on the thesaurus entries the named entity tagger or named entity extraction can find the name or label, alternate labels like synonyms and misspellings and add the name (prefered label) to the configured facets for aggregated overview, interactive filters and analytics. For example if you add the entity \"Open Semantic Search\" with/to the facet \"Software\", you will be able to use this entity or name as interactive filter and will get an aggregated overview of the count of documents matching this entity while searching for other queries. Additionally using the alternate labels, aliases or synonyms, the semantic search can not only find exactly the terms the user search for, but the search engine will find documents with alternate terms like synonyms, too.","title":"Structures search navigation and document analysis like overviews, interactive filters, semantic search (Faceted search by Thesaurus based Named Entity Extraction &amp; Named Entity Linking)"},{"location":"doc/datamanagement/thesaurus/#alternate-labels-and-synonyms","text":"Alternate spellings, aliases or synonyms of a concept can be added to \"Alternate labels\". Different fuzzy search methods are integrated to recommend alternate Labels .","title":"Alternate labels and synonyms"},{"location":"doc/datamanagement/thesaurus/#manage-misspellings-or-typos-and-errors-by-automatic-textrecognition-ocr","text":"Since no automatic textrecognition (OCR) is perfect and there are typos in documents you didn't write yourself and can not change or since your users will write terms wrong in their queries, you can manage typos and OCR errors: For each concept or name you can define hidden labels or misspellings i.e. with misspelled variants of the name or concept, so the name or concept will be found, too, even if it is misspelled in the documents.","title":"Manage misspellings or typos and errors by automatic textrecognition (OCR)"},{"location":"doc/datamanagement/thesaurus/#recommender-for-misspelled-names-or-ocr-errors","text":"The thesaurus recommender can generate automatic recommendations of such hidden labels or misspelled names from variants of misspelled text inside your documents (corpus).","title":"Recommender for misspelled names or OCR errors"},{"location":"doc/datamanagement/thesaurus/#broader-concept","text":"You can link to broader concepts up in the hierarchy or taxonomy.","title":"Broader concept"},{"location":"doc/datamanagement/thesaurus/#narrower-concept","text":"You can link to narrower concepts down in the hierarchy or taxonomy. You can use the recommender for compounded words to automatically find some recommendations for narrower terms you can add with one click.","title":"Narrower concept"},{"location":"doc/datamanagement/thesaurus/#related-terms","text":"You can link to related terms which are not on higher level or lower level of the hierarchy or taxonomy.","title":"Related terms"},{"location":"doc/datamanagement/thesaurus/#recommendation-of-related-terms","text":"Please donate for automatic recommendations of other related terms (often co-ocurences) which can be found by topic modelling (machine learning by LDA)","title":"Recommendation of related terms"},{"location":"doc/datamanagement/thesaurus/#search-query-optional","text":"Mostly you need only to set a name of a entity to tag it for exploratory search, aggregated overviews and interactive filters (faceted search) . If there is no (optional) search query in the tab query, this name will be used as the search query per default. But optional you can set a little bit different (i.e. including wildcards) or more complex queries than the name in the tab \"Query\". For example \"shooting in my town\" AND NOT \"photo shooting in my town\"","title":"Search query (optional)"},{"location":"doc/datamanagement/thesaurus/#custom-tags-optional-ie-for-alerts-or-newsfeeds-or-for-classification","text":"For example you can setup to tag all new articles with a keywords or search queries like myCompanyname or with more words like myTown murder or more complex queries like myTown AND shooting AND NOT \"shooting star\" AND NOT \"photo shooting\" with the tag maybe urgent and or important and subscribe this tag as alert.","title":"Custom tags (optional) - i.e. for alerts or newsfeeds or for classification"},{"location":"doc/datamanagement/thesaurus/#groups-lists-optional","text":"You can add named entities to groups. For example if you add the entity \"shooting\" to the group \"violence\", documents with \"shooting\" will be tagged with \"violence\", too, even if not containig the word violence. If you search or filter with the group \"violance\", documents with the named entity \"shooting\" (which was assigned to this group/list) and all other named entities assigned to this group will be found, too. So you get an aggregated overview by groups/lists or you are able to filter or search not only by single entities but by whole groups/lists.","title":"Groups / Lists (optional)"},{"location":"doc/datamanagement/thesaurus/#taxonomies-navigate-and-filter-with-groups-lists-or-collections-and-find-and-navigate-subtopics-optional","text":"Since you can set groups to a parent group, you can build a hierarchy or taxonomy to be able to find, overview or navigate subtopics. For example if you search or filter for the facet violence all terms of the group like \"shooting\" or \"murder\" will be found too and you get an aggregated overview over all forms together and over the different forms. Or for example if you add the entity or product \"Open Semantic Search\" with the alias \"@OpenSemSearch\" to the group \"Solr\" which has the parent group \"Search engine\" which has the parent group \"Open Source Software\" the documents with the content \"Open Semantic Search\" or tweets from @OpenSemSearch will be tagged and found with \"Open source\" and with \"search engine\" and \"Solr\" even if not containing this words.","title":"Taxonomies: Navigate and filter with groups, lists or collections and find and navigate subtopics (optional)"},{"location":"doc/datamanagement/thesaurus/#import-external-lists-of-names-thesaurus-or-ontologies","text":"You don't need to enter each entity like persons, organizations, locations or concepts, if there is an exisiting list or if the entries are available in other structured format yet. You can import existing, shared or external list of names, thesauri, dictionaries, vocabularies or semantic web ontologies with the lists and ontologies manager : Learn more about how to import a lists, a dictionary, a vocabulary, a thesaurus or an ontology .","title":"Import external lists of names, thesaurus or ontologies"},{"location":"doc/datamanagement/thesaurus/#interoperability-by-open-standard-for-a-simple-knowledge-organization-system-skos","text":"For interoperability reasons the thesaurus can be imported from and exported to the open standard formats Resource Description Framework (RDF) and Simple Knowledge Organization System (SKOS).","title":"Interoperability by open standard for a Simple Knowledge Organization System (SKOS)"},{"location":"doc/datamanagement/thesaurus/#integrate-and-import-thesauri-from-other-rdf-or-skos-compatible-tools-or-open-data-sources","text":"Since the ontologies manager for named entities extraction is able to import the open semantic web standard Resource Description Framework (RDF) and Simple Knowledge Organization System (SKOS) you can import or integrate (additional) thesauri from other thesaurus editors or thesaurus management tools by importing their thesauri.","title":"Integrate and import thesauri from other RDF or SKOS compatible tools or open data sources"},{"location":"doc/datamanagement/thesaurus/#open-thesauri-and-open-data-vocabularies","text":"You don't have to create an own thesaurus for common concepts or topics. In many cases it is less effort to integrate or import public thesauri from the linked open data (LOD) cloud.","title":"Open thesauri and Open Data vocabularies"},{"location":"doc/datamanagement/thesaurus/#export-thesaurus-to-rdf-and-skos","text":"The thesaurus will be exportable to the open standard format Resource Description Framework (RDF) and Simple Knowledge Organization System (SKOS) , so you can use it in other software and knowledge management tools or share or publish it.","title":"Export thesaurus to RDF and SKOS"},{"location":"doc/datamanagement/thesaurus/#methods-for-managing-a-vocabulary-for-semantic-search","text":"Thesaurus for linking concepts and managing aliases and synonyms Simple Knowledge Organization System (SKOS): Open standard for thesauri from W3C Managing Named Entities like names, persons, organizations, places, terms and concepts as database or knowledge base for Named Entity Linking Synonyms Hyponyms (narrower terms) Taxonomy (defining hierarchy or tree by links to narrower or broader terms) for taxonomy based search navigation","title":"Methods for managing a vocabulary for semantic search"},{"location":"doc/datamanagement/thesaurus/#alternate-free-software-and-open-source-tools-for-thesaurus-editing","text":"Alternate Simple Knowledge Organization System (SKOS) standard compatible free software open source tools and for editing and managing thesauri: * Skosmos (PHP & SPARQL) * SkosJS (Javascript & SPARQL) * Unilexicon * Vocbench * TemaTres * OpenSKOS * iQvoc (Ruby on rails) More in W3C Wiki ...","title":"Alternate free software and open source tools for thesaurus editing"},{"location":"doc/datamanagement/thesaurus/recommender/","text":"Recommender for thesaurus entries Recommender for misspelled names or OCR errors Since no automatic textrecognition (OCR) is perfect and there are typos in documents you didn't write yourself and can not change or since your users will write terms wrong in their search queries, you can manage typos and OCR failures in the thesaurus : For each concept or name you can define hidden labels or misspellings i.e. with misspelled variants of the name or concept, so the name or concept will be found, too, even if it is misspelled in the documents. The recommender can generate automatic recommendations of such hidden labels of misspelled names by finding such typos by edit distance (levensthein distance) on variants of misspelled text inside your documents (text corpus). Grammar based recommender for alternate labels Using stemming you can analyse and add other word forms of same term despite morphology of verbs (decline) on the base of multiple stemmers. Compounded words linking by semi-automatic recommender For easy managing of compounded words, the recommender shows you automated recommendations for compounded words from your documents (corpus), so you can add and link them with only a few clicks. You have the choice to link them as a synonym or as a broader or narrower term. Compounded words are found by wildcards . Solr config To be able to work with the recommender you have to switch the option \"stored\" to true for the fields _text_ and stemmed in the managed-schema config and setup all cach-all fields for all stemmers you want to use with the option stored enabled.","title":"Recommender for thesaurus entries"},{"location":"doc/datamanagement/thesaurus/recommender/#recommender-for-thesaurus-entries","text":"","title":"Recommender for thesaurus entries"},{"location":"doc/datamanagement/thesaurus/recommender/#recommender-for-misspelled-names-or-ocr-errors","text":"Since no automatic textrecognition (OCR) is perfect and there are typos in documents you didn't write yourself and can not change or since your users will write terms wrong in their search queries, you can manage typos and OCR failures in the thesaurus : For each concept or name you can define hidden labels or misspellings i.e. with misspelled variants of the name or concept, so the name or concept will be found, too, even if it is misspelled in the documents. The recommender can generate automatic recommendations of such hidden labels of misspelled names by finding such typos by edit distance (levensthein distance) on variants of misspelled text inside your documents (text corpus).","title":"Recommender for misspelled names or OCR errors"},{"location":"doc/datamanagement/thesaurus/recommender/#grammar-based-recommender-for-alternate-labels","text":"Using stemming you can analyse and add other word forms of same term despite morphology of verbs (decline) on the base of multiple stemmers.","title":"Grammar based recommender for alternate labels"},{"location":"doc/datamanagement/thesaurus/recommender/#compounded-words-linking-by-semi-automatic-recommender","text":"For easy managing of compounded words, the recommender shows you automated recommendations for compounded words from your documents (corpus), so you can add and link them with only a few clicks. You have the choice to link them as a synonym or as a broader or narrower term. Compounded words are found by wildcards .","title":"Compounded words linking by semi-automatic recommender"},{"location":"doc/datamanagement/thesaurus/recommender/#solr-config","text":"To be able to work with the recommender you have to switch the option \"stored\" to true for the fields _text_ and stemmed in the managed-schema config and setup all cach-all fields for all stemmers you want to use with the option stored enabled.","title":"Solr config"},{"location":"doc/datavisualization/","text":"Data visualization Datavisualization of search results or analytics results Integrated visualization tools to visualize data or visualize text with charts, graphs or wordclouds: Trend chart (how many documents when) Trend chart : When how many results or documents? Networks, relations and connections (graph visualization) The visual graph explorer for graph visualization, discovery and exploration of connections generates data visualizations of networks, connections and relations between named entities like persons, organizations or tags from the content of your documents. Word list and word cloud (words) Word list and word cloud : Most used words, concept or names Visualize text for text mining (content) Visualize text for text analysis, text mining, document mining and words : What content? Open Source Datavisualization tools for quantitative data Open Semantic Search focus is on information retrieval, text analysis and text mining of mostly qualitative data from text documents wherefore the data sources can be vast amounts or masses of documents and considering their size of the corpus or data volume big data, too. But for interactive data analytics and datavisualizations of quantitative data like numbers and diagrams you should use other open source data analytics and data visualization tools like Apache Zeppelin supporting different data stores, Kibana for Elastic Search or Banana for Solr focused more on numbers and quantity than on text and quality.","title":"Data visualization"},{"location":"doc/datavisualization/#data-visualization","text":"","title":"Data visualization"},{"location":"doc/datavisualization/#datavisualization-of-search-results-or-analytics-results","text":"Integrated visualization tools to visualize data or visualize text with charts, graphs or wordclouds:","title":"Datavisualization of search results or analytics results"},{"location":"doc/datavisualization/#trend-chart-how-many-documents-when","text":"Trend chart : When how many results or documents?","title":"Trend chart (how many documents when)"},{"location":"doc/datavisualization/#networks-relations-and-connections-graph-visualization","text":"The visual graph explorer for graph visualization, discovery and exploration of connections generates data visualizations of networks, connections and relations between named entities like persons, organizations or tags from the content of your documents.","title":"Networks, relations and connections (graph visualization)"},{"location":"doc/datavisualization/#word-list-and-word-cloud-words","text":"Word list and word cloud : Most used words, concept or names","title":"Word list and word cloud (words)"},{"location":"doc/datavisualization/#visualize-text-for-text-mining-content","text":"Visualize text for text analysis, text mining, document mining and words : What content?","title":"Visualize text for text mining (content)"},{"location":"doc/datavisualization/#open-source-datavisualization-tools-for-quantitative-data","text":"Open Semantic Search focus is on information retrieval, text analysis and text mining of mostly qualitative data from text documents wherefore the data sources can be vast amounts or masses of documents and considering their size of the corpus or data volume big data, too. But for interactive data analytics and datavisualizations of quantitative data like numbers and diagrams you should use other open source data analytics and data visualization tools like Apache Zeppelin supporting different data stores, Kibana for Elastic Search or Banana for Solr focused more on numbers and quantity than on text and quality.","title":"Open Source Datavisualization tools for quantitative data"},{"location":"doc/desktop_search/","text":"Open Semantic Desktop Search (VM) Free software for your own desktop search engine for full text search, exploratory search and text analysis on Windows or Mac Open Semantic Desktop Search is free open source software for your own desktop search engine with integrated text analytics and research tools for full text search, exploratory search & text mining in large document sets, many PDF files, Word documents and many other file formats on Windows or Mac. Desktop search engine package as virtual machine for single Linux, Windows or Mac users The free software Open Semantic Desktop Search based on Open Semantic Search is the all in one package for desktop users (including Solr search server, user interfaces, open source search tools and connectors) as virtual machine image for full text search, exploratory search, analytics and text mining in many documents on your own desktop computer or notebook on Linux , Windows or iOS (Mac) . Installation and configuration Like described in the tutorial with screenshots how to install and configure the Open Semantic Desktop Search virtual machine just import the appliance file into Virtual Box and in the settings of the virtual machine add shared folders pointing to your documents directory or directories. Starting the search engine Start Virtual Box Start the virtual machine (VM) \" Open Semantic Desktop Search \" Search, explore and analyse Use powerful research tools for full text search, exploration, discovery, analysis, text mining and document mining Index documents Indexing documents from all configured shared folders will be started automatically after starting the Desktop Search virtual machine. You can index new documents which were added after the start of the virtual machine without need to restart: Click or touch the menu Activities . Click or touch the launcher \" Index all documents for search \" for recrawling all documents and subfolders in your document folder Launch the search user interface The search interface will be started automatically after new documents were added to the document processing queue. If you close the browser, you can open the search interface again: Launch the search user interface by clicking the launcher \" Search for documents \" or one of the other powerful research tools.","title":"Open Semantic Desktop Search"},{"location":"doc/desktop_search/#open-semantic-desktop-search-vm","text":"","title":"Open Semantic Desktop Search (VM)"},{"location":"doc/desktop_search/#free-software-for-your-own-desktop-search-engine-for-full-text-search-exploratory-search-and-text-analysis-on-windows-or-mac","text":"Open Semantic Desktop Search is free open source software for your own desktop search engine with integrated text analytics and research tools for full text search, exploratory search & text mining in large document sets, many PDF files, Word documents and many other file formats on Windows or Mac.","title":"Free software for your own desktop search engine for full text search, exploratory search and text analysis on Windows or Mac"},{"location":"doc/desktop_search/#desktop-search-engine-package-as-virtual-machine-for-single-linux-windows-or-mac-users","text":"The free software Open Semantic Desktop Search based on Open Semantic Search is the all in one package for desktop users (including Solr search server, user interfaces, open source search tools and connectors) as virtual machine image for full text search, exploratory search, analytics and text mining in many documents on your own desktop computer or notebook on Linux , Windows or iOS (Mac) .","title":"Desktop search engine package as virtual machine for single Linux, Windows or Mac users"},{"location":"doc/desktop_search/#installation-and-configuration","text":"Like described in the tutorial with screenshots how to install and configure the Open Semantic Desktop Search virtual machine just import the appliance file into Virtual Box and in the settings of the virtual machine add shared folders pointing to your documents directory or directories.","title":"Installation and configuration"},{"location":"doc/desktop_search/#starting-the-search-engine","text":"Start Virtual Box Start the virtual machine (VM) \" Open Semantic Desktop Search \"","title":"Starting the search engine"},{"location":"doc/desktop_search/#search-explore-and-analyse","text":"Use powerful research tools for full text search, exploration, discovery, analysis, text mining and document mining","title":"Search, explore and analyse"},{"location":"doc/desktop_search/#index-documents","text":"Indexing documents from all configured shared folders will be started automatically after starting the Desktop Search virtual machine. You can index new documents which were added after the start of the virtual machine without need to restart: Click or touch the menu Activities . Click or touch the launcher \" Index all documents for search \" for recrawling all documents and subfolders in your document folder","title":"Index documents"},{"location":"doc/desktop_search/#launch-the-search-user-interface","text":"The search interface will be started automatically after new documents were added to the document processing queue. If you close the browser, you can open the search interface again: Launch the search user interface by clicking the launcher \" Search for documents \" or one of the other powerful research tools.","title":"Launch the search user interface"},{"location":"doc/modules/","text":"Search engine components and architecture Open source search engine architecture (components and modules) and processing (data integration, data analysis and data enrichment) Architecture overview Overview of services and main components The relations in this chart show dependencies and connections between services and main components witch show different directions than the data flow (see another flowchart of document processing and data flow ). flowchart TB subgraph CONTAINER_UI [User interface] direction TB subgraph COMPONENT_WEBSERVER [Apache webserver] direction TB subgraph COMPONENT_DJANGO[Python Django] COMPONENT_APPS[Web apps] COMPONENT_DJANGO_DB[(Django DB)] COMPONENT_APPS ----> COMPONENT_DJANGO_DB end subgraph COMPONENT_PHP[PHP] COMPONENT_SEARCH_UI[Solr-PHP-UI] end end end subgraph CONTAINER_SOLR [Apache Solr] direction TB COMPONENT_SOLR[Solr Server] click COMPONENT_SOLR \"../../solr\" COMPONENT_SOLR --> COMPONENT_SOLR_DOCUMENT_INDEX COMPONENT_SOLR --> COMPONENT_SOLR_ENTITIES_INDEX COMPONENT_SOLR_DOCUMENT_INDEX[(Document index)] COMPONENT_SOLR_ENTITIES_INDEX[(Entities index)] end subgraph CONTAINER_ETL [Open Semantic ETL] direction TB COMPONENT_OPENSEMANTICETL_FILECRAWLER[File crawler] COMPONENT_OPENSEMANTICETL_FILECRAWLER --> COMPONENT_CELERY COMPONENT_OPENSEMANTICETL_WORKER[Open Semantic ETL worker] COMPONENT_OPENSEMANTICETL_PLUGINS[ETL plugins] COMPONENT_OPENSEMANTICETL_WORKER --> COMPONENT_OPENSEMANTICETL_PLUGINS COMPONENT_CELERY[Celery task manager] click COMPONENT_CELERY \"../admin/queue/\" COMPONENT_OPENSEMANTICETL_WORKER --> COMPONENT_CELERY end subgraph CONTAINER_QUEUE [RabbitMQ] direction TB COMPONENT_RABBITMQ[RabbitMQ] click COMPONENT_RABBITMQ \"../admin/queue/\" COMPONENT_RABBITMQ --> COMPONENT_RABBITMQ_DATA COMPONENT_RABBITMQ_DATA[(Task queue)] click COMPONENT_RABBITMQ \"../admin/queue/\" end subgraph CONTAINER_TIKA [Apache Tika] direction TB COMPONENT_TIKA_SERVER[Tika Server] click COMPONENT_TIKA_SERVER \"https://github.com/opensemanticsearch/tika-server.deb\" COMPONENT_TIKA_SERVER --> COMPONENT_OCR_CACHE COMPONENT_OCR_CACHE[Tesseract OCR Cache] COMPONENT_OCR_CACHE --> COMPONENT_OCR COMPONENT_OCR_CACHE_DATA[(OCR cache)] COMPONENT_OCR_CACHE ----> COMPONENT_OCR_CACHE_DATA COMPONENT_OCR[Tesseract] click COMPONENT_OCR \"https://github.com/opensemanticsearch/tesseract-ocr-cache\" end subgraph CONTAINER_NEO4J [Neo4j] direction TB COMPONENT_NEO4J[Neo4J] click COMPONENT_NEO4J \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/export_neo4j.py\" COMPONENT_NEO4J --> COMPONENT_NEO4J_DATA COMPONENT_NEO4J_DATA[(Graph Database)] end subgraph CONTAINER_NER [SpaCy NLP] direction TB COMPONENT_NER[spacy-services] COMPONENT_NER_MODELS[(ML models)] COMPONENT_NER --> COMPONENT_NER_MODELS end COMPONENT_EL[Open Semantic Entity Search API] click COMPONENT_EL \"https://github.com/opensemanticsearch/open-semantic-entity-search-api\" COMPONENT_OPENSEMANTICETL_PLUGINS -->|Get tags and annotations| COMPONENT_APPS COMPONENT_OPENSEMANTICETL_PLUGINS -->|Entity extraction by thesaurus and ontologies| COMPONENT_EL COMPONENT_OPENSEMANTICETL_PLUGINS ---->|Metadata and text extraction| COMPONENT_TIKA_SERVER COMPONENT_OPENSEMANTICETL_PLUGINS ---->|Named entity recognition| COMPONENT_NER COMPONENT_OPENSEMANTICETL_PLUGINS ------>|Index data| COMPONENT_SOLR COMPONENT_CELERY ------>|Read and write task queue| COMPONENT_RABBITMQ COMPONENT_OPENSEMANTICETL_PLUGINS ------>|Index data| COMPONENT_NEO4J COMPONENT_EL -->|Extract entities in entities index from full text| COMPONENT_SOLR COMPONENT_SEARCH_UI -->|Search queries| COMPONENT_SOLR COMPONENT_APPS -->|Read search queries| COMPONENT_SOLR COMPONENT_APPS -->|Write entities managed by thesaurus or ontologies| COMPONENT_SOLR Flowchart of document processing and data flow flowchart TD FILEMONITORING[Filesystem monitoring] click FILEMONITORING \"../../trigger/filemonitoring/\" FILEMONITORING-->|Immediatelly add task if changed or new file| CELERY SCHEDULER[Cron scheduler] click SCHEDULER \"https://github.com/opensemanticsearch/open-semantic-search-apps/blob/master/etc/cron.d/open-semantic-search\" SCHEDULER -->|Regularly start crawler| FILECRAWLER FILECRAWLER[File directory crawler] click FILECRAWLER \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/etl_filedirectory.py\" FILECRAWLER -->|Add task for each new or changed file in crawled directory| CELERY CELERY[Celery task manager] click CELERY \"../admin/queue/\" CELERY -->|Parallel processing of files by multiple ETL workers| ETL_WORKER CELERY --> RABBITMQ RABBITMQ[(RabbitMQ task queue)] click RABBITMQ \"../admin/queue/\" RABBITMQ --> CELERY ETL_WORKER[Open Semantic ETL worker] click ETL_WORKER \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/tasks.py\" ETL_WORKER -->|Running configured plugins one by one| TIKA subgraph TIKA [Apache Tika for text extraction and metadata extraction] direction LR TIKA_PLUGIN[ETL plugin calling Tika] click TIKA_PLUGIN \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/enhance_extract_text_tika_server.py\" TIKA_PLUGIN -->|Document file| TIKA_SERVER TIKA_SERVER[Apache Tika Server] click TIKA_SERVER \"https://github.com/opensemanticsearch/tika-server.deb\" TIKA_SERVER -->|Image files or images in PDF|OCR TIKA_SERVER -->|Extracted text| TIKA_PLUGIN OCR[Tesseract OCR] click OCR \"https://github.com/opensemanticsearch/tesseract-ocr-cache\" OCR-->|Recognized plain text| TIKA_SERVER end TIKA -->|Extracted text and metadata| EntitySearchAPI subgraph EntitySearchAPI [Named Entity Extraction by lists of names, thesaurus and ontologies] direction LR EL_PLUGIN[ETL plugin for entity extraction] click EL_PLUGIN \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/enhance_entity_linking.py\" EL_PLUGIN -->|Plain text| EL EL[Open Semantic Entity Search API] click EL \"https://github.com/opensemanticsearch/open-semantic-entity-search-api\" EL -->|Extracted entities| EL_PLUGIN THESAURUS[(Thesaurus)] click THESAURUS \"https://github.com/opensemanticsearch/open-semantic-search-apps/blob/master/src/thesaurus/models.py\" THESAURUS -->|SKOS| EL ONTOLOGIES[(Ontologies)] click ONTOLOGIES \"https://github.com/opensemanticsearch/open-semantic-search-apps/blob/master/src/ontologies/models.py\" ONTOLOGIES -->|RDF| EL end EntitySearchAPI -->|Added extracted named entities by lists of names, thesaurus and ontologies| NER NER[ETL plugin for spaCy Named Entity Recognition by Machine Learning] click NER \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/enhance_ner_spacy.py\" NER -->|Added recognized named entities| ANNOTATIONS subgraph ANNOTATIONS [Get tags and annotations for this documents made by humans] direction RL ANNOTATIONS_DB[(DB with tags and annotations)] click ANNOTATIONS_DB \"https://github.com/opensemanticsearch/open-semantic-search-apps/blob/master/src/annotate/models.py\" ANNOTATIONS_DB --> ANNOTATIONS_PLUGIN ANNOTATIONS_PLUGIN[ETL enrichment plugin getting tags and annotations] click ANNOTATIONS_PLUGIN \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/enhance_annotations.py\" end ANNOTATIONS -->|Added tags and annotations| ANALYSIS_PLUGIN ANALYSIS_PLUGIN[ETL data analysis plugin like extraction amounts of money] ANALYSIS_PLUGIN -->|Added extracted amounts of money| OTHER_PLUGINS OTHER_PLUGINS[Other configured ETL Plugins] OTHER_PLUGINS -->|Plain text and strucured data| EXPORTER EXPORTER[Exporter plugins] EXPORTER -->|Index data for full text search and faceting| SOLR EXPORTER -->|Index data for full text search and faceting| ELASTICSEARCH EXPORTER -->|Index linked data for graph search| NEO4J SOLR[(Apache Solr document index)] click SOLR \"../../solr\" SOLR -->|Search results| UI UI[Web user interface for search] UI -->|Solr search query| SOLR ELASTICSEARCH[(Alternate Elastic Search)] click ELASTICSEARCH \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/export_elasticsearch.py\" NEO4J[(Neo4J Graph Database)] click NEO4J \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/export_neo4j.py\" Components and Modules User Interface : Client and user interface Search query forms : Search query form for full text search Explorer and navigator : Search with full text search and navigate (exploratory search) the index or search results with interactive filters (facets) Viewers : Parts of the UI to show different views (i.e. analytics like wordlcouds or trend charts) and previews for special formats (i.e. photos, documents, email ...) Code: /solr-php-ui/templates/ Annotators : Web Apps for tagging documents or CMS with forms and fields to manage meta data like tags or annotations Search Apps : Applications and user interfaces for search like search with lists tool or named entities manager Index and search server (Solr or Elastic Search) : Search server managing the index (indexer) and running search queries (query handler) Datamodel/Schema: src/solr.deb/var/solr/data/opensemanticsearch/conf/managed-schema Storage: /var/solr/data Log: /var/solr/logs/ Open Semantic ETL : Framework for data integration, data analysis, data enrichment and ETL (Extract, transform, load) pipelines or chains Connectors, importers, ingestors or crawlers : Import data from a data source (i.e. file system, file directory, file share, website or newsfeed) Parsers: Apache Tika to extract text and metadata from different file formats and document formats Entity extraction and entity linking : Open Semantic Entity Search API Data enrichment plugins and enhancer : Enhancing content with additional data like meta data (i.e. tagging or annotations) or analytics (i.e. OCR) ETL Exporter or Loader for Solr or Elastic Search : Indexing the data to search index Trigger : Your CMS or your file system (file system monitoring) will notify the web service (API) when there is new data or when content changed, so you dont have to burn resources for recrawl often to be able to find new or changed content very soon Web services (REST-API) : Available via standard network protocol HTTP and waiting until you (i.e. using the web admin interface) or another service (i.e. using the REST-API) demands actions like crawling a directory or a webpage and starting this actions Queue manager (Celery on RabbitMQ) : Managing task queue and starting of text extraction, analysis, data enrichment and indexing jobs by the right balance of parallel workers Scheduler : Managing starting of scheduled indexing jobs. This can be crontab for Cron starting the command line tools. Config: /etc/cron.d/open-semantic-search Document processing, extract, transform, load (ETL) and enhancing by data enrichment and data analysis How (new) data is handled by this components and ETL (extract, transform, load), document processing, data analysis and data enrichment : A user manually or a Cron daemon automatically from time to time starts a command The command line tools or the web API getting this command starts a ETL (extract, transform, load), data analysis and data enrichment chain to import, analyze and index data A input plugin or connector (i.e. the connector for the file system or the connector for a website) reads from its datasource The connectors, an Apache Tika parser, or a file format based data converter or extractor extracts data from the given document or file format The ETL framework calls all configured enhancer plugins for data enrichment to get additional analysis for the data or annotations to this data from a CMS. The output storage plugin or indexer index the text and metadata to the Solr index or to the Elastic Search index , so all other tools can search this data The user uses a user interface like the search user interface, the search apps or some other tools to search based on the search API of this index Services and Microservices Linux services: tika - Text extraction and OCR tika-fake-ocr - Text extraction without OCR solr - Search index spacy-services - spaCy NLP opensemanticetl - ETL workers rabbitmq-server - Task queue flower - Task queue monitoring user interface apache2 - Search UI - Search apps (f.e. thesaurus app or config UI) - Entity Search API User Interface and search applications Solr-PHP-UI User Interface (supports responsive design for mobiles and tablets) for search, facetted search, preview, different views and visualizations. Based on Solr client solr-php-client (pure vanilla php) and standard User Interfaces (HTML5 and CSS with Zurb Foundation ) and visualization libraries ( D3js ) so you can install and run it on standard PHP webspace without effort and without often not available special PHP-modules) Learn more ... Documentation: Howto seach Deployment /usr/share/solr-php-ui Log: /var/log/apache2/ Sourcecode: src/solr-php-ui (Github...) Index server Solr search server Preconfigured Solr Server running as daemon (so you have only to install the package and no further configuration needed) Learn more ... Annotation Tools for editing and managing metadata like tags, notes, relations and content structure (i.e. taxonomies): Open Semantic Tagger Tagger is a light weight responsive web app for tagging web pages and documents. Learn more ... Connectors Crawler, connectors, data importer and converter: Connector Files (with OCR) Crawl and index directories, files and documents into Solr. Including automatic textrecognition (OCR) support for images and grafical formats included in PDF documents (i.e. scans) Learn more ... Connector RSS (RSS-Feed) Indexes Webpages from a RSS-Newsfeed Learn more ... Connector Web (HTTP) Crawl and index Websites into Solr index. Learn more ... Connector DB (SQL, MySQL, Postgresql) Index SQL databases like MySQL or PostgreSQL into Solr. Learn more ... Connector Scraper (Scraping with Scrapy) ETL and webscraping framework to crawl, extract, transform and load structured data from websites (scraping). Learn more ... Scheduler If you use our connectors and want most flexibility use Cron and write a cronjob using our command line tools within a crontab or call our REST-API within another webservice (i.e. webcron). Queue manager Reads and manages trigger signals for starting indexing queued files by batch mode (parallel processing but because of limited RAM resources with a maximum count of workers/processes at same time) with opensemanticsearch-etl-file. Filenames can be appended to the queue by the REST API, Webinterface or command line tool. Learn more ... Data enrichment (Enhancer) Will enhance the indexed content with meta data or analytics Learn more ... Enhancer OCR Automatic textrecognition (OCR) for image files and images and graphics inside PDF (i.e. scans). Learn more ... Enhancer RDF Will enhance content with metadata in Resource Description Framework (RDF) format stored on a meta data server (i.e. tags and annotations in a Semantic Mediawiki or in Drupal CMS ) Learn more ... Enhancer XMP sidecar files Metadata like tags or descriptions for photos are often saved in XMP (Extensible Metadata Plattform) sidecar files (i.e. by Adobe Photoshop Lightroom . This enhancer adds the metadata of this sidecar files to the index of the original document. Learn more ... Enhancer ZIP This enhancer recognizes and unzips zip archives to index documents and files inside a zip files, too. Learn more ... Web Services Web admin interface Admin interface to start actions like crawling a directory or a webpage via web interface without command line tools and starting this actions. Learn more ... REST-API Application programming interface (API) available via generic and standard network protocol HTTP and waiting until another (web) service or software demands for an action like crawling a directory or a webpage or indexing changed data (i.e. directly started after data change by a trigger of the cms) and starting this actions. Learn more ... Trigger Using triggers you don't need to recrawl often to be able to find new or changed content within seconds: If there are hundreds of Gigabytes or some Terabytes of data and millions of files, standard recrawls can take hours in which your document can not be found and eat many resources. With triggers that works the other way: your CMS or file server will send a signal if there is new content or a litte part has changed and the queue manager will index only this file or page very soon. Trigger Filemonitoring File system monitoring based on itnotify . Monitors files and file folders and index them (again), so that new or changed documents or files can be found within seconds and without recrawl often (which would burn many ressources). Learn more ... Trigger Drupal After saving a page the Drupal module notifies the search engine about changed or new content. Learn more ... Generic triggers Like for Drupal (see before) there are generic trigger modules available for many other software projects, too. So install them and configure them to the URL of our REST-API to recrawl changed data of the other software or webservices.","title":"Search engine components and architecture"},{"location":"doc/modules/#search-engine-components-and-architecture","text":"Open source search engine architecture (components and modules) and processing (data integration, data analysis and data enrichment)","title":"Search engine components and architecture"},{"location":"doc/modules/#architecture-overview","text":"","title":"Architecture overview"},{"location":"doc/modules/#overview-of-services-and-main-components","text":"The relations in this chart show dependencies and connections between services and main components witch show different directions than the data flow (see another flowchart of document processing and data flow ). flowchart TB subgraph CONTAINER_UI [User interface] direction TB subgraph COMPONENT_WEBSERVER [Apache webserver] direction TB subgraph COMPONENT_DJANGO[Python Django] COMPONENT_APPS[Web apps] COMPONENT_DJANGO_DB[(Django DB)] COMPONENT_APPS ----> COMPONENT_DJANGO_DB end subgraph COMPONENT_PHP[PHP] COMPONENT_SEARCH_UI[Solr-PHP-UI] end end end subgraph CONTAINER_SOLR [Apache Solr] direction TB COMPONENT_SOLR[Solr Server] click COMPONENT_SOLR \"../../solr\" COMPONENT_SOLR --> COMPONENT_SOLR_DOCUMENT_INDEX COMPONENT_SOLR --> COMPONENT_SOLR_ENTITIES_INDEX COMPONENT_SOLR_DOCUMENT_INDEX[(Document index)] COMPONENT_SOLR_ENTITIES_INDEX[(Entities index)] end subgraph CONTAINER_ETL [Open Semantic ETL] direction TB COMPONENT_OPENSEMANTICETL_FILECRAWLER[File crawler] COMPONENT_OPENSEMANTICETL_FILECRAWLER --> COMPONENT_CELERY COMPONENT_OPENSEMANTICETL_WORKER[Open Semantic ETL worker] COMPONENT_OPENSEMANTICETL_PLUGINS[ETL plugins] COMPONENT_OPENSEMANTICETL_WORKER --> COMPONENT_OPENSEMANTICETL_PLUGINS COMPONENT_CELERY[Celery task manager] click COMPONENT_CELERY \"../admin/queue/\" COMPONENT_OPENSEMANTICETL_WORKER --> COMPONENT_CELERY end subgraph CONTAINER_QUEUE [RabbitMQ] direction TB COMPONENT_RABBITMQ[RabbitMQ] click COMPONENT_RABBITMQ \"../admin/queue/\" COMPONENT_RABBITMQ --> COMPONENT_RABBITMQ_DATA COMPONENT_RABBITMQ_DATA[(Task queue)] click COMPONENT_RABBITMQ \"../admin/queue/\" end subgraph CONTAINER_TIKA [Apache Tika] direction TB COMPONENT_TIKA_SERVER[Tika Server] click COMPONENT_TIKA_SERVER \"https://github.com/opensemanticsearch/tika-server.deb\" COMPONENT_TIKA_SERVER --> COMPONENT_OCR_CACHE COMPONENT_OCR_CACHE[Tesseract OCR Cache] COMPONENT_OCR_CACHE --> COMPONENT_OCR COMPONENT_OCR_CACHE_DATA[(OCR cache)] COMPONENT_OCR_CACHE ----> COMPONENT_OCR_CACHE_DATA COMPONENT_OCR[Tesseract] click COMPONENT_OCR \"https://github.com/opensemanticsearch/tesseract-ocr-cache\" end subgraph CONTAINER_NEO4J [Neo4j] direction TB COMPONENT_NEO4J[Neo4J] click COMPONENT_NEO4J \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/export_neo4j.py\" COMPONENT_NEO4J --> COMPONENT_NEO4J_DATA COMPONENT_NEO4J_DATA[(Graph Database)] end subgraph CONTAINER_NER [SpaCy NLP] direction TB COMPONENT_NER[spacy-services] COMPONENT_NER_MODELS[(ML models)] COMPONENT_NER --> COMPONENT_NER_MODELS end COMPONENT_EL[Open Semantic Entity Search API] click COMPONENT_EL \"https://github.com/opensemanticsearch/open-semantic-entity-search-api\" COMPONENT_OPENSEMANTICETL_PLUGINS -->|Get tags and annotations| COMPONENT_APPS COMPONENT_OPENSEMANTICETL_PLUGINS -->|Entity extraction by thesaurus and ontologies| COMPONENT_EL COMPONENT_OPENSEMANTICETL_PLUGINS ---->|Metadata and text extraction| COMPONENT_TIKA_SERVER COMPONENT_OPENSEMANTICETL_PLUGINS ---->|Named entity recognition| COMPONENT_NER COMPONENT_OPENSEMANTICETL_PLUGINS ------>|Index data| COMPONENT_SOLR COMPONENT_CELERY ------>|Read and write task queue| COMPONENT_RABBITMQ COMPONENT_OPENSEMANTICETL_PLUGINS ------>|Index data| COMPONENT_NEO4J COMPONENT_EL -->|Extract entities in entities index from full text| COMPONENT_SOLR COMPONENT_SEARCH_UI -->|Search queries| COMPONENT_SOLR COMPONENT_APPS -->|Read search queries| COMPONENT_SOLR COMPONENT_APPS -->|Write entities managed by thesaurus or ontologies| COMPONENT_SOLR","title":"Overview of services and main components"},{"location":"doc/modules/#flowchart-of-document-processing-and-data-flow","text":"flowchart TD FILEMONITORING[Filesystem monitoring] click FILEMONITORING \"../../trigger/filemonitoring/\" FILEMONITORING-->|Immediatelly add task if changed or new file| CELERY SCHEDULER[Cron scheduler] click SCHEDULER \"https://github.com/opensemanticsearch/open-semantic-search-apps/blob/master/etc/cron.d/open-semantic-search\" SCHEDULER -->|Regularly start crawler| FILECRAWLER FILECRAWLER[File directory crawler] click FILECRAWLER \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/etl_filedirectory.py\" FILECRAWLER -->|Add task for each new or changed file in crawled directory| CELERY CELERY[Celery task manager] click CELERY \"../admin/queue/\" CELERY -->|Parallel processing of files by multiple ETL workers| ETL_WORKER CELERY --> RABBITMQ RABBITMQ[(RabbitMQ task queue)] click RABBITMQ \"../admin/queue/\" RABBITMQ --> CELERY ETL_WORKER[Open Semantic ETL worker] click ETL_WORKER \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/tasks.py\" ETL_WORKER -->|Running configured plugins one by one| TIKA subgraph TIKA [Apache Tika for text extraction and metadata extraction] direction LR TIKA_PLUGIN[ETL plugin calling Tika] click TIKA_PLUGIN \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/enhance_extract_text_tika_server.py\" TIKA_PLUGIN -->|Document file| TIKA_SERVER TIKA_SERVER[Apache Tika Server] click TIKA_SERVER \"https://github.com/opensemanticsearch/tika-server.deb\" TIKA_SERVER -->|Image files or images in PDF|OCR TIKA_SERVER -->|Extracted text| TIKA_PLUGIN OCR[Tesseract OCR] click OCR \"https://github.com/opensemanticsearch/tesseract-ocr-cache\" OCR-->|Recognized plain text| TIKA_SERVER end TIKA -->|Extracted text and metadata| EntitySearchAPI subgraph EntitySearchAPI [Named Entity Extraction by lists of names, thesaurus and ontologies] direction LR EL_PLUGIN[ETL plugin for entity extraction] click EL_PLUGIN \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/enhance_entity_linking.py\" EL_PLUGIN -->|Plain text| EL EL[Open Semantic Entity Search API] click EL \"https://github.com/opensemanticsearch/open-semantic-entity-search-api\" EL -->|Extracted entities| EL_PLUGIN THESAURUS[(Thesaurus)] click THESAURUS \"https://github.com/opensemanticsearch/open-semantic-search-apps/blob/master/src/thesaurus/models.py\" THESAURUS -->|SKOS| EL ONTOLOGIES[(Ontologies)] click ONTOLOGIES \"https://github.com/opensemanticsearch/open-semantic-search-apps/blob/master/src/ontologies/models.py\" ONTOLOGIES -->|RDF| EL end EntitySearchAPI -->|Added extracted named entities by lists of names, thesaurus and ontologies| NER NER[ETL plugin for spaCy Named Entity Recognition by Machine Learning] click NER \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/enhance_ner_spacy.py\" NER -->|Added recognized named entities| ANNOTATIONS subgraph ANNOTATIONS [Get tags and annotations for this documents made by humans] direction RL ANNOTATIONS_DB[(DB with tags and annotations)] click ANNOTATIONS_DB \"https://github.com/opensemanticsearch/open-semantic-search-apps/blob/master/src/annotate/models.py\" ANNOTATIONS_DB --> ANNOTATIONS_PLUGIN ANNOTATIONS_PLUGIN[ETL enrichment plugin getting tags and annotations] click ANNOTATIONS_PLUGIN \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/enhance_annotations.py\" end ANNOTATIONS -->|Added tags and annotations| ANALYSIS_PLUGIN ANALYSIS_PLUGIN[ETL data analysis plugin like extraction amounts of money] ANALYSIS_PLUGIN -->|Added extracted amounts of money| OTHER_PLUGINS OTHER_PLUGINS[Other configured ETL Plugins] OTHER_PLUGINS -->|Plain text and strucured data| EXPORTER EXPORTER[Exporter plugins] EXPORTER -->|Index data for full text search and faceting| SOLR EXPORTER -->|Index data for full text search and faceting| ELASTICSEARCH EXPORTER -->|Index linked data for graph search| NEO4J SOLR[(Apache Solr document index)] click SOLR \"../../solr\" SOLR -->|Search results| UI UI[Web user interface for search] UI -->|Solr search query| SOLR ELASTICSEARCH[(Alternate Elastic Search)] click ELASTICSEARCH \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/export_elasticsearch.py\" NEO4J[(Neo4J Graph Database)] click NEO4J \"https://github.com/opensemanticsearch/open-semantic-etl/blob/master/src/opensemanticetl/export_neo4j.py\"","title":"Flowchart of document processing and data flow"},{"location":"doc/modules/#components-and-modules","text":"User Interface : Client and user interface Search query forms : Search query form for full text search Explorer and navigator : Search with full text search and navigate (exploratory search) the index or search results with interactive filters (facets) Viewers : Parts of the UI to show different views (i.e. analytics like wordlcouds or trend charts) and previews for special formats (i.e. photos, documents, email ...) Code: /solr-php-ui/templates/ Annotators : Web Apps for tagging documents or CMS with forms and fields to manage meta data like tags or annotations Search Apps : Applications and user interfaces for search like search with lists tool or named entities manager Index and search server (Solr or Elastic Search) : Search server managing the index (indexer) and running search queries (query handler) Datamodel/Schema: src/solr.deb/var/solr/data/opensemanticsearch/conf/managed-schema Storage: /var/solr/data Log: /var/solr/logs/ Open Semantic ETL : Framework for data integration, data analysis, data enrichment and ETL (Extract, transform, load) pipelines or chains Connectors, importers, ingestors or crawlers : Import data from a data source (i.e. file system, file directory, file share, website or newsfeed) Parsers: Apache Tika to extract text and metadata from different file formats and document formats Entity extraction and entity linking : Open Semantic Entity Search API Data enrichment plugins and enhancer : Enhancing content with additional data like meta data (i.e. tagging or annotations) or analytics (i.e. OCR) ETL Exporter or Loader for Solr or Elastic Search : Indexing the data to search index Trigger : Your CMS or your file system (file system monitoring) will notify the web service (API) when there is new data or when content changed, so you dont have to burn resources for recrawl often to be able to find new or changed content very soon Web services (REST-API) : Available via standard network protocol HTTP and waiting until you (i.e. using the web admin interface) or another service (i.e. using the REST-API) demands actions like crawling a directory or a webpage and starting this actions Queue manager (Celery on RabbitMQ) : Managing task queue and starting of text extraction, analysis, data enrichment and indexing jobs by the right balance of parallel workers Scheduler : Managing starting of scheduled indexing jobs. This can be crontab for Cron starting the command line tools. Config: /etc/cron.d/open-semantic-search","title":"Components and Modules"},{"location":"doc/modules/#document-processing-extract-transform-load-etl-and-enhancing-by-data-enrichment-and-data-analysis","text":"How (new) data is handled by this components and ETL (extract, transform, load), document processing, data analysis and data enrichment : A user manually or a Cron daemon automatically from time to time starts a command The command line tools or the web API getting this command starts a ETL (extract, transform, load), data analysis and data enrichment chain to import, analyze and index data A input plugin or connector (i.e. the connector for the file system or the connector for a website) reads from its datasource The connectors, an Apache Tika parser, or a file format based data converter or extractor extracts data from the given document or file format The ETL framework calls all configured enhancer plugins for data enrichment to get additional analysis for the data or annotations to this data from a CMS. The output storage plugin or indexer index the text and metadata to the Solr index or to the Elastic Search index , so all other tools can search this data The user uses a user interface like the search user interface, the search apps or some other tools to search based on the search API of this index","title":"Document processing, extract, transform, load (ETL) and enhancing by data enrichment and data analysis"},{"location":"doc/modules/#services-and-microservices","text":"Linux services: tika - Text extraction and OCR tika-fake-ocr - Text extraction without OCR solr - Search index spacy-services - spaCy NLP opensemanticetl - ETL workers rabbitmq-server - Task queue flower - Task queue monitoring user interface apache2 - Search UI - Search apps (f.e. thesaurus app or config UI) - Entity Search API","title":"Services and Microservices"},{"location":"doc/modules/#user-interface-and-search-applications","text":"","title":"User Interface and search applications"},{"location":"doc/modules/#solr-php-ui","text":"User Interface (supports responsive design for mobiles and tablets) for search, facetted search, preview, different views and visualizations. Based on Solr client solr-php-client (pure vanilla php) and standard User Interfaces (HTML5 and CSS with Zurb Foundation ) and visualization libraries ( D3js ) so you can install and run it on standard PHP webspace without effort and without often not available special PHP-modules) Learn more ... Documentation: Howto seach Deployment /usr/share/solr-php-ui Log: /var/log/apache2/ Sourcecode: src/solr-php-ui (Github...)","title":"Solr-PHP-UI"},{"location":"doc/modules/#index-server","text":"","title":"Index server"},{"location":"doc/modules/#solr-search-server","text":"Preconfigured Solr Server running as daemon (so you have only to install the package and no further configuration needed) Learn more ...","title":"Solr search server"},{"location":"doc/modules/#annotation","text":"Tools for editing and managing metadata like tags, notes, relations and content structure (i.e. taxonomies):","title":"Annotation"},{"location":"doc/modules/#open-semantic-tagger","text":"Tagger is a light weight responsive web app for tagging web pages and documents. Learn more ...","title":"Open Semantic Tagger"},{"location":"doc/modules/#connectors","text":"Crawler, connectors, data importer and converter:","title":"Connectors"},{"location":"doc/modules/#connector-files-with-ocr","text":"Crawl and index directories, files and documents into Solr. Including automatic textrecognition (OCR) support for images and grafical formats included in PDF documents (i.e. scans) Learn more ...","title":"Connector Files (with OCR)"},{"location":"doc/modules/#connector-rss-rss-feed","text":"Indexes Webpages from a RSS-Newsfeed Learn more ...","title":"Connector RSS (RSS-Feed)"},{"location":"doc/modules/#connector-web-http","text":"Crawl and index Websites into Solr index. Learn more ...","title":"Connector Web (HTTP)"},{"location":"doc/modules/#connector-db-sql-mysql-postgresql","text":"Index SQL databases like MySQL or PostgreSQL into Solr. Learn more ...","title":"Connector DB (SQL, MySQL, Postgresql)"},{"location":"doc/modules/#connector-scraper-scraping-with-scrapy","text":"ETL and webscraping framework to crawl, extract, transform and load structured data from websites (scraping). Learn more ...","title":"Connector Scraper (Scraping with Scrapy)"},{"location":"doc/modules/#scheduler","text":"If you use our connectors and want most flexibility use Cron and write a cronjob using our command line tools within a crontab or call our REST-API within another webservice (i.e. webcron).","title":"Scheduler"},{"location":"doc/modules/#queue-manager","text":"Reads and manages trigger signals for starting indexing queued files by batch mode (parallel processing but because of limited RAM resources with a maximum count of workers/processes at same time) with opensemanticsearch-etl-file. Filenames can be appended to the queue by the REST API, Webinterface or command line tool. Learn more ...","title":"Queue manager"},{"location":"doc/modules/#data-enrichment-enhancer","text":"Will enhance the indexed content with meta data or analytics Learn more ...","title":"Data enrichment (Enhancer)"},{"location":"doc/modules/#enhancer-ocr","text":"Automatic textrecognition (OCR) for image files and images and graphics inside PDF (i.e. scans). Learn more ...","title":"Enhancer OCR"},{"location":"doc/modules/#enhancer-rdf","text":"Will enhance content with metadata in Resource Description Framework (RDF) format stored on a meta data server (i.e. tags and annotations in a Semantic Mediawiki or in Drupal CMS ) Learn more ...","title":"Enhancer RDF"},{"location":"doc/modules/#enhancer-xmp-sidecar-files","text":"Metadata like tags or descriptions for photos are often saved in XMP (Extensible Metadata Plattform) sidecar files (i.e. by Adobe Photoshop Lightroom . This enhancer adds the metadata of this sidecar files to the index of the original document. Learn more ...","title":"Enhancer XMP sidecar files"},{"location":"doc/modules/#enhancer-zip","text":"This enhancer recognizes and unzips zip archives to index documents and files inside a zip files, too. Learn more ...","title":"Enhancer ZIP"},{"location":"doc/modules/#web-services","text":"","title":"Web Services"},{"location":"doc/modules/#web-admin-interface","text":"Admin interface to start actions like crawling a directory or a webpage via web interface without command line tools and starting this actions. Learn more ...","title":"Web admin interface"},{"location":"doc/modules/#rest-api","text":"Application programming interface (API) available via generic and standard network protocol HTTP and waiting until another (web) service or software demands for an action like crawling a directory or a webpage or indexing changed data (i.e. directly started after data change by a trigger of the cms) and starting this actions. Learn more ...","title":"REST-API"},{"location":"doc/modules/#trigger","text":"Using triggers you don't need to recrawl often to be able to find new or changed content within seconds: If there are hundreds of Gigabytes or some Terabytes of data and millions of files, standard recrawls can take hours in which your document can not be found and eat many resources. With triggers that works the other way: your CMS or file server will send a signal if there is new content or a litte part has changed and the queue manager will index only this file or page very soon.","title":"Trigger"},{"location":"doc/modules/#trigger-filemonitoring","text":"File system monitoring based on itnotify . Monitors files and file folders and index them (again), so that new or changed documents or files can be found within seconds and without recrawl often (which would burn many ressources). Learn more ...","title":"Trigger Filemonitoring"},{"location":"doc/modules/#trigger-drupal","text":"After saving a page the Drupal module notifies the search engine about changed or new content. Learn more ...","title":"Trigger Drupal"},{"location":"doc/modules/#generic-triggers","text":"Like for Drupal (see before) there are generic trigger modules available for many other software projects, too. So install them and configure them to the URL of our REST-API to recrawl changed data of the other software or webservices.","title":"Generic triggers"},{"location":"doc/search/","text":"How to search, explore, analyze, structure, filter and visualize large document collections or many search results Semantic search, exploratory search, interactive filters, data visualization, information retrieval, document discovery & text mining How to combine semantic search, exploratory search, interactive filters & data visualizations for searching, information retrieval, document discovery, content mining & text mining to analyze, structure, filter and visualize search results, a large document collection or data: Index documents or data To enable a fast search and data analysis, you have to extract and to index the data. If not done automatically from time to time (f.e. by a cron job), start a crawl of the directory with the command line tool or with the web user interface or if you use the desktop search version you will find the option \"Index for search\" in the context menu for files and directories within the file manager. Or copy the documents to a directory that is monitored or work in such a directory or fileshare, so new and changed files and documents will be indexed immediately and automatically. After the text was extracted from the different formats or recognized by OCR and the data was enriched with different analyzers or qualitative data or structure, you can search, explore and analyse the texts and structures: Search operators You can use powerful search operators for your search query. For example you can use wildcards like *, boolean algebra like AND, OR and NOT or fuzzy search. Overview and exploration with facets With the faceted search (the right navigation sidebar for your search results), you can see an aggregated overview for the different facets like paths, concepts, persons, locations or organizations showing, how many documents matching this entity. Interactive filters You can use this overviews or named entities as interactive filter to narrow down search results. So a click to a facet (i.e. an organization) will drill down the search results to fewer documents, matching this additional facet/filter, too. Structure by named entities like organizations, concepts or tags After importing or editing some thesaurus entries or named entities like persons, locations, organizations or concepts , you can explore your documents or search results by this named entities, which will be available as such aggregated overview and interactive filters. If such named entities can not be extracted automatically by structured metadata of the thesuarus, your Named Entities database or named entity recognition because not matching their queries or machine learning models for automatic extraction of persons, organizations or places, you can tag and annotate documents manually: Just click \"Tagging and annotation\" for this document in the search results to annotate this document. Add tags (f.e. concepts, persons, organizations or locations) or write some free text into the notes field. Table view The sortable table view shows all columns/fields of the result list, for example URL, title, author and results of configured analyzers for data enrichment (f.e. extraction of email-addresses). Preview The preview shows the full texts of the documents. Your search context like the search query or active filters are marked inside the text, so you can see fast what is interesting for you and why the document has been found to your search context. Analysis and data visualization The tab/button Analyze provides different views and user interfaces for analysis and data visualization for the search results or for the whole document collection: Visualize and analyze dates with trend chars Visualize and analyze dates with trend charts : Text mining and content analysis Visualize and analyze contents with a word cloud and a words list for text mining : Visualize and analyze networks and connections Visualize and analyze networks, connections and relations as visual graph : Interoperable open standards for data import and data export Since this free software is open source and uses open standards you can integrate other tools and data or you can export the enriched data and use other additional tools for example for further and advanced data analysis tools, text analysis, text mining and document mining tools or data visualization tools.","title":"Search"},{"location":"doc/search/#how-to-search-explore-analyze-structure-filter-and-visualize-large-document-collections-or-many-search-results","text":"","title":"How to search, explore, analyze, structure, filter and visualize large document collections or many search results"},{"location":"doc/search/#semantic-search-exploratory-search-interactive-filters-data-visualization-information-retrieval-document-discovery-text-mining","text":"How to combine semantic search, exploratory search, interactive filters & data visualizations for searching, information retrieval, document discovery, content mining & text mining to analyze, structure, filter and visualize search results, a large document collection or data:","title":"Semantic search, exploratory search, interactive filters, data visualization, information retrieval, document discovery &amp; text mining"},{"location":"doc/search/#index-documents-or-data","text":"To enable a fast search and data analysis, you have to extract and to index the data. If not done automatically from time to time (f.e. by a cron job), start a crawl of the directory with the command line tool or with the web user interface or if you use the desktop search version you will find the option \"Index for search\" in the context menu for files and directories within the file manager. Or copy the documents to a directory that is monitored or work in such a directory or fileshare, so new and changed files and documents will be indexed immediately and automatically. After the text was extracted from the different formats or recognized by OCR and the data was enriched with different analyzers or qualitative data or structure, you can search, explore and analyse the texts and structures:","title":"Index documents or data"},{"location":"doc/search/#search-operators","text":"You can use powerful search operators for your search query. For example you can use wildcards like *, boolean algebra like AND, OR and NOT or fuzzy search.","title":"Search operators"},{"location":"doc/search/#overview-and-exploration-with-facets","text":"With the faceted search (the right navigation sidebar for your search results), you can see an aggregated overview for the different facets like paths, concepts, persons, locations or organizations showing, how many documents matching this entity.","title":"Overview and exploration with facets"},{"location":"doc/search/#interactive-filters","text":"You can use this overviews or named entities as interactive filter to narrow down search results. So a click to a facet (i.e. an organization) will drill down the search results to fewer documents, matching this additional facet/filter, too.","title":"Interactive filters"},{"location":"doc/search/#structure-by-named-entities-like-organizations-concepts-or-tags","text":"After importing or editing some thesaurus entries or named entities like persons, locations, organizations or concepts , you can explore your documents or search results by this named entities, which will be available as such aggregated overview and interactive filters. If such named entities can not be extracted automatically by structured metadata of the thesuarus, your Named Entities database or named entity recognition because not matching their queries or machine learning models for automatic extraction of persons, organizations or places, you can tag and annotate documents manually: Just click \"Tagging and annotation\" for this document in the search results to annotate this document. Add tags (f.e. concepts, persons, organizations or locations) or write some free text into the notes field.","title":"Structure by named entities like organizations, concepts or tags"},{"location":"doc/search/#table-view","text":"The sortable table view shows all columns/fields of the result list, for example URL, title, author and results of configured analyzers for data enrichment (f.e. extraction of email-addresses).","title":"Table view"},{"location":"doc/search/#preview","text":"The preview shows the full texts of the documents. Your search context like the search query or active filters are marked inside the text, so you can see fast what is interesting for you and why the document has been found to your search context.","title":"Preview"},{"location":"doc/search/#analysis-and-data-visualization","text":"The tab/button Analyze provides different views and user interfaces for analysis and data visualization for the search results or for the whole document collection:","title":"Analysis and data visualization"},{"location":"doc/search/#visualize-and-analyze-dates-with-trend-chars","text":"Visualize and analyze dates with trend charts :","title":"Visualize and analyze dates with trend chars"},{"location":"doc/search/#text-mining-and-content-analysis","text":"Visualize and analyze contents with a word cloud and a words list for text mining :","title":"Text mining and content analysis"},{"location":"doc/search/#visualize-and-analyze-networks-and-connections","text":"Visualize and analyze networks, connections and relations as visual graph :","title":"Visualize and analyze networks and connections"},{"location":"doc/search/#interoperable-open-standards-for-data-import-and-data-export","text":"Since this free software is open source and uses open standards you can integrate other tools and data or you can export the enriched data and use other additional tools for example for further and advanced data analysis tools, text analysis, text mining and document mining tools or data visualization tools.","title":"Interoperable open standards for data import and data export"},{"location":"doc/search/csv/","text":"Managing (very big) CSV spreadsheets and tables Import, browse, search and filter structured data from CSV files If you want search, navigate, browse and filter a CSV spreadsheet even if it is too big for excel or Open Office Calc: Just copy the CSV file to a directory with file monitoring or index it the standard way by indexing a file from your filesystem or URI from the web . So you are able to search for the content of the CSV file. If the CSV file is in standard format and the CSV enhancer is set to on, you can even browse and filter rows and columns using the table view . But sometimes you might want to use such additional functionality even if the data is not stored in standard CSV format. With the webapp CSV manager you can set additional metadata (i.e. parameters about the CSV format) with the comfortable user interface: If you see, that some chars are not ok, so you have to set the charset manually from default (UTF-8) to another charset If the CSV dialect autodetection doesn't work for a special format, you can easily configure custom delimiter and quote chars If you want to map columns to existing fields or facets If you want to extract column titles from the csv instead of default (column 1, column 2, ...) Usage: The CSV user interface Click the button \"Add new csv\" in the list view. Upload or reference a CSV file Just upload a csv file to CSV manager or set a reference (URL) to the URI field. Set CSV format If not standard CSV format, set autodetection or overwrite standard settings like delimiter or quote char with custom values: Import CSV data to search engine After saving your CSV settings click the button \"Import\" to import with this settings the data of the CSV file to the search engine. Check imported data After importing check by table view if the structure and charset settings were right.","title":"Managing (very big) CSV spreadsheets and tables"},{"location":"doc/search/csv/#managing-very-big-csv-spreadsheets-and-tables","text":"","title":"Managing (very big) CSV spreadsheets and tables"},{"location":"doc/search/csv/#import-browse-search-and-filter-structured-data-from-csv-files","text":"If you want search, navigate, browse and filter a CSV spreadsheet even if it is too big for excel or Open Office Calc: Just copy the CSV file to a directory with file monitoring or index it the standard way by indexing a file from your filesystem or URI from the web . So you are able to search for the content of the CSV file. If the CSV file is in standard format and the CSV enhancer is set to on, you can even browse and filter rows and columns using the table view . But sometimes you might want to use such additional functionality even if the data is not stored in standard CSV format. With the webapp CSV manager you can set additional metadata (i.e. parameters about the CSV format) with the comfortable user interface: If you see, that some chars are not ok, so you have to set the charset manually from default (UTF-8) to another charset If the CSV dialect autodetection doesn't work for a special format, you can easily configure custom delimiter and quote chars If you want to map columns to existing fields or facets If you want to extract column titles from the csv instead of default (column 1, column 2, ...)","title":"Import, browse, search and filter structured data from CSV files"},{"location":"doc/search/csv/#usage-the-csv-user-interface","text":"Click the button \"Add new csv\" in the list view.","title":"Usage: The CSV user interface"},{"location":"doc/search/csv/#upload-or-reference-a-csv-file","text":"Just upload a csv file to CSV manager or set a reference (URL) to the URI field.","title":"Upload or reference a CSV file"},{"location":"doc/search/csv/#set-csv-format","text":"If not standard CSV format, set autodetection or overwrite standard settings like delimiter or quote char with custom values:","title":"Set CSV format"},{"location":"doc/search/csv/#import-csv-data-to-search-engine","text":"After saving your CSV settings click the button \"Import\" to import with this settings the data of the CSV file to the search engine.","title":"Import CSV data to search engine"},{"location":"doc/search/csv/#check-imported-data","text":"After importing check by table view if the structure and charset settings were right.","title":"Check imported data"},{"location":"doc/search/fuzzy/","text":"Fuzzy search How to find results despite incomplete or imperfect search queries, misspelled names, typos and low text quality or low OCR quality If you're sure that all search query terms are in the document(s) you search, you can search with the default operator AND. If you search for a exact sentence or phrase (all search terms in exact this form and order with no other words in between), you can switch to \"Phrase search\". But if you're not sure about all terms, you can use and combine different search techniques like the OR operator and scoring, wildcards, stemming, synonyms and fuzziness: Use OR operator and sort by scoring If you use / switch to the OR operator, you will often find results without all search terms in the documents (but often relevant, since one your searched term may occur in other form). But even getting many irrelevant results in which only a small part or even a single word of your search query occurs, if using this search paradigma: The more of your search terms occur in the document, the higher is the document is scored and you will see documents with most matches on top. So you will often find documents even if one of your search terms is misspelled or not exact the same concept used in the document. Find alternate word forms (Stemming) Considering grammar rules to find other word forms, too (stemming) , the search engine will find more. For example if you search for company corruption you will find companies corrupted , too. If you want only results matching exactly the spelling in your search query, you can disable stemming by disabling the option \"Find other word forms, too\". If you enable stemming for the whole search query, you can disable it for single words / terms by using the operator exact: But such automatic heuristics can fail, especially for example on irregular verbs, where for example another grammatical form like \"went\" is not only \"go\" with a suffix (like f.e. -ing). Human curated dictionary of grammar forms and synonyms (Linked Data Knowledge Graph of Lexemes as Open Data from WikiData) By considering lexemes by import of Wikidata Lexemes as Solr search engine synonyms you find documents including many such more complicated / irregular grammar forms, too. Hint: For manually find variants for your search, you can browse the structured data of lexemes and synonyms in the structured data base and linked open data knowledge graph WikiData by the web user interface Ordia . Import Wikidata lexemes to synonyms config The lexemes import tool can be configured by command line parameters and will be integrated to our web UI next. Find unknown word parts with wildcards Multiple character wildcard searches with * looks for 0 or more characters. So corrupt* will find corrupt, corrupt ion , corrupt ed ... but not anti corruption *rupt will find corrupt or interrupt but wont find corrupt ion or interrupt ion ... *rupt* will find corrupt, corruption, anticorruption, interrupt and interruption ... You can also use the wildcard searches in the middle of a term: co*upt will find corupt or corrupt Find results with typos (or if not sure how to spell a name) by edit distance (Levensthein distance) Fuzziness by edit distance is useful if you search for names you don't know how to spell exactly. Or if you want to consider typos or misspelled names in the documents. Or if some documents are images or scanned documents (like PDF documents containing scanned pages only in graphical formats instead of digital text), so you maybe find more documents for your query even if the results of automated text-recognition (OCR) are not perfect , i.e. because single chars were recognized wrong. To do a fuzzy search use the tilde, \"~\", symbol at the end of a single word / term you want to search with fuzziness. For example to search for a term similar in spelling to \" roam \" use the fuzzy search: roam~ This search will find terms like foam and roams . similar~ words~ would find not only similar words but similer works , too because both words are set to fuzzy search. Setting maximal edit distance (number of chars, that may differ) If you use the ~ operator for fuzziness, the default maximal edit distance is two chars. This is the maximal number of chars that can be changed, added or deleted to match a term that is searched with fuzziness. To change the maximum number of changeable chars, set the number after the ~ operator. Example: searchterm~3 Proximity search to find name variants with additional name parts in between A proximity search looks for terms or name parts that are within a specific distance from one another. So a proximity search for \"Barack Obama\"~2 will find name variants like Barack Hussein Obama , too, which would not be found by a phrase search for \"Barack Obama\". To perform a proximity search, add the tilde character ~ and a numeric value to the end of a search phrase. The numeric value sets how many words may maximal occur in between (the maximal distance of term movements needed to match the specified phrase). Find synonyms and hyponyms If you don't switch that off for your query by disabling search for synonyms in advanced search options and you don't use the operator \"exact:\" for the search term, the search engine can find synonyms (f.e. if you search for car you want to find automobile, too) and hyponyms (if you search for a car, you want to find truck, too). You can setup synonyms with the thesaurus manager . Just add a name or concept with the synonym as an alias or import a thesaurus or ontology with synonyms to the ontologies manager . Please donate so we can integrate (technically existing) features with open data from Wiktionary, so most synonyms and more forms of irregular verbs (\"went\" is not only \"go\" with a suffix) will be preconfigured out of the box. More like this (Term vector model) Based on a relevant document find other documents with many same words (\"more like this\"). Please donate... Find similarity by machine learning (Latent Dirichlet allocation (LDA), clustering, classification, co-ocurrences) Find documents with related words (not only the same words in other forms or synonyms). Please donate...","title":"Fuzzy search"},{"location":"doc/search/fuzzy/#fuzzy-search","text":"","title":"Fuzzy search"},{"location":"doc/search/fuzzy/#how-to-find-results-despite-incomplete-or-imperfect-search-queries-misspelled-names-typos-and-low-text-quality-or-low-ocr-quality","text":"If you're sure that all search query terms are in the document(s) you search, you can search with the default operator AND. If you search for a exact sentence or phrase (all search terms in exact this form and order with no other words in between), you can switch to \"Phrase search\". But if you're not sure about all terms, you can use and combine different search techniques like the OR operator and scoring, wildcards, stemming, synonyms and fuzziness:","title":"How to find results despite incomplete or imperfect search queries, misspelled names, typos and low text quality or low OCR quality"},{"location":"doc/search/fuzzy/#use-or-operator-and-sort-by-scoring","text":"If you use / switch to the OR operator, you will often find results without all search terms in the documents (but often relevant, since one your searched term may occur in other form). But even getting many irrelevant results in which only a small part or even a single word of your search query occurs, if using this search paradigma: The more of your search terms occur in the document, the higher is the document is scored and you will see documents with most matches on top. So you will often find documents even if one of your search terms is misspelled or not exact the same concept used in the document.","title":"Use OR operator and sort by scoring"},{"location":"doc/search/fuzzy/#find-alternate-word-forms-stemming","text":"Considering grammar rules to find other word forms, too (stemming) , the search engine will find more. For example if you search for company corruption you will find companies corrupted , too. If you want only results matching exactly the spelling in your search query, you can disable stemming by disabling the option \"Find other word forms, too\". If you enable stemming for the whole search query, you can disable it for single words / terms by using the operator exact: But such automatic heuristics can fail, especially for example on irregular verbs, where for example another grammatical form like \"went\" is not only \"go\" with a suffix (like f.e. -ing).","title":"Find alternate word forms (Stemming)"},{"location":"doc/search/fuzzy/#human-curated-dictionary-of-grammar-forms-and-synonyms-linked-data-knowledge-graph-of-lexemes-as-open-data-from-wikidata","text":"By considering lexemes by import of Wikidata Lexemes as Solr search engine synonyms you find documents including many such more complicated / irregular grammar forms, too. Hint: For manually find variants for your search, you can browse the structured data of lexemes and synonyms in the structured data base and linked open data knowledge graph WikiData by the web user interface Ordia .","title":"Human curated dictionary of grammar forms and synonyms (Linked Data Knowledge Graph of Lexemes as Open Data from WikiData)"},{"location":"doc/search/fuzzy/#import-wikidata-lexemes-to-synonyms-config","text":"The lexemes import tool can be configured by command line parameters and will be integrated to our web UI next.","title":"Import Wikidata lexemes to synonyms config"},{"location":"doc/search/fuzzy/#find-unknown-word-parts-with-wildcards","text":"Multiple character wildcard searches with * looks for 0 or more characters. So corrupt* will find corrupt, corrupt ion , corrupt ed ... but not anti corruption *rupt will find corrupt or interrupt but wont find corrupt ion or interrupt ion ... *rupt* will find corrupt, corruption, anticorruption, interrupt and interruption ... You can also use the wildcard searches in the middle of a term: co*upt will find corupt or corrupt","title":"Find unknown word parts with wildcards"},{"location":"doc/search/fuzzy/#find-results-with-typos-or-if-not-sure-how-to-spell-a-name-by-edit-distance-levensthein-distance","text":"Fuzziness by edit distance is useful if you search for names you don't know how to spell exactly. Or if you want to consider typos or misspelled names in the documents. Or if some documents are images or scanned documents (like PDF documents containing scanned pages only in graphical formats instead of digital text), so you maybe find more documents for your query even if the results of automated text-recognition (OCR) are not perfect , i.e. because single chars were recognized wrong. To do a fuzzy search use the tilde, \"~\", symbol at the end of a single word / term you want to search with fuzziness. For example to search for a term similar in spelling to \" roam \" use the fuzzy search: roam~ This search will find terms like foam and roams . similar~ words~ would find not only similar words but similer works , too because both words are set to fuzzy search.","title":"Find results with typos (or if not sure how to spell a name) by edit distance (Levensthein distance)"},{"location":"doc/search/fuzzy/#setting-maximal-edit-distance-number-of-chars-that-may-differ","text":"If you use the ~ operator for fuzziness, the default maximal edit distance is two chars. This is the maximal number of chars that can be changed, added or deleted to match a term that is searched with fuzziness. To change the maximum number of changeable chars, set the number after the ~ operator. Example: searchterm~3","title":"Setting maximal edit distance (number of chars, that may differ)"},{"location":"doc/search/fuzzy/#proximity-search-to-find-name-variants-with-additional-name-parts-in-between","text":"A proximity search looks for terms or name parts that are within a specific distance from one another. So a proximity search for \"Barack Obama\"~2 will find name variants like Barack Hussein Obama , too, which would not be found by a phrase search for \"Barack Obama\". To perform a proximity search, add the tilde character ~ and a numeric value to the end of a search phrase. The numeric value sets how many words may maximal occur in between (the maximal distance of term movements needed to match the specified phrase).","title":"Proximity search to find name variants with additional name parts in between"},{"location":"doc/search/fuzzy/#find-synonyms-and-hyponyms","text":"If you don't switch that off for your query by disabling search for synonyms in advanced search options and you don't use the operator \"exact:\" for the search term, the search engine can find synonyms (f.e. if you search for car you want to find automobile, too) and hyponyms (if you search for a car, you want to find truck, too). You can setup synonyms with the thesaurus manager . Just add a name or concept with the synonym as an alias or import a thesaurus or ontology with synonyms to the ontologies manager . Please donate so we can integrate (technically existing) features with open data from Wiktionary, so most synonyms and more forms of irregular verbs (\"went\" is not only \"go\" with a suffix) will be preconfigured out of the box.","title":"Find synonyms and hyponyms"},{"location":"doc/search/fuzzy/#more-like-this-term-vector-model","text":"Based on a relevant document find other documents with many same words (\"more like this\"). Please donate...","title":"More like this (Term vector model)"},{"location":"doc/search/fuzzy/#find-similarity-by-machine-learning-latent-dirichlet-allocation-lda-clustering-classification-co-ocurrences","text":"Find documents with related words (not only the same words in other forms or synonyms). Please donate...","title":"Find similarity by machine learning (Latent Dirichlet allocation (LDA), clustering, classification, co-ocurrences)"},{"location":"doc/search/fuzzy/synonames/","text":"Find documents with different first name variants (Synonames) To quote an article about how Synonames helps OCCRP investigate people across languages and alphabets \" A single name can have many equivalents when transliterated across writing systems or represented across cultures. A Russian named \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 might open a U.K. bank account as Aleksandr, while a German Friedrich might introduce himself to Americans as \u201cFred\u201d. These variations arise naturally with cross-cultural exchange, without any malicious intent, and the human brain can usually toggle between them effortlessly. But they pose a bigger problem in data-driven research. \" To find documents that contain only such alternate first name variants, the full text search engine of Open Semantic Search integrates the multilingual open data knowledge of name variants from Wikidata extracted by the Open Source tool Synonames as synonyms out of the box. So if you do not disable the search option \"Find other word forms, too\" and search for the German first name \"Markus\", you will find documents which contain only the English variant \"Mark\", too. Full names and aliases Synonames only \"translates\" single name parts (forename) for many names for all searches. The advantage is, that this general forename synonyms often works for yet unknown people which are not part of your database, thesaurus or ontology, too. But it does not know about full name aliases, fake names, former names and pseudonyms of concrete people or their aliases. If you want to be able to find full name aliases with different forenames and different last names, too, you can add aliases to your thesaurus or import some open data ontologies like for example lists of politicians of your parliament by the ontologies manager which suports multi word synonyms like full names or aliases, too. So you can find work from Kurt Tucholsky published with his alias \"Peter Panter\" or documents about Angela Merkel containing \"only\" the variant \"Angela Merkelova\" or her former name \"Angela Kasner\", too. Open Source code and Open Data The open source code to extract the open data knowledge on first name variants from Wikidata: Source code repository of Synonymes Our open source integration with the Solr Synonym Graph: Source code repository of Synonames to Solr","title":"Find documents with different first name variants (Synonames)"},{"location":"doc/search/fuzzy/synonames/#find-documents-with-different-first-name-variants-synonames","text":"To quote an article about how Synonames helps OCCRP investigate people across languages and alphabets \" A single name can have many equivalents when transliterated across writing systems or represented across cultures. A Russian named \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 might open a U.K. bank account as Aleksandr, while a German Friedrich might introduce himself to Americans as \u201cFred\u201d. These variations arise naturally with cross-cultural exchange, without any malicious intent, and the human brain can usually toggle between them effortlessly. But they pose a bigger problem in data-driven research. \" To find documents that contain only such alternate first name variants, the full text search engine of Open Semantic Search integrates the multilingual open data knowledge of name variants from Wikidata extracted by the Open Source tool Synonames as synonyms out of the box. So if you do not disable the search option \"Find other word forms, too\" and search for the German first name \"Markus\", you will find documents which contain only the English variant \"Mark\", too.","title":"Find documents with different first name variants (Synonames)"},{"location":"doc/search/fuzzy/synonames/#full-names-and-aliases","text":"Synonames only \"translates\" single name parts (forename) for many names for all searches. The advantage is, that this general forename synonyms often works for yet unknown people which are not part of your database, thesaurus or ontology, too. But it does not know about full name aliases, fake names, former names and pseudonyms of concrete people or their aliases. If you want to be able to find full name aliases with different forenames and different last names, too, you can add aliases to your thesaurus or import some open data ontologies like for example lists of politicians of your parliament by the ontologies manager which suports multi word synonyms like full names or aliases, too. So you can find work from Kurt Tucholsky published with his alias \"Peter Panter\" or documents about Angela Merkel containing \"only\" the variant \"Angela Merkelova\" or her former name \"Angela Kasner\", too.","title":"Full names and aliases"},{"location":"doc/search/fuzzy/synonames/#open-source-code-and-open-data","text":"The open source code to extract the open data knowledge on first name variants from Wikidata: Source code repository of Synonymes Our open source integration with the Solr Synonym Graph: Source code repository of Synonames to Solr","title":"Open Source code and Open Data"},{"location":"doc/search/graph/","text":"Searching Linked Data (Open Semantic Knowledge Graph Search) Knowledge graph exploration and graph navigation The graph/network analysis view shows you the direct and indirect relations, connections and networks between named entities like persons, organizations or main concepts which occur together (co-occurrences) in your content, datasources and documents or are connected in your Linked Data Knowledge Graph. Graph visualization and graph analysis by full-text search You can start a graph visualization from full text search (not only for entities, but for keywords in your documents, too) in the search user interface. Therefore click on the view \"Connections (Graph)\" in tab/menu analysis. There you can set which types (classes) of entities and connections (properties) to use for graph analysis and graph visualization and with how many of each entities to start with. By click on the button \"Visualize graph\" you open the graph explorer and can extend the graph by clicking an entity / node. Graph visualization of potential connections between entities within documents (co-occurrences of named entities like persons, organizations or concepts) You can view and analyze graphs and potential connections because of occuring in same documents (co-occuring named entities like persons, organizations, products, materials or concepts) with the visual analysis view graph : Semantic Search and Text Mining on Linked Data With the Resource Description Framework (RDF) plugin you can use the semantic search engine as enterprise search engine and text mining platform for full text search, thesaurus based semantic search, faceted search and text mining of strings and texts (f.e. labels or literals) in structured data like linked data (LD), your domain knowledge graph, enterprise knowledge graphs or linked open data (LOD) knowledge graphs: Index linked data from Resource Description Framework (RDF) for full text search After import and indexing of Resource Description Framework (RDF) graphs you can analyse, search and filter the RDF data (triples) granular by entities (RDF subjects): Granular filtering & analysis by seperated search result for each knowledge graph entity or subject For each knowledge graph entity (RDF subject) there is an separated and granular search result with the document content type \"knowledge graph\" including all its properties (RDF predicates) as facets/fields/columns and all values (RDF objects) as values in such facets/fields/columns. Full text search, semantic text analysis and interactive filters or faceted search on linked data knowledge graphs So you can find, filter and explore your linked data knowledge graph entries by full text search, overviews and granular interactive filters (faceted search on linked data) or by more advanced text mining and natural language processing techniques for text mining on linked data knowledge graphs.","title":"Searching Linked Data (Open Semantic Knowledge Graph Search)"},{"location":"doc/search/graph/#searching-linked-data-open-semantic-knowledge-graph-search","text":"","title":"Searching Linked Data (Open Semantic Knowledge Graph Search)"},{"location":"doc/search/graph/#knowledge-graph-exploration-and-graph-navigation","text":"The graph/network analysis view shows you the direct and indirect relations, connections and networks between named entities like persons, organizations or main concepts which occur together (co-occurrences) in your content, datasources and documents or are connected in your Linked Data Knowledge Graph.","title":"Knowledge graph exploration and graph navigation"},{"location":"doc/search/graph/#graph-visualization-and-graph-analysis-by-full-text-search","text":"You can start a graph visualization from full text search (not only for entities, but for keywords in your documents, too) in the search user interface. Therefore click on the view \"Connections (Graph)\" in tab/menu analysis. There you can set which types (classes) of entities and connections (properties) to use for graph analysis and graph visualization and with how many of each entities to start with. By click on the button \"Visualize graph\" you open the graph explorer and can extend the graph by clicking an entity / node.","title":"Graph visualization and graph analysis by full-text search"},{"location":"doc/search/graph/#graph-visualization-of-potential-connections-between-entities-within-documents-co-occurrences-of-named-entities-like-persons-organizations-or-concepts","text":"You can view and analyze graphs and potential connections because of occuring in same documents (co-occuring named entities like persons, organizations, products, materials or concepts) with the visual analysis view graph :","title":"Graph visualization of potential connections between entities within documents (co-occurrences of named entities like persons, organizations or concepts)"},{"location":"doc/search/graph/#semantic-search-and-text-mining-on-linked-data","text":"With the Resource Description Framework (RDF) plugin you can use the semantic search engine as enterprise search engine and text mining platform for full text search, thesaurus based semantic search, faceted search and text mining of strings and texts (f.e. labels or literals) in structured data like linked data (LD), your domain knowledge graph, enterprise knowledge graphs or linked open data (LOD) knowledge graphs:","title":"Semantic Search and Text Mining on Linked Data"},{"location":"doc/search/graph/#index-linked-data-from-resource-description-framework-rdf-for-full-text-search","text":"After import and indexing of Resource Description Framework (RDF) graphs you can analyse, search and filter the RDF data (triples) granular by entities (RDF subjects):","title":"Index linked data from Resource Description Framework (RDF) for full text search"},{"location":"doc/search/graph/#granular-filtering-analysis-by-seperated-search-result-for-each-knowledge-graph-entity-or-subject","text":"For each knowledge graph entity (RDF subject) there is an separated and granular search result with the document content type \"knowledge graph\" including all its properties (RDF predicates) as facets/fields/columns and all values (RDF objects) as values in such facets/fields/columns.","title":"Granular filtering &amp; analysis by seperated search result for each knowledge graph entity or subject"},{"location":"doc/search/graph/#full-text-search-semantic-text-analysis-and-interactive-filters-or-faceted-search-on-linked-data-knowledge-graphs","text":"So you can find, filter and explore your linked data knowledge graph entries by full text search, overviews and granular interactive filters (faceted search on linked data) or by more advanced text mining and natural language processing techniques for text mining on linked data knowledge graphs.","title":"Full text search, semantic text analysis and interactive filters or faceted search on linked data knowledge graphs"},{"location":"doc/search/list/","text":"Fuzzy search by lists of names If you have not only some search queries but a whole list (i.e. a list of company names, products or organizations) and you want to search for every entry of this list, if there are results in your data, you can import this list of names, dictionary or thesaurus with the ontologies manager and get an overview in how many documents the entries occur . Such an aggregated overview of named entities like names, organizations, persons, location or main concepts is very usefull since it is available automatically for all documents and all searches. But this works only if the names in the list are written exact the same like the names in the documents. If you want to do a fuzzy search with a list to find the names even if not written correctly or not complete, you can use the fuzzy listsearch feature: Web interface (App) for fuzzy list search Navigate to listsearch (menu point in your top bar, if your administrator allowed and configured that). Just paste a list with one entry per line to the \"Query list\" textarea and start the search by pressing the \"Search\" button. This list can be (parts of) the content of a text file with one name per line or a single column (with many rows) copied and pasted from a spreadsheet. Fuzzy search Searching with fuzzy search will take more time, but will find similar results, too - which often are exactly what you search for but it can not be found by exact matching i.e. because of typos or if a part of a company name like the suffix Ltd. is missing in some documents but is part of your search query. Or you want to find documents even if some chars were recognized wrong while automatic textrecognition (OCR) of scans). So there will be a result list for each type of the enabled searches and fuzzy search methods. Each additional search type or fuzzy search method query will exclude all results from (better matching but not finding all) enabled search types before, so you dont have to check the same results multiple times trying to find the additional found results by this additional fuzzy search method. Search results Such a result list contains entries, that have been found in your indexed documents or data including the name of the entry, the count of results with a link to this results: Sorting by score The result lists contain a column with the maximum score of the documents that matched that entry showing how near the best matching document is to your list entry or query. Since the score column is sortable, you can check the most promising matches (most parts of the list entry found) until the results are too far away from your query (i.e. too few words of the query found) and do the same for the results of each different fuzzy search method. So you can even use very very fuzzy search methods like \"match if some similar words\"-search using the search operator OR and fuzzy search, where often the most scored documents are what you search and you would not find them by the other methods (i.e. if a part of a company name like the suffix Ltd. is missing in some documents but is part of your search query) but most parts of the very big result list is very far away from the query and was found only because containing (a) very often used word(s) which are part of the query. Filter query If you want to search for the entries of the list in a search context like special paths, URIs or only in documents containing some other words or matching a more or less complex search query, you can define additional search terms or filters in the field \"filter query\".","title":"Search by list of names (batch searches)"},{"location":"doc/search/list/#fuzzy-search-by-lists-of-names","text":"If you have not only some search queries but a whole list (i.e. a list of company names, products or organizations) and you want to search for every entry of this list, if there are results in your data, you can import this list of names, dictionary or thesaurus with the ontologies manager and get an overview in how many documents the entries occur . Such an aggregated overview of named entities like names, organizations, persons, location or main concepts is very usefull since it is available automatically for all documents and all searches. But this works only if the names in the list are written exact the same like the names in the documents. If you want to do a fuzzy search with a list to find the names even if not written correctly or not complete, you can use the fuzzy listsearch feature:","title":"Fuzzy search by lists of names"},{"location":"doc/search/list/#web-interface-app-for-fuzzy-list-search","text":"Navigate to listsearch (menu point in your top bar, if your administrator allowed and configured that). Just paste a list with one entry per line to the \"Query list\" textarea and start the search by pressing the \"Search\" button. This list can be (parts of) the content of a text file with one name per line or a single column (with many rows) copied and pasted from a spreadsheet.","title":"Web interface (App) for fuzzy list search"},{"location":"doc/search/list/#fuzzy-search","text":"Searching with fuzzy search will take more time, but will find similar results, too - which often are exactly what you search for but it can not be found by exact matching i.e. because of typos or if a part of a company name like the suffix Ltd. is missing in some documents but is part of your search query. Or you want to find documents even if some chars were recognized wrong while automatic textrecognition (OCR) of scans). So there will be a result list for each type of the enabled searches and fuzzy search methods. Each additional search type or fuzzy search method query will exclude all results from (better matching but not finding all) enabled search types before, so you dont have to check the same results multiple times trying to find the additional found results by this additional fuzzy search method.","title":"Fuzzy search"},{"location":"doc/search/list/#search-results","text":"Such a result list contains entries, that have been found in your indexed documents or data including the name of the entry, the count of results with a link to this results:","title":"Search results"},{"location":"doc/search/list/#sorting-by-score","text":"The result lists contain a column with the maximum score of the documents that matched that entry showing how near the best matching document is to your list entry or query. Since the score column is sortable, you can check the most promising matches (most parts of the list entry found) until the results are too far away from your query (i.e. too few words of the query found) and do the same for the results of each different fuzzy search method. So you can even use very very fuzzy search methods like \"match if some similar words\"-search using the search operator OR and fuzzy search, where often the most scored documents are what you search and you would not find them by the other methods (i.e. if a part of a company name like the suffix Ltd. is missing in some documents but is part of your search query) but most parts of the very big result list is very far away from the query and was found only because containing (a) very often used word(s) which are part of the query.","title":"Sorting by score"},{"location":"doc/search/list/#filter-query","text":"If you want to search for the entries of the list in a search context like special paths, URIs or only in documents containing some other words or matching a more or less complex search query, you can define additional search terms or filters in the field \"filter query\".","title":"Filter query"},{"location":"doc/search/operators/","text":"Search operators and wildcards You can use and combine different search operators and wildcards in your search queries : Wildcard searches To perform a single character wildcard search use the \"?\" symbol. To perform a multiple character wildcard search use the \"*\" symbol. Unknown or all parts with * Multiple character wildcard searches with * looks for zero, one or more characters. So corrupt* will find corrupt, corrupt ion , corrupt ed ... but not anticorruption *rupt will find corrupt or interrupt but wont find corrupt ion or interrupt ion ... *rupt* will find corrupt, corruption, anticorruption, interrupt and interruption ... You can also use the wildcard searches in the middle of a term: co*upt will find corupt or corrupt Single character wildcard The single character wildcard search looks for terms that match that with the single character replaced. For example, to search for \"text\" or \"test\" you can use the search: te?t Boolean operators Boolean operators allow terms to be combined through logic operators. Lucene supports AND, \"+\", OR, NOT and \"-\" as Boolean operators(Note: Boolean operators must be ALL CAPS). AND search engine will find all documents which contains the word engine AND the word search search AND engine would return the same results OR search OR research will find all documents which contains the word search or the word research Exclusion with - or NOT The \"-\" or prohibit operator excludes documents that contain the term after the \"-\" symbol: search -engine or search NOT engine or search AND NOT engine will find all documents which contain the word search and do not contain the word engine Phrase searches Phrase search with double quotes A Phrase is a group of words surrounded by double quotes such as \"search engine\" which will find exactly this phrase and will not find i search an engine Fuzzy searches Find results with typos or names not exactly pronounced by edit distance (Levensthein distance) Fuzziness by edit distance is useful if you search for names you don't know how to spell exactly or if you want to consider typos in the documents or if some documents are images or scanned documents (like PDF documents containing scanned pages only in graphical formats instead of digital text), so you maybe find more documents for your query even if the results of automated text-recognition (OCR) are not perfect). To do a fuzzy search use the tilde, \"~\", symbol at the end of a single word / term you want to search with fuzziness. For example to search for a term similar in spelling to \" roam \" use the fuzzy search: roam~ This search will find terms like foam and roams . similar~ words~ would find not only similar words but similer works , too because both words are set to fuzzy search. Setting maximal edit distance (number of chars, that may differ) If you use the ~ operator for fuzziness, the default maximal edit distance is two chars. This is the maximal number of chars that can be changed, added or deleted to match a term that is searched with fuzziness. To change the maximum number of changeable chars, set the number after the ~ operator. Example: searchterm~3 Grammar (Stemming) Considering grammar rules to find other word forms, too (stemming) , the search engine will find more. For example if you search for company corruption you will find companies corrupted , too. If you want only results matching exactly the spelling in your search query, you can disable stemming by disabling the option \"Find other word forms, too\". If you disable stemming for the whole search query, you can enable it for single terms by using the operator stemmed: If you enable stemming for the whole search query, you can disable stemming for single terms by using the operator exact: Grouping Combine criterias with OR, AND and parentheses You can use parentheses to group clauses to form sub queries: (search OR research) AND (engine OR software) will find: * documents containing the words search and engine * documents containing the words search and software * documents containing the words research and engine * documents containing the words research and software * documents containing the words search and research and software and engine But it wont find a document containing the words search and research without at least one of the words engine or software . Only one of the two last words would be enough, because connected with OR, but one of them is absolutelly neccecary because this criteria combination (connected by parantheses) is connected with AND","title":"Search operators"},{"location":"doc/search/operators/#search-operators-and-wildcards","text":"You can use and combine different search operators and wildcards in your search queries :","title":"Search operators and wildcards"},{"location":"doc/search/operators/#wildcard-searches","text":"To perform a single character wildcard search use the \"?\" symbol. To perform a multiple character wildcard search use the \"*\" symbol.","title":"Wildcard searches"},{"location":"doc/search/operators/#unknown-or-all-parts-with","text":"Multiple character wildcard searches with * looks for zero, one or more characters. So corrupt* will find corrupt, corrupt ion , corrupt ed ... but not anticorruption *rupt will find corrupt or interrupt but wont find corrupt ion or interrupt ion ... *rupt* will find corrupt, corruption, anticorruption, interrupt and interruption ... You can also use the wildcard searches in the middle of a term: co*upt will find corupt or corrupt","title":"Unknown or all parts with *"},{"location":"doc/search/operators/#single-character-wildcard","text":"The single character wildcard search looks for terms that match that with the single character replaced. For example, to search for \"text\" or \"test\" you can use the search: te?t","title":"Single character wildcard"},{"location":"doc/search/operators/#boolean-operators","text":"Boolean operators allow terms to be combined through logic operators. Lucene supports AND, \"+\", OR, NOT and \"-\" as Boolean operators(Note: Boolean operators must be ALL CAPS).","title":"Boolean operators"},{"location":"doc/search/operators/#and","text":"search engine will find all documents which contains the word engine AND the word search search AND engine would return the same results","title":"AND"},{"location":"doc/search/operators/#or","text":"search OR research will find all documents which contains the word search or the word research","title":"OR"},{"location":"doc/search/operators/#exclusion-with-or-not","text":"The \"-\" or prohibit operator excludes documents that contain the term after the \"-\" symbol: search -engine or search NOT engine or search AND NOT engine will find all documents which contain the word search and do not contain the word engine","title":"Exclusion with - or NOT"},{"location":"doc/search/operators/#phrase-searches","text":"","title":"Phrase searches"},{"location":"doc/search/operators/#phrase-search-with-double-quotes","text":"A Phrase is a group of words surrounded by double quotes such as \"search engine\" which will find exactly this phrase and will not find i search an engine","title":"Phrase search with double quotes"},{"location":"doc/search/operators/#fuzzy-searches","text":"","title":"Fuzzy searches"},{"location":"doc/search/operators/#find-results-with-typos-or-names-not-exactly-pronounced-by-edit-distance-levensthein-distance","text":"Fuzziness by edit distance is useful if you search for names you don't know how to spell exactly or if you want to consider typos in the documents or if some documents are images or scanned documents (like PDF documents containing scanned pages only in graphical formats instead of digital text), so you maybe find more documents for your query even if the results of automated text-recognition (OCR) are not perfect). To do a fuzzy search use the tilde, \"~\", symbol at the end of a single word / term you want to search with fuzziness. For example to search for a term similar in spelling to \" roam \" use the fuzzy search: roam~ This search will find terms like foam and roams . similar~ words~ would find not only similar words but similer works , too because both words are set to fuzzy search.","title":"Find results with typos or names not exactly pronounced by edit distance (Levensthein distance)"},{"location":"doc/search/operators/#setting-maximal-edit-distance-number-of-chars-that-may-differ","text":"If you use the ~ operator for fuzziness, the default maximal edit distance is two chars. This is the maximal number of chars that can be changed, added or deleted to match a term that is searched with fuzziness. To change the maximum number of changeable chars, set the number after the ~ operator. Example: searchterm~3","title":"Setting maximal edit distance (number of chars, that may differ)"},{"location":"doc/search/operators/#grammar-stemming","text":"Considering grammar rules to find other word forms, too (stemming) , the search engine will find more. For example if you search for company corruption you will find companies corrupted , too. If you want only results matching exactly the spelling in your search query, you can disable stemming by disabling the option \"Find other word forms, too\". If you disable stemming for the whole search query, you can enable it for single terms by using the operator stemmed: If you enable stemming for the whole search query, you can disable stemming for single terms by using the operator exact:","title":"Grammar (Stemming)"},{"location":"doc/search/operators/#grouping","text":"","title":"Grouping"},{"location":"doc/search/operators/#combine-criterias-with-or-and-and-parentheses","text":"You can use parentheses to group clauses to form sub queries: (search OR research) AND (engine OR software) will find: * documents containing the words search and engine * documents containing the words search and software * documents containing the words research and engine * documents containing the words research and software * documents containing the words search and research and software and engine But it wont find a document containing the words search and research without at least one of the words engine or software . Only one of the two last words would be enough, because connected with OR, but one of them is absolutelly neccecary because this criteria combination (connected by parantheses) is connected with AND","title":"Combine criterias with OR, AND and parentheses"},{"location":"doc/search/table/","text":"Table view Search for a query. Click on the button \"Table\" to switch from standard list view to the structured table view. Press a column header to sort the table view with this column ascending. Press the same column header a second time to sort with this column descending.","title":"Table view"},{"location":"doc/search/table/#table-view","text":"Search for a query. Click on the button \"Table\" to switch from standard list view to the structured table view. Press a column header to sort the table view with this column ascending. Press the same column header a second time to sort with this column descending.","title":"Table view"},{"location":"doc/tutorial/","text":"Getting started Set up a search engine server in a few steps Open Semantic Desktop Search If you are an user and want only search for yourself, you maybe want to use the Open Semantic Desktop Search virtual maschine or the encrypted Live-System InvestigateIX , which is easier to install for single end users. Open Semantic Search Server The Open Semantic Search Server installation tutorial shows how to setup a search engine as a public website or on an intranet server for admins running a Debian GNU/Linux or Ubuntu Linux (web)server or within an existing virtual maschine. Search, explore, analyze and filter Enter a search query and enjoy the interactive userinterface for navigation, faceted search, exploratory search, interactive filters, analytics and visualizations . Config You will find flexible config options in the config files in /etc/opensemanticsearch/ .","title":"Getting started"},{"location":"doc/tutorial/#getting-started","text":"","title":"Getting started"},{"location":"doc/tutorial/#set-up-a-search-engine-server-in-a-few-steps","text":"","title":"Set up a search engine server in a few steps"},{"location":"doc/tutorial/#open-semantic-desktop-search","text":"If you are an user and want only search for yourself, you maybe want to use the Open Semantic Desktop Search virtual maschine or the encrypted Live-System InvestigateIX , which is easier to install for single end users.","title":"Open Semantic Desktop Search"},{"location":"doc/tutorial/#open-semantic-search-server","text":"The Open Semantic Search Server installation tutorial shows how to setup a search engine as a public website or on an intranet server for admins running a Debian GNU/Linux or Ubuntu Linux (web)server or within an existing virtual maschine.","title":"Open Semantic Search Server"},{"location":"doc/tutorial/#search-explore-analyze-and-filter","text":"Enter a search query and enjoy the interactive userinterface for navigation, faceted search, exploratory search, interactive filters, analytics and visualizations .","title":"Search, explore, analyze and filter"},{"location":"doc/tutorial/#config","text":"You will find flexible config options in the config files in /etc/opensemanticsearch/ .","title":"Config"},{"location":"download/","text":"Download Get the virtual machine, bundle or components and modules you want to use as open source software for free and please make a donation for further development : Open Semantic Search for single desktop computer or notebook users All in one bundles for easy installation Open Semantic Desktop Search Open Semantic Desktop Search is the all in one package for desktop users as virtual machine image configured for search on your own desktop computer or laptop running on Linux, Windows or iOS for Mac . Virtual machine disk image (Virtual Box): Multilingual VM with English keyboard settings (configurable), the OCR dictionaries / languages and some text analytics components are preset to English, but configurable multilingual. open-semantic-desktop-search_22.02.27.ova (4 GB) German (Deutsch) VM with German keyboard settings (configurable), the OCR dictionaries / languages and some text analytics components are preset to German, but configurable multilingual. Coming soon. Open Semantic Search Server for admins, research teams and organizations All in one packages for easy installation of a search server (clients only need a standard browser) Open Semantic Search Server (Package) Open Semantic Search Server is the all in one package (including Solr server, user interfaces, tools and connectors) for easy full installation on a Debian or Ubuntu based Linux server or within am existing Debian or Ubuntu based Linux virtual machine (VM). This bundle includes most modules and connectors / further packages. Just install with one command , index something and search. Debian Bullseye 11 and Ubuntu Focal 20.04 LTS: open-semantic-search_22.02.27.deb (~300 MB) Docker images Coming soon.","title":"Download"},{"location":"download/#download","text":"Get the virtual machine, bundle or components and modules you want to use as open source software for free and please make a donation for further development :","title":"Download"},{"location":"download/#open-semantic-search-for-single-desktop-computer-or-notebook-users","text":"All in one bundles for easy installation","title":"Open Semantic Search for single desktop computer or notebook users"},{"location":"download/#open-semantic-desktop-search","text":"Open Semantic Desktop Search is the all in one package for desktop users as virtual machine image configured for search on your own desktop computer or laptop running on Linux, Windows or iOS for Mac . Virtual machine disk image (Virtual Box):","title":"Open Semantic Desktop Search"},{"location":"download/#multilingual","text":"VM with English keyboard settings (configurable), the OCR dictionaries / languages and some text analytics components are preset to English, but configurable multilingual. open-semantic-desktop-search_22.02.27.ova (4 GB)","title":"Multilingual"},{"location":"download/#german-deutsch","text":"VM with German keyboard settings (configurable), the OCR dictionaries / languages and some text analytics components are preset to German, but configurable multilingual. Coming soon.","title":"German (Deutsch)"},{"location":"download/#open-semantic-search-server-for-admins-research-teams-and-organizations","text":"All in one packages for easy installation of a search server (clients only need a standard browser)","title":"Open Semantic Search Server for admins, research teams and organizations"},{"location":"download/#open-semantic-search-server-package","text":"Open Semantic Search Server is the all in one package (including Solr server, user interfaces, tools and connectors) for easy full installation on a Debian or Ubuntu based Linux server or within am existing Debian or Ubuntu based Linux virtual machine (VM). This bundle includes most modules and connectors / further packages. Just install with one command , index something and search. Debian Bullseye 11 and Ubuntu Focal 20.04 LTS: open-semantic-search_22.02.27.deb (~300 MB)","title":"Open Semantic Search Server (Package)"},{"location":"download/#docker-images","text":"Coming soon.","title":"Docker images"},{"location":"enhancer/csv/","text":"CSV Spreadsheets Enhanced by this module you can not only find content of the CSV file but use or filter the structure of a CSV spreadsheet or table while searching. So a CSV (comma seperated values) spreadsheet or table can not only be found as a single file where one word of the search query is in a row and another word of the search query in another row so that the whole file would be found even if the data of both rows is not connected, but you can search for queries that should be found in rows instead of the full CSV table. Additionally you can browse trough the rows and filter columns with the comfortable table view . This module supports a good working auto-detection of the encoding or charset of the CSV file and a less good working auto-detection of the CSV dialect.","title":"CSV Spreadsheets"},{"location":"enhancer/csv/#csv-spreadsheets","text":"Enhanced by this module you can not only find content of the CSV file but use or filter the structure of a CSV spreadsheet or table while searching. So a CSV (comma seperated values) spreadsheet or table can not only be found as a single file where one word of the search query is in a row and another word of the search query in another row so that the whole file would be found even if the data of both rows is not connected, but you can search for queries that should be found in rows instead of the full CSV table. Additionally you can browse trough the rows and filter columns with the comfortable table view . This module supports a good working auto-detection of the encoding or charset of the CSV file and a less good working auto-detection of the CSV dialect.","title":"CSV Spreadsheets"},{"location":"enhancer/geonames/","text":"Extract locations from GeoNames database This enhancer plugin extracts locations in GeoNames databases from documents, making this locations available to exploratory search, aggregated overviews and einteractive filters. Data protection and privacy: No cloud or external webservice used This plugin won't call an external webservice, so wether the analyzed texts nor the extracted locations will be sent to cloud or external web services. Installation and config Install GeoNames plugin Download and install the package opensemanticsearch-enhancer-geonames Download GeoNames database parts Download the interesting data (f.e. all names of cities with more than 1000 people or the data of your country) from Geonames data dumps . Extract the ZIP archive(s) to the directory /etc/opensemanticsearch/geonames/ Config plugin Config the databases you want to use for the whole system in /etc/opensemanticsearch/etl or for a special connector (i.e. in /etc/opensemanticsearch/connector-files ): config['geonames'] = [ '/etc/opensemanticsearch/geonames/cities1000.txt' ] You can configure more than one geonames database (which will need more ressources and cause more fase positives, because more Locations have the same name like an common word in your language): config['geonames'] = [ '/etc/opensemanticsearch/geonames/DE.txt', '/etc/opensemanticsearch/geonames/AT.txt', '/etc/opensemanticsearch/geonames/CH.txt' ] Enable plugin Enable the enhancer plugin in the same connector config: config['plugins'].append('enhance_geonames') Hint: Check for as few locations as possible Since there are many places in the world that have the same name like popular words of your language, you should check only for the interesting geoname data or parts of it. For example often its enough to check only the most important town names of your country and adding smaller local places for you very interesting custom locations to the named entities manager. Or you create custom GeoNames data files by filtering very granular of geonames database parts, i.e. for all locations within a radius to your main location.","title":"Extract locations from GeoNames database"},{"location":"enhancer/geonames/#extract-locations-from-geonames-database","text":"This enhancer plugin extracts locations in GeoNames databases from documents, making this locations available to exploratory search, aggregated overviews and einteractive filters.","title":"Extract locations from GeoNames database"},{"location":"enhancer/geonames/#data-protection-and-privacy-no-cloud-or-external-webservice-used","text":"This plugin won't call an external webservice, so wether the analyzed texts nor the extracted locations will be sent to cloud or external web services.","title":"Data protection and privacy: No cloud or external webservice used"},{"location":"enhancer/geonames/#installation-and-config","text":"","title":"Installation and config"},{"location":"enhancer/geonames/#install-geonames-plugin","text":"Download and install the package opensemanticsearch-enhancer-geonames","title":"Install GeoNames plugin"},{"location":"enhancer/geonames/#download-geonames-database-parts","text":"Download the interesting data (f.e. all names of cities with more than 1000 people or the data of your country) from Geonames data dumps . Extract the ZIP archive(s) to the directory /etc/opensemanticsearch/geonames/","title":"Download GeoNames database parts"},{"location":"enhancer/geonames/#config-plugin","text":"Config the databases you want to use for the whole system in /etc/opensemanticsearch/etl or for a special connector (i.e. in /etc/opensemanticsearch/connector-files ): config['geonames'] = [ '/etc/opensemanticsearch/geonames/cities1000.txt' ] You can configure more than one geonames database (which will need more ressources and cause more fase positives, because more Locations have the same name like an common word in your language): config['geonames'] = [ '/etc/opensemanticsearch/geonames/DE.txt', '/etc/opensemanticsearch/geonames/AT.txt', '/etc/opensemanticsearch/geonames/CH.txt' ]","title":"Config plugin"},{"location":"enhancer/geonames/#enable-plugin","text":"Enable the enhancer plugin in the same connector config: config['plugins'].append('enhance_geonames')","title":"Enable plugin"},{"location":"enhancer/geonames/#hint-check-for-as-few-locations-as-possible","text":"Since there are many places in the world that have the same name like popular words of your language, you should check only for the interesting geoname data or parts of it. For example often its enough to check only the most important town names of your country and adding smaller local places for you very interesting custom locations to the named entities manager. Or you create custom GeoNames data files by filtering very granular of geonames database parts, i.e. for all locations within a radius to your main location.","title":"Hint: Check for as few locations as possible"},{"location":"enhancer/named_entities_manager/","text":"Data enrichment module for Named Entity Extraction of managed Named Entities Named Entities Manager is an user interface (webapp) for management of named entities like persons of interest, organizations, locations or concepts. So a data enrichment module for rule based or text pattern based Entity Extraction or list or ontology based Named Entity Extraction or a integrated framework for Named Entity Recognition (NER) can recognize them and tag them for exploratory search like aggregated overviews and interactive filters. Usage See the user documentation Installation download the module opensemanticsearch-ner-python-django Copy the directory entities from the zip file into your Django apps directory Enable the new app: Add \" entities \" to your INSTALLED_APPS setting like this: INSTALLED_APPS = ( ... 'entities', ) * Include the entities URLconf in your project urls.py like this: url(r'^entities/', include('entities.urls', namespace=\"entities\")), * Call http://localhost/search-apps/entities/apply after each data import or after editing your entities or by Cron from time to time until this module will be better integrated into the connectors by - instead of search for each query after indexing - compare the data with all queries before indexing, since such queries can not be only static strings (which are extracted on indexing time automatically without searching for them) but can be complex queires with wildcards and boolean operators, too.","title":"Data enrichment module for Named Entity Extraction of managed Named Entities"},{"location":"enhancer/named_entities_manager/#data-enrichment-module-for-named-entity-extraction-of-managed-named-entities","text":"Named Entities Manager is an user interface (webapp) for management of named entities like persons of interest, organizations, locations or concepts. So a data enrichment module for rule based or text pattern based Entity Extraction or list or ontology based Named Entity Extraction or a integrated framework for Named Entity Recognition (NER) can recognize them and tag them for exploratory search like aggregated overviews and interactive filters.","title":"Data enrichment module for Named Entity Extraction of managed Named Entities"},{"location":"enhancer/named_entities_manager/#usage","text":"See the user documentation","title":"Usage"},{"location":"enhancer/named_entities_manager/#installation","text":"download the module opensemanticsearch-ner-python-django Copy the directory entities from the zip file into your Django apps directory Enable the new app: Add \" entities \" to your INSTALLED_APPS setting like this: INSTALLED_APPS = ( ... 'entities', ) * Include the entities URLconf in your project urls.py like this: url(r'^entities/', include('entities.urls', namespace=\"entities\")), * Call http://localhost/search-apps/entities/apply after each data import or after editing your entities or by Cron from time to time until this module will be better integrated into the connectors by - instead of search for each query after indexing - compare the data with all queries before indexing, since such queries can not be only static strings (which are extracted on indexing time automatically without searching for them) but can be complex queires with wildcards and boolean operators, too.","title":"Installation"},{"location":"enhancer/named_entitiy_recognition/stanford_named_entity_recognizer/","text":"Named Entity Recognition by Stanford Named Entity Recognizer (NER) Automatic Named Entity Recognition by machine learning (ML) for automatic classification and annotation of text parts Extracted named entities like Persons, Organizations or Locations (Named entity extraction) are used for structured navigation, aggregated overviews and interactive filters (faceted search) . Additionally to known named entities in a thesaurus or imported ontologies this data analysis plugin integrates Named Entity Recognition (NER) by Stanford Named Entity Recognizer (Stanford NER) . Named Entity Extraction of yet unknown entities or names So by integration of machine learning for analysing the structure of the text and classifying parts/words of the sentences to categories like person, location or organization, many yet unknown named entities can be extracted, which aren't configured or listed yet in the thesaurus Named entities Manager or a list or ontology. Therefore it uses models trained with existing annotations of a large text corpus, so after that they can predict / guess if a part of a sentence is a name of a person, a name of an organization, a verb or a place. Configuration of the Stanford NER plugin To enable the automatic named entity recognition with Stanford Named Entitiy Recognizer (NER), enable this plugin to your document processing chain in your ETL config /etc/opensemanticsearch/etl or a connector config: config['plugins'].append('enhance_ner_stanford') Config language (machine learning model) You can config alternate classifiers, i.e. for a machine learning model for another language than english or for your own domain knowledge model by the config option config['stanford_ner_classifier'] or dependent on autodetected document language in the config option config['stanford_ner_classifiers'] in the same config file, where you find additional information on this options. How to improve performance Named Entity Extraction of named entities managed by the Named Entities Manager or lists of names works very fast for (not complex queries) like static names or strings. So if you know many names yet, you should setup them by the Lists and Ontologies Manager, since that works more precise and so you get faster and more complete results for known names. In comparison to the integration of lists of entitiy names while indexing process the machine learning plugin for Stanford NER works very slow. Not only because Named Entities Recognition of unknown entities by machine learning needs much more CPU power and RAM than Entity Linking of known entities by Solr standard text analysis methods using limited well known names/words instead of much more complex probability and cluster based classification models. The architecture of calling Stanford Named Entitiy Recognizer (NER) from the used Python library Natural Language Toolkit (NLTK) is not optimized for speed if you want to analyse many texts, since it loads the JAR and the whole classification model for each text analysis,. For more performance for faster analysis of many documents you should setup an Stanford Stanford Named Entitiy Recognizer server instead of using this default NER plugin using NLTK which uses the Stanford Named Entitiy Recognizer command line tool. So the analysis of each document would take a few seconds less time (the time the Java based command line tool need to load and to load and initialize the machine learning model). Or wait, until the existing Stanford NER integration with Apache Tika will be default feature working out of the box, since our Apache Tika is running as server that has to load only once. Dependencies and used libraries Stanford Named Entity Recognizer (NER) Stanford Tagger Module from the Python natural language processing library Natural Language Toolkit (NLTK) Debian package: python-nltk","title":"Named Entity Recognition by Stanford Named Entity Recognizer (NER)"},{"location":"enhancer/named_entitiy_recognition/stanford_named_entity_recognizer/#named-entity-recognition-by-stanford-named-entity-recognizer-ner","text":"","title":"Named Entity Recognition by Stanford Named Entity Recognizer (NER)"},{"location":"enhancer/named_entitiy_recognition/stanford_named_entity_recognizer/#automatic-named-entity-recognition-by-machine-learning-ml-for-automatic-classification-and-annotation-of-text-parts","text":"Extracted named entities like Persons, Organizations or Locations (Named entity extraction) are used for structured navigation, aggregated overviews and interactive filters (faceted search) . Additionally to known named entities in a thesaurus or imported ontologies this data analysis plugin integrates Named Entity Recognition (NER) by Stanford Named Entity Recognizer (Stanford NER) .","title":"Automatic Named Entity Recognition by machine learning (ML) for automatic classification and annotation of text parts"},{"location":"enhancer/named_entitiy_recognition/stanford_named_entity_recognizer/#named-entity-extraction-of-yet-unknown-entities-or-names","text":"So by integration of machine learning for analysing the structure of the text and classifying parts/words of the sentences to categories like person, location or organization, many yet unknown named entities can be extracted, which aren't configured or listed yet in the thesaurus Named entities Manager or a list or ontology. Therefore it uses models trained with existing annotations of a large text corpus, so after that they can predict / guess if a part of a sentence is a name of a person, a name of an organization, a verb or a place.","title":"Named Entity Extraction of yet unknown entities or names"},{"location":"enhancer/named_entitiy_recognition/stanford_named_entity_recognizer/#configuration-of-the-stanford-ner-plugin","text":"To enable the automatic named entity recognition with Stanford Named Entitiy Recognizer (NER), enable this plugin to your document processing chain in your ETL config /etc/opensemanticsearch/etl or a connector config: config['plugins'].append('enhance_ner_stanford')","title":"Configuration of the Stanford NER plugin"},{"location":"enhancer/named_entitiy_recognition/stanford_named_entity_recognizer/#config-language-machine-learning-model","text":"You can config alternate classifiers, i.e. for a machine learning model for another language than english or for your own domain knowledge model by the config option config['stanford_ner_classifier'] or dependent on autodetected document language in the config option config['stanford_ner_classifiers'] in the same config file, where you find additional information on this options.","title":"Config language (machine learning model)"},{"location":"enhancer/named_entitiy_recognition/stanford_named_entity_recognizer/#how-to-improve-performance","text":"Named Entity Extraction of named entities managed by the Named Entities Manager or lists of names works very fast for (not complex queries) like static names or strings. So if you know many names yet, you should setup them by the Lists and Ontologies Manager, since that works more precise and so you get faster and more complete results for known names. In comparison to the integration of lists of entitiy names while indexing process the machine learning plugin for Stanford NER works very slow. Not only because Named Entities Recognition of unknown entities by machine learning needs much more CPU power and RAM than Entity Linking of known entities by Solr standard text analysis methods using limited well known names/words instead of much more complex probability and cluster based classification models. The architecture of calling Stanford Named Entitiy Recognizer (NER) from the used Python library Natural Language Toolkit (NLTK) is not optimized for speed if you want to analyse many texts, since it loads the JAR and the whole classification model for each text analysis,. For more performance for faster analysis of many documents you should setup an Stanford Stanford Named Entitiy Recognizer server instead of using this default NER plugin using NLTK which uses the Stanford Named Entitiy Recognizer command line tool. So the analysis of each document would take a few seconds less time (the time the Java based command line tool need to load and to load and initialize the machine learning model). Or wait, until the existing Stanford NER integration with Apache Tika will be default feature working out of the box, since our Apache Tika is running as server that has to load only once.","title":"How to improve performance"},{"location":"enhancer/named_entitiy_recognition/stanford_named_entity_recognizer/#dependencies-and-used-libraries","text":"Stanford Named Entity Recognizer (NER) Stanford Tagger Module from the Python natural language processing library Natural Language Toolkit (NLTK) Debian package: python-nltk","title":"Dependencies and used libraries"},{"location":"enhancer/ocr/","text":"Automatic text recogniton in images by optical character recognition (OCR) Optical character recognition (OCR) for automatic text recognition of text in image files or embedded images like scanned documents within a PDF document Text stored in image formats like JPG, PNG, TIFF or GIF (i.e. scans, photos or screenshots) can not be found by standard fulltext search. So this enhancer enriches meta data of images like filename, format and size with results from automatic text recognition (OCR). Since many information is not searchable by fulltext search because its in graphical formats embedded in PDF documents or Powerpoint presentations (i.e. screenshots instead of text format), the enhancer extracts images from PDF files for automatic textrecognition (OCR), too. Enable OCR Some OCR tasks are off by default, because it slows down indexing. It uses many processor resources and will need many seconds for each graphic file. If you want to enable OCR for images: Since meanwhile Apache Tika will OCR most images for you by default, if Teseract is installed, you don't have to activate the disabled plugin for OCR of images. Enable OCR for embedded images or scans inside PDF documents But if you enabled OCR by installing Tesseract, you should enable OCR for images inside PDF files, too, since many PDF files are scans and do contain much text data only as graphics. Therefore enable the plugin enhance_pdf_ocr . OCR language Setting OCR language to an other language than english: 1. Install the Tesseract language package (for german: tesseract-ocr-deu ). See the list of available languages for Debian or Ubuntu . 2. set option ocr_language to the language of your documents. Default is eng for english (in tesseract its eng , not en !). For german set deu (in tesseract its not de !): `# language for automatic text recognition config['ocr_lang'] = \"eng\" config['ocr_lang']=\"deu\"` Deskewing low quality scans before OCR To enable an additional deskewing of low quality scans with Scantailor before OCR for getting better results in some cases: Install Scantailor: apt-get install scantailor Enable additional optimization with Scantailor before OCR by enable the plugin enhance_ocr_descew in your connector or ETL config. Hint for user of older Debian versions: If this package doesn't exist in the Debian version you use, you can get a package in the Debian backports repository.","title":"Automatic text recogniton in images by optical character recognition (OCR)"},{"location":"enhancer/ocr/#automatic-text-recogniton-in-images-by-optical-character-recognition-ocr","text":"","title":"Automatic text recogniton in images by optical character recognition (OCR)"},{"location":"enhancer/ocr/#optical-character-recognition-ocr-for-automatic-text-recognition-of-text-in-image-files-or-embedded-images-like-scanned-documents-within-a-pdf-document","text":"Text stored in image formats like JPG, PNG, TIFF or GIF (i.e. scans, photos or screenshots) can not be found by standard fulltext search. So this enhancer enriches meta data of images like filename, format and size with results from automatic text recognition (OCR). Since many information is not searchable by fulltext search because its in graphical formats embedded in PDF documents or Powerpoint presentations (i.e. screenshots instead of text format), the enhancer extracts images from PDF files for automatic textrecognition (OCR), too.","title":"Optical character recognition (OCR) for automatic text recognition of text in image files or embedded images like scanned documents within a PDF document"},{"location":"enhancer/ocr/#enable-ocr","text":"Some OCR tasks are off by default, because it slows down indexing. It uses many processor resources and will need many seconds for each graphic file. If you want to enable OCR for images: Since meanwhile Apache Tika will OCR most images for you by default, if Teseract is installed, you don't have to activate the disabled plugin for OCR of images.","title":"Enable OCR"},{"location":"enhancer/ocr/#enable-ocr-for-embedded-images-or-scans-inside-pdf-documents","text":"But if you enabled OCR by installing Tesseract, you should enable OCR for images inside PDF files, too, since many PDF files are scans and do contain much text data only as graphics. Therefore enable the plugin enhance_pdf_ocr .","title":"Enable OCR for embedded images or scans inside PDF documents"},{"location":"enhancer/ocr/#ocr-language","text":"Setting OCR language to an other language than english: 1. Install the Tesseract language package (for german: tesseract-ocr-deu ). See the list of available languages for Debian or Ubuntu . 2. set option ocr_language to the language of your documents. Default is eng for english (in tesseract its eng , not en !). For german set deu (in tesseract its not de !): `# language for automatic text recognition","title":"OCR language"},{"location":"enhancer/ocr/#configocr_lang-eng","text":"config['ocr_lang']=\"deu\"`","title":"config['ocr_lang'] = \"eng\""},{"location":"enhancer/ocr/#deskewing-low-quality-scans-before-ocr","text":"To enable an additional deskewing of low quality scans with Scantailor before OCR for getting better results in some cases: Install Scantailor: apt-get install scantailor Enable additional optimization with Scantailor before OCR by enable the plugin enhance_ocr_descew in your connector or ETL config. Hint for user of older Debian versions: If this package doesn't exist in the Debian version you use, you can get a package in the Debian backports repository.","title":"Deskewing low quality scans before OCR"},{"location":"enhancer/rdf/","text":"Meta data enrichment or annotator from Resource Description Framework (RDF) to Solr or Elasticsearch Annotator for labels from ontologies or RDF datasets to Solr or Elastic Search documents This enhancer will enrich existing files, content or URIs with additional metadata in Resource Description Framework (RDF) standard format stored on the semantic web or intranet i.e. on a meta data server, annotation server or CMS (i.e. tags and annotations in a Semantic Mediawiki or in Drupal CMS ) and annotates the index of Elastic Search or Solr with the labels of the RDF annotations. Therefore it calls the meta data server with the URI as parameter or query. If you want to import whole RDF graphs from RDF files or (parts) from a triplestore as content or data source for search and text mining, use the RDF connector instead. Configuration Config file: */etc/opensemanticsearch/enhancer/rdf* Servers Set the url of your meta data server or a list of meta data servers delivering RDF meta data for other URLs. As parameter you can use [uri] as template for the URL or [uri_md5] for the MD5 Checksum of the URL, so the CMS is able to search for the meta data of the URL of the original content you want to enrich while indexing. config['metaserver'] = { 'http://localhost/drupal/rdf?uri=[uri]', 'http://localhost/mediawiki/index.php/Special:ExportRDF?page=[uri_md5]', } Mappings RDF properties to Solr fields or facets So you can map RDF properties to Solr fields / facets / interactive filters: config['property2facet'] = { 'http://purl.org/dc/terms/Tags': 'tag_ss', 'http://purl.org/dc/terms/location': 'location_ss' } You can use standard fields like title, author or content or tags (Solr field name: tag_ss). Custom fields / facets If there are no convenient fields / facets yet, you can map the RDF properties to additional custom fields/facets: If you dont want to use standard fields like title, author and content you dont have to change the config file managed-schema (which defines the fields of the Solr index). Dynamic fields Our Solr server is preconfigurated with dynamic fields so that you can fill standard fields like title or content or additional dynamic fields like yourfield_b for one boolean, yourfield_bs for booleans, yourfield_s for a string and yourfield_ss for some strings to be filled with data. Enable new fields or facets in the user interface If you did not use preconfigurated fields like tags (fieldname is tag_ss ) and want to use them not only to find data but as interactive filters (facets) for the navigation: To enable your own additional fields as facets (interactive filters) in the user interface just map the technical solr fieldnames to user friendly labels in the config of the user interface with the option $cfg[facet] .","title":"Meta data enrichment or annotator from Resource Description Framework (RDF) to Solr or Elasticsearch"},{"location":"enhancer/rdf/#meta-data-enrichment-or-annotator-from-resource-description-framework-rdf-to-solr-or-elasticsearch","text":"","title":"Meta data enrichment or annotator from Resource Description Framework (RDF) to Solr or Elasticsearch"},{"location":"enhancer/rdf/#annotator-for-labels-from-ontologies-or-rdf-datasets-to-solr-or-elastic-search-documents","text":"This enhancer will enrich existing files, content or URIs with additional metadata in Resource Description Framework (RDF) standard format stored on the semantic web or intranet i.e. on a meta data server, annotation server or CMS (i.e. tags and annotations in a Semantic Mediawiki or in Drupal CMS ) and annotates the index of Elastic Search or Solr with the labels of the RDF annotations. Therefore it calls the meta data server with the URI as parameter or query. If you want to import whole RDF graphs from RDF files or (parts) from a triplestore as content or data source for search and text mining, use the RDF connector instead.","title":"Annotator for labels from ontologies or RDF datasets to Solr or Elastic Search documents"},{"location":"enhancer/rdf/#configuration","text":"Config file: */etc/opensemanticsearch/enhancer/rdf*","title":"Configuration"},{"location":"enhancer/rdf/#servers","text":"Set the url of your meta data server or a list of meta data servers delivering RDF meta data for other URLs. As parameter you can use [uri] as template for the URL or [uri_md5] for the MD5 Checksum of the URL, so the CMS is able to search for the meta data of the URL of the original content you want to enrich while indexing. config['metaserver'] = { 'http://localhost/drupal/rdf?uri=[uri]', 'http://localhost/mediawiki/index.php/Special:ExportRDF?page=[uri_md5]', }","title":"Servers"},{"location":"enhancer/rdf/#mappings-rdf-properties-to-solr-fields-or-facets","text":"So you can map RDF properties to Solr fields / facets / interactive filters: config['property2facet'] = { 'http://purl.org/dc/terms/Tags': 'tag_ss', 'http://purl.org/dc/terms/location': 'location_ss' } You can use standard fields like title, author or content or tags (Solr field name: tag_ss).","title":"Mappings RDF properties to Solr fields or facets"},{"location":"enhancer/rdf/#custom-fields-facets","text":"If there are no convenient fields / facets yet, you can map the RDF properties to additional custom fields/facets: If you dont want to use standard fields like title, author and content you dont have to change the config file managed-schema (which defines the fields of the Solr index).","title":"Custom fields / facets"},{"location":"enhancer/rdf/#dynamic-fields","text":"Our Solr server is preconfigurated with dynamic fields so that you can fill standard fields like title or content or additional dynamic fields like yourfield_b for one boolean, yourfield_bs for booleans, yourfield_s for a string and yourfield_ss for some strings to be filled with data.","title":"Dynamic fields"},{"location":"enhancer/rdf/#enable-new-fields-or-facets-in-the-user-interface","text":"If you did not use preconfigurated fields like tags (fieldname is tag_ss ) and want to use them not only to find data but as interactive filters (facets) for the navigation: To enable your own additional fields as facets (interactive filters) in the user interface just map the technical solr fieldnames to user friendly labels in the config of the user interface with the option $cfg[facet] .","title":"Enable new fields or facets in the user interface"},{"location":"enhancer/rdf-drupal/","text":"Enhancer RDF Drupal Metadata management like tagging or annotation with Drupal This module integrates the powerful and generic Drupal CMS for metadata management like tagging or annotation . Drupal provides flexible custom data structures and easy to use user interfaces to configure them. So you can use powerful Drupal features like custom fields , flexible data structures like taxonomies for tags or flexible user management , i.e. for tagging, verification, (re)viewing, classifying or (re)ranking large document sets. Since you can use the flexible user management tools of Drupal, you can use this module for crowdsourcing on the net (i.e. for datajournalism) using only the data of the users you trust or which was moderated or verified. Installation and configuration Download and extract the module zip file Import our content type meta Import the view edit.view into your drupal views Import the view rdf.view into drupal views Config metadataserver in /etc/solr/solr-enhancer-rdf to the url of your drupal installation Usage Example for a direct link for editing metadata for opensemanticsearch.org: http://localhost/drupal/edit?uri=http://www.opensemanticsearch.org There are two ways you dont need to type in such URLs manually: Configure your Drupal link http://localhost/drupal/edit?uri=[URI] for tagging and annotation in the search user interface config. Or use a bookmarklet integrated in your Web browser, so you can add or annotate a new URI with one click. Custom metadata content type and custom form You can add own custom fields to our metadata content type meta to customize it. Or you use your own custom metadata content types and custom forms: Just add a field named \"uri\" (internal saved as field_uri in drupal) to the (new) content type you want to use for metadata of this URL and change the contenttype from meta to your own in the views edit and rdf .","title":"Enhancer RDF Drupal"},{"location":"enhancer/rdf-drupal/#enhancer-rdf-drupal","text":"","title":"Enhancer RDF Drupal"},{"location":"enhancer/rdf-drupal/#metadata-management-like-tagging-or-annotation-with-drupal","text":"This module integrates the powerful and generic Drupal CMS for metadata management like tagging or annotation . Drupal provides flexible custom data structures and easy to use user interfaces to configure them. So you can use powerful Drupal features like custom fields , flexible data structures like taxonomies for tags or flexible user management , i.e. for tagging, verification, (re)viewing, classifying or (re)ranking large document sets. Since you can use the flexible user management tools of Drupal, you can use this module for crowdsourcing on the net (i.e. for datajournalism) using only the data of the users you trust or which was moderated or verified.","title":"Metadata management like tagging or annotation with Drupal"},{"location":"enhancer/rdf-drupal/#installation-and-configuration","text":"Download and extract the module zip file Import our content type meta Import the view edit.view into your drupal views Import the view rdf.view into drupal views Config metadataserver in /etc/solr/solr-enhancer-rdf to the url of your drupal installation","title":"Installation and configuration"},{"location":"enhancer/rdf-drupal/#usage","text":"Example for a direct link for editing metadata for opensemanticsearch.org: http://localhost/drupal/edit?uri=http://www.opensemanticsearch.org There are two ways you dont need to type in such URLs manually: Configure your Drupal link http://localhost/drupal/edit?uri=[URI] for tagging and annotation in the search user interface config. Or use a bookmarklet integrated in your Web browser, so you can add or annotate a new URI with one click.","title":"Usage"},{"location":"enhancer/rdf-drupal/#custom-metadata-content-type-and-custom-form","text":"You can add own custom fields to our metadata content type meta to customize it. Or you use your own custom metadata content types and custom forms: Just add a field named \"uri\" (internal saved as field_uri in drupal) to the (new) content type you want to use for metadata of this URL and change the contenttype from meta to your own in the views edit and rdf .","title":"Custom metadata content type and custom form"},{"location":"enhancer/regex/","text":"Extract structured data from text by text patterns (Regular Expressions) You can extract some structured data for aggregated overviews, interactive navigation and interactive filters (faceted search), data analysis and data visualization from unstructured text with regular expressions (RegEx). You can extract some structured data i.e. for aggregated overviews, interactive navigation and interactive filters (faceted search), data analysis and data visualization from unstructured text by extraction of the interesting text parts to structured fields, properties or facets by defining text patterns with regular expressions (RegEx). Named entities like persons, organizations, locations or concepts Regular expressions doesn't mean Named Entities with names that are strings that do not change. If you want to identify named entities like known persons, organizations, location or concepts, you should use the Named Entities Manager with a comfortable user interface and which needs less resources. Regular expressions for defining text patterns Usage of the regular expression plugin is for text patterns with dynamic parts or variables. Preconfigured regular expressions There are some preconfigurated config files in /etc/opensemanticsearch/regex like regex-email.tsv extracting email-addresses or regex-money.tsv for extraction of amounts of money. Custom regular expressions If you want to add custom regular expressions, you should use the custom regex config file /etc/opensemanticsearch/regex/regex-custom.tsv so you can updates the preconfigurated lists automatically without overwriting your own config. Custom regular expressions lists Sometimes you might want not only to add some custom regular expressions, but a whole list or more than one. For example if you want to exchange such configs or lists with partners, so you should seperate such configs for special topics from your custom regular expressions. To add another regex list just add to the config in /etc/opensemanticsearch/etl config['regex_lists'].append('/etc/opensemanticsearch/regex/regex-anOwnList.tsv') Config format for extraction of text patterns The text patterns are configured in a textfile config in TSV format (tab seperated values). For a new pattern just add a new line to the config. Begin with the text patterns you search for as a regular expression . Optionally you can add a tab and the name of a facet/field where to write the matches. If no facet / field added, the default facet/field \"Tags\" will be used. Lines with # as first char of the line are ignored. Example: # Write text like *1000.00 $* or *100$* to the facet *dollar\\_ss* [0-9]+[\\.\\,]?[0-9]*[:blank:]*\\$ dollar_ss Extract only parts of a pattern (RegEx groups) Sometimes you don't want to extract the whole matching pattern like size of 1000 m , but only a part of it like only the number. Regular expressions can group part of the pattern with () If your pattern defines RegEx groups you can configure with an additional tab and a following number which groups result will be used. If you set the group to 0, the whole matching pattern will be extracted, if you set the group to 1 the (first) group of the regular expression will be extracted to the configured facet/field. If you set 2 the second group (if more than one group defined) or the regular expression will be extracted and so on... Example: Size of ([0-9]+)[:blank:]?m size_ss 1","title":"Extract structured data from text by text patterns (Regular Expressions)"},{"location":"enhancer/regex/#extract-structured-data-from-text-by-text-patterns-regular-expressions","text":"You can extract some structured data for aggregated overviews, interactive navigation and interactive filters (faceted search), data analysis and data visualization from unstructured text with regular expressions (RegEx). You can extract some structured data i.e. for aggregated overviews, interactive navigation and interactive filters (faceted search), data analysis and data visualization from unstructured text by extraction of the interesting text parts to structured fields, properties or facets by defining text patterns with regular expressions (RegEx).","title":"Extract structured data from text by text patterns (Regular Expressions)"},{"location":"enhancer/regex/#named-entities-like-persons-organizations-locations-or-concepts","text":"Regular expressions doesn't mean Named Entities with names that are strings that do not change. If you want to identify named entities like known persons, organizations, location or concepts, you should use the Named Entities Manager with a comfortable user interface and which needs less resources.","title":"Named entities like persons, organizations, locations or concepts"},{"location":"enhancer/regex/#regular-expressions-for-defining-text-patterns","text":"Usage of the regular expression plugin is for text patterns with dynamic parts or variables.","title":"Regular expressions for defining text patterns"},{"location":"enhancer/regex/#preconfigured-regular-expressions","text":"There are some preconfigurated config files in /etc/opensemanticsearch/regex like regex-email.tsv extracting email-addresses or regex-money.tsv for extraction of amounts of money.","title":"Preconfigured regular expressions"},{"location":"enhancer/regex/#custom-regular-expressions","text":"If you want to add custom regular expressions, you should use the custom regex config file /etc/opensemanticsearch/regex/regex-custom.tsv so you can updates the preconfigurated lists automatically without overwriting your own config.","title":"Custom regular expressions"},{"location":"enhancer/regex/#custom-regular-expressions-lists","text":"Sometimes you might want not only to add some custom regular expressions, but a whole list or more than one. For example if you want to exchange such configs or lists with partners, so you should seperate such configs for special topics from your custom regular expressions. To add another regex list just add to the config in /etc/opensemanticsearch/etl config['regex_lists'].append('/etc/opensemanticsearch/regex/regex-anOwnList.tsv')","title":"Custom regular expressions lists"},{"location":"enhancer/regex/#config-format-for-extraction-of-text-patterns","text":"The text patterns are configured in a textfile config in TSV format (tab seperated values). For a new pattern just add a new line to the config. Begin with the text patterns you search for as a regular expression . Optionally you can add a tab and the name of a facet/field where to write the matches. If no facet / field added, the default facet/field \"Tags\" will be used. Lines with # as first char of the line are ignored. Example: # Write text like *1000.00 $* or *100$* to the facet *dollar\\_ss* [0-9]+[\\.\\,]?[0-9]*[:blank:]*\\$ dollar_ss","title":"Config format for extraction of text patterns"},{"location":"enhancer/regex/#extract-only-parts-of-a-pattern-regex-groups","text":"Sometimes you don't want to extract the whole matching pattern like size of 1000 m , but only a part of it like only the number. Regular expressions can group part of the pattern with () If your pattern defines RegEx groups you can configure with an additional tab and a following number which groups result will be used. If you set the group to 0, the whole matching pattern will be extracted, if you set the group to 1 the (first) group of the regular expression will be extracted to the configured facet/field. If you set 2 the second group (if more than one group defined) or the regular expression will be extracted and so on... Example: Size of ([0-9]+)[:blank:]?m size_ss 1","title":"Extract only parts of a pattern (RegEx groups)"},{"location":"enhancer/speech/","text":"Speech recognition for audio files This data enrichment plugin extracts text from speech in audio or sound files like WAV or MP3 so it there are findable by content automatically even if not tagged. For privacy issues it does not upload your file to cloud services. So you can use this plugin even offline without any internet connection. Todo: This plugin works at the moment only for short audio files. For very long audios please wait, until the plugin will integrate a tool, which cuts too long audios into parts (which should be between words in a certain time, not exactly after a certain time).","title":"Speech recognition for audio files"},{"location":"enhancer/speech/#speech-recognition-for-audio-files","text":"This data enrichment plugin extracts text from speech in audio or sound files like WAV or MP3 so it there are findable by content automatically even if not tagged. For privacy issues it does not upload your file to cloud services. So you can use this plugin even offline without any internet connection. Todo: This plugin works at the moment only for short audio files. For very long audios please wait, until the plugin will integrate a tool, which cuts too long audios into parts (which should be between words in a certain time, not exactly after a certain time).","title":"Speech recognition for audio files"},{"location":"enhancer/xmp-sidecar-files/","text":"XMP sidecar files This enhancer module reads additionally image metadata like title, tags or descriptions from XMP (Extensible Metadata Plattform) sidecar files i.e. from the photo database Adobe Photoshop Lightroom or JPhototagger . Installation Just download and install the module. Configuration Just setup in the config file /etc/opensemanticsearch/connector-files the following option: config['xmp'] = True","title":"XMP sidecar files"},{"location":"enhancer/xmp-sidecar-files/#xmp-sidecar-files","text":"This enhancer module reads additionally image metadata like title, tags or descriptions from XMP (Extensible Metadata Plattform) sidecar files i.e. from the photo database Adobe Photoshop Lightroom or JPhototagger .","title":"XMP sidecar files"},{"location":"enhancer/xmp-sidecar-files/#installation","text":"Just download and install the module.","title":"Installation"},{"location":"enhancer/xmp-sidecar-files/#configuration","text":"Just setup in the config file /etc/opensemanticsearch/connector-files the following option: config['xmp'] = True","title":"Configuration"},{"location":"enhancer/zip/","text":"Enhancer ZIP This enhancer recognizes ZIP archives to unzip its content and index each file inside the zip file, too. So it extends the possibility to search for the ZIP file itself and its metadata like the contained filenames to be able to find the full contents of zipped documents, presentations and all other files within the archive file, too. Additionally all files inside the ZIP-archive will be enhanced by all other configured enhancers, too. I.e. with the OCR enhancer for automatic text recognition for images even inside the ZIP files.","title":"Enhancer ZIP"},{"location":"enhancer/zip/#enhancer-zip","text":"This enhancer recognizes ZIP archives to unzip its content and index each file inside the zip file, too. So it extends the possibility to search for the ZIP file itself and its metadata like the contained filenames to be able to find the full contents of zipped documents, presentations and all other files within the archive file, too. Additionally all files inside the ZIP-archive will be enhanced by all other configured enhancers, too. I.e. with the OCR enhancer for automatic text recognition for images even inside the ZIP files.","title":"Enhancer ZIP"},{"location":"etl/","text":"Open Semantic ETL toolkit for data integration, data analysis, document analysis, information extraction & data enrichment Open source frameworks for data integration, document processing, information extraction, data analysis, merging & combining data, content enrichment and data enrichment pipelines Since most data is available in open standards or extractable by open source software libraries and free software, you can use different open source toolkits or frameworks to extract, transform and load (ETL) data into the search index. The preconfigured Open Semantic ETL is a Python based lightweight, flexible, extendable, modular and interoperable free software and open source ETL (extract, transform, load), content enrichment and data enrichment framework, toolkit or data enrichment management system for document processing, automated content analysis and media analysis, information extraction, merge and data enrichment pipelines managing multiple, different and modular import, extraction, content analysis, data combining or data enrichment plugins . Since the architecture of the search engine is modular, using open standards for Linked Data and Semantic Web like RDF or SKOS and the basis Elastic Search or Solr providing common standard APIs, you can use or integrate many other alternate Open Source ETL and data analysis tools instead or additionally. Extract structured data from unstructured documents (Information extraction), merge and enrich data with multiple other data sources and data analysis tools Modular data enrichment plugins (enhancer) extract structured data from even from unstructured documents or plain text and enhance or enrich the content with additional meta data or analytics. Document processing, document analysis, data integration, content analysis and data enrichment pipeline (Enhancement chain) The document processing pipeline or chain is a list of data enrichment plugins (enhancer), which will be runned for each document to enrich, analyse or link them with additional data or analysis. A part of the default document processing pipeline configuration is for example: * Crawl a directory and its files and subdirectories * Filter blacklists (filter_blacklist) * Filter if file indexed yet and not modified since last indexing (filter_file_not_modified) * Extract text (enhance_text) * OCR images (enhance_ocr) * Adding annotations and tags (enhance_rdf) * Exporting or indexing to SQL or NoSQL database and/or search index (f.e. Apache Solr or Elastic Search or a Linked Data triplestore like Apache Jena Fuseki ) Semantic data enrichment plugins Such modular data enrichment plugins (enhancer) will extract structured data from unstrucutred documents, enhance or enrich the content with additional meta data or analytics. For example the named entities extractor or OCR for image files . Configuration of a custom document processing, content analysis and data enrichment pipeline If you use only Open Semantic ETL you can use */etc/etl/config* to setup your data analysis and data enrichment chain and to set an db/exporter/writer where to store or index the results (for example Solr , Elastic Search , a triplestore or a database). If you use the preconfigurated full search engine Open Semantic Search the pipeline or the enabled plugins are configurated for all data sources in */etc/opensemanticsearch/etl* or can be overwritten or extended for each data source or connector in their specialized configs like *connector-files* . The analysis chain runs in order, since some plugins depend on data analysis of other plugins. You can add additional or new data enrichment plugins to config['plugins'] . Or you overwrite this config option to define a custom data enrichment pipeline with only a few needed plugins. You can overwrite the config by parameters of the command line tools, for example to use a custom config file or to set the plugin for your data analysis and data enrichment chain. So you can set a custom config file on the command line with the parameter --config , for example: etl-file --config */etc/etl/MyCustomConfig* *filename* Usage: Extract, analyze and enrich Extract, analyze and enrich and export data from files or webpages: etl-file *filename* to import/extract/analyze/enrich a file or etl-file *directory* to import/extract/analyze/enrich all files of a directory and its sub directories or etl-file-monitoring *directory or file* for monitoring a directory and import/extract/analyze/enrich new or changed files or etl-web *uri* to download a file or webpage from the web and extract, analyze and enrich it Enrich parts, enrich later, add additional enrichments, update data enrichments or distributed data enrichment The tool etl-enrich can run data enrichment parts or plugins which are not enabled in your default document processing pipe later or from time to time. For example sometimes its better to index all documents without OCR in short time and after that to do the OCR of the documents with images which will need long time. So the users are able to search in most documents and text, not having to wait until only few parts and only for a few documents like some text in images are recognized in a long time process first before other documents after them were indexed, which takes only very few time, because there are no images. Or you can do expensive data enrichment like OCR at night or on low server load or distribute this work on different processors (parallel processing) or servers (cluster) or web services (cloud). Another possibility is to enrich with tools or webservices that imporoved or updated their results because of better analytics quality or more available data from time to time to integrate newer data or analytcs results. Or to enrich later with a additional webservice, without to have to run the full document processing chain again. Or if a webservice was not available while indexing to enrich data with its analytics later. Run additional data analysis or data enrichment plugins You can run the tool from REST-API or on the command line: etl-enrich --plugins *pluginname* Optional you can add a search or filter query, so only the interesting or important data or document(s) will get enriched: etl-enrich --plugins *pluginname* --query *query* Enrichment with results of webservices and APIs You can analyze your data with internal webservices (or if you dont need privacy with external webservices or \"the cloud\") and read the results with the standard RDF enhancer plugin. If the webservice results are not in standard Semantic Web formats, but only an API, you can call the API and read the results in your favorite programming language in a custom data enrichment plugin: Development of own data enrichment plugins You can develop own data enrichment plugins with few lines of code using the data enrichment interfaces for Python plugins, Javascript plugins or Java Plugins. Or you develop or use a webservice in your favorite programming language and read its results with the standard RDF enhancer plugin. Learn more about development of data enrichment plugins ... Scale for big data analysis If there is big data and you have to index faster, you can scale by parallel processing, search clusters and optimizing RAM settings . Other frameworks for data integration for data warehouse or for extraction, transformation and load (ETL) or data enrichment There are powerful open source ETL frameworks (extraction, transformation and load) for data integration, mapping, filtering and transformation for data warehousing with powerful features and graphical user interfaces (GUI). If there is an output plugin for Solr or if the tools can export data in a format which can be imported by one of our connectors (crawler, extraction or input plugin) , you can use this open source frameworks like Kettle or Talend Open Studio to integrate, extract, transform, enrich and load (index) data to the search engine. Other data enrichment frameworks based on Semantic Web standards You can use the Apache Stanbol data enrichment framework and one or more of its many enhancer engines . Transform unstructured documents to semantic linked data There are not only output/storage/index writer/exporter plugins for search engines like Solr or Elastic Search, but for triplestores, too: So with its triplestore storage or output plugins for RDF export it can integrate unstructured data like document files and document analysis like OCR to structured Linked Data like RDF graphs and with Semantic Web Tools like a triplestore and many other tools working with open standards for the Semantic Web . So it can not only be used for data analysis, data enrichment and merging but as a converter from more or less unstructured document files to linked data like PDF to RDF, Word to RDF, DOC to RDF, OCR to RDF, JPG to RDF, PNG to RDF or as flexible importer for Semantic Web infrastructure like triplestores, too. Or it can be seen as an Semantic Web-API for legacy files, documents or command line data analysis tools serving their data or analysis results to Semantic Web infrastructure. Another Framework for getting Semantic Web or Linked Data graphs with structured data from unstructured or legacy files are: * Linked Pipes ETL : RDF based open source framework for extract transform load (Java) * LDIF : linked data integration framework * Karma : Data integration tool * Silk : open source framework for integrating heterogeneous data sources * Apache Stanbol : Data enrichment framework reads text and serves a RDF graph Linked Data and Semantic Web Linked Data (LD) or Semantic Web (Web 3.0) and Linked Open Data (LOD) based on standard formats like RDF, JSON-LD or Turtle makes data integration, data enrichment and merging data sources easier. Parts of such databases or graphs can be queried and filtered via the query language SPARQL. The data is available in open standard linked data formats and can be imported to your favorite formats and accessable with your favorite tools, software or library. Merging multiple data sources to one graph database To merge data from different semantic web sources, just load their graphs (database with linked data) into the triplestore (database server) and match or map the properties (in traditional databases this would be columns or data fields). Your triple store or frameworks like for example OpenRDF bring some tools to load linked data in RDF or Turtle format into the triplestore. To download and import external graph databases or external linked data, you can use Semantic Web ETL frameworks: * Unified Views : ETL tool for RDF data Merging data fields of different data sources and IDs (Entity Mapping) If the different data sources don't use a common standard for data fields or IDs like Dublin Core or Wikidata : To merge, connect and map the data fields of different data sources, you can add another graph: an ontology for mapping/connecting different property names or IDs. With the property \"Same as\" you can map or merge different data sources to one graph. Tools and Methods Semantic Web: Entity Mapping with properties like Same As or linking different relations of concepts with Simple Knowledge Organisation System ( SKOS ) Record linkage (for example with Joins: Solr Join or SQL Join (if same keys or ids in both datasets) and/or Leavensthein distance for similarity Open Refine reconcile API Open Data sources: Data enrichment with Linked Open Data Since there is many open data in this open standard formats for linked data, you can enrich your data with many free knowledge bases like WikiData or DBPedia (the structured database of Wikipedia) , which is available via a SPARQL Endpoint. Other open source projects Tika : Content Analysis Toolkit: extracts content and meta data like author and from different document and file formats Apache ManifoldCF : Data integration framework reads different datasources and export to index Apache UIMA - Pipelines to get strucutred information out of unstructured documents or data DKPro Core : Provides UIMA components wrapping natural language processing (NLP) tools so they can be used interchangeably in UIMA processing pipelines Kite morphlines : ETL framework to import data to Solr or Hadoop Apache Flume : Framework for data pipes Kettle : ETL framework with GUI Talend Open Studio : ETL framework with GUI Apache NiFi : Automated and managed flow and transformation of information between systems DSWARM : Data management platform for enrichment, normalization and linkage of knowledge data structures Europeana Code repository : ETL tools for Europeana library Scrapy : Crawler and data extractor for extracting structured data from websites not providing an API for getting JSON or linked data like RDF but for example HTML without semantic annotations like RDFa MapReduceIndexerTool: Indexing many files to Solr faster using an Hadoop cluster (MapReduce is used for parallel processing on a cluster) Freme Project : Open framework for multilingual and semantic enrichment of digital content Methods Data integration Extract, transform, load (ETL) Optical character recognition (OCR) Information extraction Entity Linking Crawling Document processing Document automation Document modelling","title":"Open Semantic ETL toolkit for data integration, data analysis, document analysis, information extraction & data enrichment"},{"location":"etl/#open-semantic-etl-toolkit-for-data-integration-data-analysis-document-analysis-information-extraction-data-enrichment","text":"","title":"Open Semantic ETL toolkit for data integration, data analysis, document analysis, information extraction &amp; data enrichment"},{"location":"etl/#open-source-frameworks-for-data-integration-document-processing-information-extraction-data-analysis-merging-combining-data-content-enrichment-and-data-enrichment-pipelines","text":"Since most data is available in open standards or extractable by open source software libraries and free software, you can use different open source toolkits or frameworks to extract, transform and load (ETL) data into the search index. The preconfigured Open Semantic ETL is a Python based lightweight, flexible, extendable, modular and interoperable free software and open source ETL (extract, transform, load), content enrichment and data enrichment framework, toolkit or data enrichment management system for document processing, automated content analysis and media analysis, information extraction, merge and data enrichment pipelines managing multiple, different and modular import, extraction, content analysis, data combining or data enrichment plugins . Since the architecture of the search engine is modular, using open standards for Linked Data and Semantic Web like RDF or SKOS and the basis Elastic Search or Solr providing common standard APIs, you can use or integrate many other alternate Open Source ETL and data analysis tools instead or additionally.","title":"Open source frameworks for data integration, document processing, information extraction, data analysis, merging &amp; combining data, content enrichment and data enrichment pipelines"},{"location":"etl/#extract-structured-data-from-unstructured-documents-information-extraction-merge-and-enrich-data-with-multiple-other-data-sources-and-data-analysis-tools","text":"Modular data enrichment plugins (enhancer) extract structured data from even from unstructured documents or plain text and enhance or enrich the content with additional meta data or analytics.","title":"Extract structured data from unstructured documents (Information extraction), merge and enrich data with multiple other data sources and data analysis tools"},{"location":"etl/#document-processing-document-analysis-data-integration-content-analysis-and-data-enrichment-pipeline-enhancement-chain","text":"The document processing pipeline or chain is a list of data enrichment plugins (enhancer), which will be runned for each document to enrich, analyse or link them with additional data or analysis. A part of the default document processing pipeline configuration is for example: * Crawl a directory and its files and subdirectories * Filter blacklists (filter_blacklist) * Filter if file indexed yet and not modified since last indexing (filter_file_not_modified) * Extract text (enhance_text) * OCR images (enhance_ocr) * Adding annotations and tags (enhance_rdf) * Exporting or indexing to SQL or NoSQL database and/or search index (f.e. Apache Solr or Elastic Search or a Linked Data triplestore like Apache Jena Fuseki )","title":"Document processing, document analysis, data integration, content analysis and data enrichment pipeline (Enhancement chain)"},{"location":"etl/#semantic-data-enrichment-plugins","text":"Such modular data enrichment plugins (enhancer) will extract structured data from unstrucutred documents, enhance or enrich the content with additional meta data or analytics. For example the named entities extractor or OCR for image files .","title":"Semantic data enrichment plugins"},{"location":"etl/#configuration-of-a-custom-document-processing-content-analysis-and-data-enrichment-pipeline","text":"If you use only Open Semantic ETL you can use */etc/etl/config* to setup your data analysis and data enrichment chain and to set an db/exporter/writer where to store or index the results (for example Solr , Elastic Search , a triplestore or a database). If you use the preconfigurated full search engine Open Semantic Search the pipeline or the enabled plugins are configurated for all data sources in */etc/opensemanticsearch/etl* or can be overwritten or extended for each data source or connector in their specialized configs like *connector-files* . The analysis chain runs in order, since some plugins depend on data analysis of other plugins. You can add additional or new data enrichment plugins to config['plugins'] . Or you overwrite this config option to define a custom data enrichment pipeline with only a few needed plugins. You can overwrite the config by parameters of the command line tools, for example to use a custom config file or to set the plugin for your data analysis and data enrichment chain. So you can set a custom config file on the command line with the parameter --config , for example: etl-file --config */etc/etl/MyCustomConfig* *filename*","title":"Configuration of a custom document processing, content analysis and data enrichment pipeline"},{"location":"etl/#usage-extract-analyze-and-enrich","text":"Extract, analyze and enrich and export data from files or webpages: etl-file *filename* to import/extract/analyze/enrich a file or etl-file *directory* to import/extract/analyze/enrich all files of a directory and its sub directories or etl-file-monitoring *directory or file* for monitoring a directory and import/extract/analyze/enrich new or changed files or etl-web *uri* to download a file or webpage from the web and extract, analyze and enrich it","title":"Usage: Extract, analyze and enrich"},{"location":"etl/#enrich-parts-enrich-later-add-additional-enrichments-update-data-enrichments-or-distributed-data-enrichment","text":"The tool etl-enrich can run data enrichment parts or plugins which are not enabled in your default document processing pipe later or from time to time. For example sometimes its better to index all documents without OCR in short time and after that to do the OCR of the documents with images which will need long time. So the users are able to search in most documents and text, not having to wait until only few parts and only for a few documents like some text in images are recognized in a long time process first before other documents after them were indexed, which takes only very few time, because there are no images. Or you can do expensive data enrichment like OCR at night or on low server load or distribute this work on different processors (parallel processing) or servers (cluster) or web services (cloud). Another possibility is to enrich with tools or webservices that imporoved or updated their results because of better analytics quality or more available data from time to time to integrate newer data or analytcs results. Or to enrich later with a additional webservice, without to have to run the full document processing chain again. Or if a webservice was not available while indexing to enrich data with its analytics later.","title":"Enrich parts, enrich later, add additional enrichments, update data enrichments or distributed data enrichment"},{"location":"etl/#run-additional-data-analysis-or-data-enrichment-plugins","text":"You can run the tool from REST-API or on the command line: etl-enrich --plugins *pluginname* Optional you can add a search or filter query, so only the interesting or important data or document(s) will get enriched: etl-enrich --plugins *pluginname* --query *query*","title":"Run additional data analysis or data enrichment plugins"},{"location":"etl/#enrichment-with-results-of-webservices-and-apis","text":"You can analyze your data with internal webservices (or if you dont need privacy with external webservices or \"the cloud\") and read the results with the standard RDF enhancer plugin. If the webservice results are not in standard Semantic Web formats, but only an API, you can call the API and read the results in your favorite programming language in a custom data enrichment plugin:","title":"Enrichment with results of webservices and APIs"},{"location":"etl/#development-of-own-data-enrichment-plugins","text":"You can develop own data enrichment plugins with few lines of code using the data enrichment interfaces for Python plugins, Javascript plugins or Java Plugins. Or you develop or use a webservice in your favorite programming language and read its results with the standard RDF enhancer plugin. Learn more about development of data enrichment plugins ...","title":"Development of own data enrichment plugins"},{"location":"etl/#scale-for-big-data-analysis","text":"If there is big data and you have to index faster, you can scale by parallel processing, search clusters and optimizing RAM settings .","title":"Scale for big data analysis"},{"location":"etl/#other-frameworks-for-data-integration-for-data-warehouse-or-for-extraction-transformation-and-load-etl-or-data-enrichment","text":"There are powerful open source ETL frameworks (extraction, transformation and load) for data integration, mapping, filtering and transformation for data warehousing with powerful features and graphical user interfaces (GUI). If there is an output plugin for Solr or if the tools can export data in a format which can be imported by one of our connectors (crawler, extraction or input plugin) , you can use this open source frameworks like Kettle or Talend Open Studio to integrate, extract, transform, enrich and load (index) data to the search engine.","title":"Other frameworks for data integration for data warehouse or for extraction, transformation and load (ETL) or data enrichment"},{"location":"etl/#other-data-enrichment-frameworks-based-on-semantic-web-standards","text":"You can use the Apache Stanbol data enrichment framework and one or more of its many enhancer engines .","title":"Other data enrichment frameworks based on Semantic Web standards"},{"location":"etl/#transform-unstructured-documents-to-semantic-linked-data","text":"There are not only output/storage/index writer/exporter plugins for search engines like Solr or Elastic Search, but for triplestores, too: So with its triplestore storage or output plugins for RDF export it can integrate unstructured data like document files and document analysis like OCR to structured Linked Data like RDF graphs and with Semantic Web Tools like a triplestore and many other tools working with open standards for the Semantic Web . So it can not only be used for data analysis, data enrichment and merging but as a converter from more or less unstructured document files to linked data like PDF to RDF, Word to RDF, DOC to RDF, OCR to RDF, JPG to RDF, PNG to RDF or as flexible importer for Semantic Web infrastructure like triplestores, too. Or it can be seen as an Semantic Web-API for legacy files, documents or command line data analysis tools serving their data or analysis results to Semantic Web infrastructure. Another Framework for getting Semantic Web or Linked Data graphs with structured data from unstructured or legacy files are: * Linked Pipes ETL : RDF based open source framework for extract transform load (Java) * LDIF : linked data integration framework * Karma : Data integration tool * Silk : open source framework for integrating heterogeneous data sources * Apache Stanbol : Data enrichment framework reads text and serves a RDF graph","title":"Transform unstructured documents to semantic linked data"},{"location":"etl/#linked-data-and-semantic-web","text":"Linked Data (LD) or Semantic Web (Web 3.0) and Linked Open Data (LOD) based on standard formats like RDF, JSON-LD or Turtle makes data integration, data enrichment and merging data sources easier. Parts of such databases or graphs can be queried and filtered via the query language SPARQL. The data is available in open standard linked data formats and can be imported to your favorite formats and accessable with your favorite tools, software or library.","title":"Linked Data and Semantic Web"},{"location":"etl/#merging-multiple-data-sources-to-one-graph-database","text":"To merge data from different semantic web sources, just load their graphs (database with linked data) into the triplestore (database server) and match or map the properties (in traditional databases this would be columns or data fields). Your triple store or frameworks like for example OpenRDF bring some tools to load linked data in RDF or Turtle format into the triplestore. To download and import external graph databases or external linked data, you can use Semantic Web ETL frameworks: * Unified Views : ETL tool for RDF data","title":"Merging multiple data sources to one graph database"},{"location":"etl/#merging-data-fields-of-different-data-sources-and-ids-entity-mapping","text":"If the different data sources don't use a common standard for data fields or IDs like Dublin Core or Wikidata : To merge, connect and map the data fields of different data sources, you can add another graph: an ontology for mapping/connecting different property names or IDs. With the property \"Same as\" you can map or merge different data sources to one graph.","title":"Merging data fields of different data sources and IDs (Entity Mapping)"},{"location":"etl/#tools-and-methods","text":"Semantic Web: Entity Mapping with properties like Same As or linking different relations of concepts with Simple Knowledge Organisation System ( SKOS ) Record linkage (for example with Joins: Solr Join or SQL Join (if same keys or ids in both datasets) and/or Leavensthein distance for similarity Open Refine reconcile API","title":"Tools and Methods"},{"location":"etl/#open-data-sources-data-enrichment-with-linked-open-data","text":"Since there is many open data in this open standard formats for linked data, you can enrich your data with many free knowledge bases like WikiData or DBPedia (the structured database of Wikipedia) , which is available via a SPARQL Endpoint.","title":"Open Data sources: Data enrichment with Linked Open Data"},{"location":"etl/#other-open-source-projects","text":"Tika : Content Analysis Toolkit: extracts content and meta data like author and from different document and file formats Apache ManifoldCF : Data integration framework reads different datasources and export to index Apache UIMA - Pipelines to get strucutred information out of unstructured documents or data DKPro Core : Provides UIMA components wrapping natural language processing (NLP) tools so they can be used interchangeably in UIMA processing pipelines Kite morphlines : ETL framework to import data to Solr or Hadoop Apache Flume : Framework for data pipes Kettle : ETL framework with GUI Talend Open Studio : ETL framework with GUI Apache NiFi : Automated and managed flow and transformation of information between systems DSWARM : Data management platform for enrichment, normalization and linkage of knowledge data structures Europeana Code repository : ETL tools for Europeana library Scrapy : Crawler and data extractor for extracting structured data from websites not providing an API for getting JSON or linked data like RDF but for example HTML without semantic annotations like RDFa MapReduceIndexerTool: Indexing many files to Solr faster using an Hadoop cluster (MapReduce is used for parallel processing on a cluster) Freme Project : Open framework for multilingual and semantic enrichment of digital content","title":"Other open source projects"},{"location":"etl/#methods","text":"Data integration Extract, transform, load (ETL) Optical character recognition (OCR) Information extraction Entity Linking Crawling Document processing Document automation Document modelling","title":"Methods"},{"location":"etl/elasticsearch/","text":"Index legacy documents, files and directories & file shares to Elastic Search How to index document files like Word documents or PDF files, file directories and file shares to Elastic Search with open source tools? You can import files, documents, directories and file shares to Elastic Search index with the Open Semantic ETL framework and its files connector (all components are free software & open source): Example Download and install Open Semantic ETL Elastic Search edition package Index a directory with document files: opensemanticsearch-index-file */home/user/Documents* Search the content of the files with Elastic Search or Kibana. Installation Install a preconfigured Open Semantic ETL package (Please donate: or Open Semantic Desktop Search and Open Semantic Search appliance) with the flavour \"elastic search\"-edition. or install Open Semantic ETL and set in the config file /etc/etl/config the option \"export\" to \"elasticsearch\" . Data enrichment and data analysis Documents are analyzed and enriched by enhancer plugins. For example image files or images inside PDF documents are enriched by automatic textrecognition (OCR), so it is possible to find text even in graphic format based PDFs like scans or photographed documents. Config language for OCR For better OCR results you can configure another language (default language: english). Alternate methods to start indexing files To import / index directories, documents and files you can use one of our standard tools and system integrations: * Just add to shared folder to the Open Semantic Search Virtual Machine like Open Semantic Desktop Search or Open Semantic Search Appliance * Command line tools (so you can integrate with own scripts or standard Linux tools like Cron) * Plugins for file managers like Nautilus or the Windows Filemanager * Firefox Plugins * Web API (REST API) * File monitoring tools Search user interface After document processing and indexing you can use the Open Source user interface Kibana for full text search in your files and documents. Performance We've got a first donation to improve performance, so only few donations are neccessary so one of the next versions will be able to index more documents in less time. More about how to increase performance soon. Import CSV files Standard CSV tables are indexed automatically, soon. Please donate so we can release the CSV Manager UI for Elastic Search sooner, so you can import very big CSV files in a more structured form and import non-standard CSV files easier. Elastic Search based Desktop Search A very easy to install and fully preconfigured Open Semantic Desktop Search Elastic Search and Kibana flavoured Virtual Machine is planned. Please donate to realize this earlier. Named entities extraction Please donate so we can release the Thesaurus for Named Entity Extraction for Elastic Search sooner.","title":"Index legacy documents, files and directories & file shares to Elastic Search"},{"location":"etl/elasticsearch/#index-legacy-documents-files-and-directories-file-shares-to-elastic-search","text":"","title":"Index legacy documents, files and directories &amp; file shares to Elastic Search"},{"location":"etl/elasticsearch/#how-to-index-document-files-like-word-documents-or-pdf-files-file-directories-and-file-shares-to-elastic-search-with-open-source-tools","text":"You can import files, documents, directories and file shares to Elastic Search index with the Open Semantic ETL framework and its files connector (all components are free software & open source):","title":"How to index document files like Word documents or PDF files, file directories and file shares to Elastic Search with open source tools?"},{"location":"etl/elasticsearch/#example","text":"Download and install Open Semantic ETL Elastic Search edition package Index a directory with document files: opensemanticsearch-index-file */home/user/Documents* Search the content of the files with Elastic Search or Kibana.","title":"Example"},{"location":"etl/elasticsearch/#installation","text":"Install a preconfigured Open Semantic ETL package (Please donate: or Open Semantic Desktop Search and Open Semantic Search appliance) with the flavour \"elastic search\"-edition. or install Open Semantic ETL and set in the config file /etc/etl/config the option \"export\" to \"elasticsearch\" .","title":"Installation"},{"location":"etl/elasticsearch/#data-enrichment-and-data-analysis","text":"Documents are analyzed and enriched by enhancer plugins. For example image files or images inside PDF documents are enriched by automatic textrecognition (OCR), so it is possible to find text even in graphic format based PDFs like scans or photographed documents.","title":"Data enrichment and data analysis"},{"location":"etl/elasticsearch/#config-language-for-ocr","text":"For better OCR results you can configure another language (default language: english).","title":"Config language for OCR"},{"location":"etl/elasticsearch/#alternate-methods-to-start-indexing-files","text":"To import / index directories, documents and files you can use one of our standard tools and system integrations: * Just add to shared folder to the Open Semantic Search Virtual Machine like Open Semantic Desktop Search or Open Semantic Search Appliance * Command line tools (so you can integrate with own scripts or standard Linux tools like Cron) * Plugins for file managers like Nautilus or the Windows Filemanager * Firefox Plugins * Web API (REST API) * File monitoring tools","title":"Alternate methods to start indexing files"},{"location":"etl/elasticsearch/#search-user-interface","text":"After document processing and indexing you can use the Open Source user interface Kibana for full text search in your files and documents.","title":"Search user interface"},{"location":"etl/elasticsearch/#performance","text":"We've got a first donation to improve performance, so only few donations are neccessary so one of the next versions will be able to index more documents in less time. More about how to increase performance soon.","title":"Performance"},{"location":"etl/elasticsearch/#import-csv-files","text":"Standard CSV tables are indexed automatically, soon. Please donate so we can release the CSV Manager UI for Elastic Search sooner, so you can import very big CSV files in a more structured form and import non-standard CSV files easier.","title":"Import CSV files"},{"location":"etl/elasticsearch/#elastic-search-based-desktop-search","text":"A very easy to install and fully preconfigured Open Semantic Desktop Search Elastic Search and Kibana flavoured Virtual Machine is planned. Please donate to realize this earlier.","title":"Elastic Search based Desktop Search"},{"location":"etl/elasticsearch/#named-entities-extraction","text":"Please donate so we can release the Thesaurus for Named Entity Extraction for Elastic Search sooner.","title":"Named entities extraction"},{"location":"etl/export/","text":"title: Open Semantic ETL: Data Export authors: - Markus Mandalka Open Semantic ETL: Data Export Output plugins store or export results of the data extraction and data analysis chain to different formats or databases. For example to a search index like Solr or Elastic Search or to a triple store , a database or a file.","title":"Index"},{"location":"etl/export/#open-semantic-etl-data-export","text":"Output plugins store or export results of the data extraction and data analysis chain to different formats or databases. For example to a search index like Solr or Elastic Search or to a triple store , a database or a file.","title":"Open Semantic ETL: Data Export"},{"location":"etl/export/csv/","text":"CSV Export Write data to a CSV spreadsheet, so you can import the data with other software or with Open Office Calc, Libreoffice Calc or Excel. If you use output plugins to index you data to a search index like Solr or Elastic Search , you can read and filter the data with their powerful search APIs. Just set the Solr result writer to CSV format by the wt paramater: localhost:8983/solr/core1/select?q=*:*&wt=csv","title":"CSV Export"},{"location":"etl/export/csv/#csv-export","text":"Write data to a CSV spreadsheet, so you can import the data with other software or with Open Office Calc, Libreoffice Calc or Excel. If you use output plugins to index you data to a search index like Solr or Elastic Search , you can read and filter the data with their powerful search APIs. Just set the Solr result writer to CSV format by the wt paramater: localhost:8983/solr/core1/select?q=*:*&wt=csv","title":"CSV Export"},{"location":"etl/export/elasticsearch/","text":"ETL to Elastic Search index This plugin index extracted and enriched data to Elastic Search . To configure Open Semantic ETL or a Open Semantic Search connector config to index to Elastic Search, overwrite the output plugin settings in /etc/opensemanticsearch/etl : config['export'] = 'elasticsearch' Optionally you can set another Elastic Search indexname (default: opensemanticsearch ). config['index'] = 'anotherIndex'","title":"ETL to Elastic Search index"},{"location":"etl/export/elasticsearch/#etl-to-elastic-search-index","text":"This plugin index extracted and enriched data to Elastic Search . To configure Open Semantic ETL or a Open Semantic Search connector config to index to Elastic Search, overwrite the output plugin settings in /etc/opensemanticsearch/etl : config['export'] = 'elasticsearch' Optionally you can set another Elastic Search indexname (default: opensemanticsearch ). config['index'] = 'anotherIndex'","title":"ETL to Elastic Search index"},{"location":"etl/export/json/","text":"title: Open Semantic ETL: Export results to JSON authors: - Markus Mandalka Open Semantic ETL: Export results to JSON This is one of the output plugins for standalone installations of Open Semantic ETL without a full search engine, for example for some data analysis, enrichment or jobs saved to an output file or to a(nother) (maybe light-weight) database. So if you set config['export'] = 'export_json' so the output of etl-file *filename* or etl-web *url* will be exported as JSON. To write the JSON output to a file, set the argument --outputfile , for example etl-file --outputfile *outputfilename* *inputfilename* or etl-web --outputfile *outputfilename* *uri* API with JSON results If you use output plugins to write your data to a search index like Solr or Elastic Search , you can read and filter it with their more powerful search APIs: For example REST-API of Solr for searching or / and getting data in XML or JSON format or use Solr Client APIs to get data with your favorite programming language.","title":"Index"},{"location":"etl/export/json/#open-semantic-etl-export-results-to-json","text":"This is one of the output plugins for standalone installations of Open Semantic ETL without a full search engine, for example for some data analysis, enrichment or jobs saved to an output file or to a(nother) (maybe light-weight) database. So if you set config['export'] = 'export_json' so the output of etl-file *filename* or etl-web *url* will be exported as JSON. To write the JSON output to a file, set the argument --outputfile , for example etl-file --outputfile *outputfilename* *inputfilename* or etl-web --outputfile *outputfilename* *uri*","title":"Open Semantic ETL: Export results to JSON"},{"location":"etl/export/json/#api-with-json-results","text":"If you use output plugins to write your data to a search index like Solr or Elastic Search , you can read and filter it with their more powerful search APIs: For example REST-API of Solr for searching or / and getting data in XML or JSON format or use Solr Client APIs to get data with your favorite programming language.","title":"API with JSON results"},{"location":"etl/export/rdf/","text":"Export extracted and enriched data to a graph or triplestore (RDF) Export data, data enrichments and analysis results to a graph or triplestore Exports data, data enrichments or data analysis results as linked data in Resource Description Format (RDF or Turtle/NT) or to a triplestore (graph database) like Apache Jena Fuseki , Sesame, RDF4J or OpenRDF . So you can use many other tools or share the data using open standards for Linked Data and Semantic Web . Mappings RDF properties to fields or facets If you use connector and data enrichment plugins that don't work with semantic web formats, you can map legacy fields or facets to RDF properties. The mapping is configured in an graph, for example RDF or a turtle (NT) file. More soon ...","title":"Export extracted and enriched data to a graph or triplestore (RDF)"},{"location":"etl/export/rdf/#export-extracted-and-enriched-data-to-a-graph-or-triplestore-rdf","text":"","title":"Export extracted and enriched data to a graph or triplestore (RDF)"},{"location":"etl/export/rdf/#export-data-data-enrichments-and-analysis-results-to-a-graph-or-triplestore","text":"Exports data, data enrichments or data analysis results as linked data in Resource Description Format (RDF or Turtle/NT) or to a triplestore (graph database) like Apache Jena Fuseki , Sesame, RDF4J or OpenRDF . So you can use many other tools or share the data using open standards for Linked Data and Semantic Web .","title":"Export data, data enrichments and analysis results to a graph or triplestore"},{"location":"etl/export/rdf/#mappings-rdf-properties-to-fields-or-facets","text":"If you use connector and data enrichment plugins that don't work with semantic web formats, you can map legacy fields or facets to RDF properties. The mapping is configured in an graph, for example RDF or a turtle (NT) file. More soon ...","title":"Mappings RDF properties to fields or facets"},{"location":"etl/export/solr/","text":"ETL to Solr search index ETL exporter configuration To export the results of document extraction, data analyis and data enrichment to a Solr search server (preconfigurated default, if you use the Open Semantic Search packages), set the output Plugin to export_solr : config['export'] = 'export_solr' Solr server configuration If not default http://localhost:8983/solr/ configure config['solr'] to your Solr server, for example: config['solr'] = 'http://localhost:8983/solr/' Solr core configuration config['index'] = 'core1'","title":"ETL to Solr search index"},{"location":"etl/export/solr/#etl-to-solr-search-index","text":"","title":"ETL to Solr search index"},{"location":"etl/export/solr/#etl-exporter-configuration","text":"To export the results of document extraction, data analyis and data enrichment to a Solr search server (preconfigurated default, if you use the Open Semantic Search packages), set the output Plugin to export_solr : config['export'] = 'export_solr'","title":"ETL exporter configuration"},{"location":"etl/export/solr/#solr-server-configuration","text":"If not default http://localhost:8983/solr/ configure config['solr'] to your Solr server, for example: config['solr'] = 'http://localhost:8983/solr/'","title":"Solr server configuration"},{"location":"etl/export/solr/#solr-core-configuration","text":"config['index'] = 'core1'","title":"Solr core configuration"},{"location":"etl/export/solr2rdf/","text":"Solr2RDF Export Solr fields to RDF graph (Linked Data and Semantic Web) This Solr reads data from Solr fields/facets and convert it to a RDF graph so you can import the data to a standard triplestore for Linked Data and Semantic Web.","title":"Solr2RDF"},{"location":"etl/export/solr2rdf/#solr2rdf","text":"","title":"Solr2RDF"},{"location":"etl/export/solr2rdf/#export-solr-fields-to-rdf-graph-linked-data-and-semantic-web","text":"This Solr reads data from Solr fields/facets and convert it to a RDF graph so you can import the data to a standard triplestore for Linked Data and Semantic Web.","title":"Export Solr fields to RDF graph (Linked Data and Semantic Web)"},{"location":"graph-explorer/","text":"Open Semantic Visual Graph Explorer for Discovery and Visualization of Linked Data Open Source tool and user interface (UI) for discovery, exploration and visualization of a graph The open source tool Open Semantic Visual Linked Data Knowledge Graph Explorer is a web app providing user interfaces (UI) to discover, explore and visualize linked data in a graph for visualization and exploration of direct and indirect connections between entities like people, organizations and locations in your Linked Data Knowledge Graph (for example extracted from your documents by Open Semantic Search or Open Semantic ETL using Named Entity Extraction and Named Entity Recognition). Visualize relations of entities like persons or organizations within and across documents (co-occurrences of named entities) The network analysis / graph visualization shows you the relations, connections and networks between named entities like persons, organizations or main concepts which occur together (co-occurrences) in your content, datasources and documents. Usage of the Graph User Interface Read more about the features, graph user interfaces and how to use them in the documentation . Free software (Open Source) The source code of the free software is available in the Git repository open-semantic-visual-graph-explorer . Integrates Python Django, Apache Solr and Cytoscape.js The Django web app for discovery, exploration and visualization of a graph integrates a Neo4j graph database (planed) with documents in a Apache Solr search index with the Cytoscape.js graph visualization framework. Dependencies If you do not want to use the preconfigured Debian or Ubuntu packages, you have to setup the following dependencies: Python 3 (https://www.python.org/) Django (https://www.djangoproject.com/) cytoscape.js (Git: https://github.com/cytoscape/cytoscape.js) cytoscape.js-panzoom (Git: https://github.com/cytoscape/cytoscape.js-panzoom) Foundation (https://foundation.zurb.com/) Optional dependencies / integration Optional dependencies (at least one of them needed) for integrated graph database(s) or faceted search index where your knowledge graph, entities, connections and/or documents are stored: Apache Solr (https://lucene.apache.org/solr/) Planned: Neo4j (https://neo4j.com) Planned: SPARQL triplestore like Apache Jena (https://jena.apache.org/)","title":"Open Semantic Visual Graph Explorer for Discovery and Visualization of Linked Data"},{"location":"graph-explorer/#open-semantic-visual-graph-explorer-for-discovery-and-visualization-of-linked-data","text":"","title":"Open Semantic Visual Graph Explorer for Discovery and Visualization of Linked Data"},{"location":"graph-explorer/#open-source-tool-and-user-interface-ui-for-discovery-exploration-and-visualization-of-a-graph","text":"The open source tool Open Semantic Visual Linked Data Knowledge Graph Explorer is a web app providing user interfaces (UI) to discover, explore and visualize linked data in a graph for visualization and exploration of direct and indirect connections between entities like people, organizations and locations in your Linked Data Knowledge Graph (for example extracted from your documents by Open Semantic Search or Open Semantic ETL using Named Entity Extraction and Named Entity Recognition).","title":"Open Source tool and user interface (UI) for discovery, exploration and visualization of a graph"},{"location":"graph-explorer/#visualize-relations-of-entities-like-persons-or-organizations-within-and-across-documents-co-occurrences-of-named-entities","text":"The network analysis / graph visualization shows you the relations, connections and networks between named entities like persons, organizations or main concepts which occur together (co-occurrences) in your content, datasources and documents.","title":"Visualize relations of entities like persons or organizations within and across documents (co-occurrences of named entities)"},{"location":"graph-explorer/#usage-of-the-graph-user-interface","text":"Read more about the features, graph user interfaces and how to use them in the documentation .","title":"Usage of the Graph User Interface"},{"location":"graph-explorer/#free-software-open-source","text":"The source code of the free software is available in the Git repository open-semantic-visual-graph-explorer .","title":"Free software (Open Source)"},{"location":"graph-explorer/#integrates-python-django-apache-solr-and-cytoscapejs","text":"The Django web app for discovery, exploration and visualization of a graph integrates a Neo4j graph database (planed) with documents in a Apache Solr search index with the Cytoscape.js graph visualization framework.","title":"Integrates Python Django, Apache Solr and Cytoscape.js"},{"location":"graph-explorer/#dependencies","text":"If you do not want to use the preconfigured Debian or Ubuntu packages, you have to setup the following dependencies: Python 3 (https://www.python.org/) Django (https://www.djangoproject.com/) cytoscape.js (Git: https://github.com/cytoscape/cytoscape.js) cytoscape.js-panzoom (Git: https://github.com/cytoscape/cytoscape.js-panzoom) Foundation (https://foundation.zurb.com/)","title":"Dependencies"},{"location":"graph-explorer/#optional-dependencies-integration","text":"Optional dependencies (at least one of them needed) for integrated graph database(s) or faceted search index where your knowledge graph, entities, connections and/or documents are stored: Apache Solr (https://lucene.apache.org/solr/) Planned: Neo4j (https://neo4j.com) Planned: SPARQL triplestore like Apache Jena (https://jena.apache.org/)","title":"Optional dependencies / integration"},{"location":"investigative_journalism/howto-analyze-leaks/","text":"How investigative journalists can start to investigate, analyze and explore massive leaks of large document collections with many files much faster A (yet quick and very dirty and work in progress for coming workshop) tutorial on new UIs (more easier to use UIs planned, so it will be extended and imports of entities will get easier next weeks and months) for faster picking of low hanging fruits in big data leak analysis with limited computing resources: After an overview of useful and powerful data and text analysis methods (which need long document processing time on initial import, if very large amount of files ) it describes some methods, config options and new user interfaces for investigative journalists which empower investigative journalism to pick low hanging fruits of huge leaks faster and start to investigate and analyze a big leak much earlier . Free Open Source search tools for leak analysis, exploration and discovery for investigative reporting There are many powerful and useful research features by free software for information retrieval and integrated open source tools for data analysis to explore and analyze the contents of leaks and document collections by faceted search and overviews, thesaurus, ontologies, lists of named entities (batch search) for automatic entity extraction of known entities and other domain knowledge like important concepts, by machine learning (ML) tools (\"artificial intelligence\" AI) for natural language processing (NLP) like automatic named entity recognition (NER) of yet unknown entities, fuzzy search and other powerful Text mining features which are useful in investigative journalism: Full text search The full text search user interface allows you to search the index with powerful search operators i.e. for germany OR german* OR deu OR Berlin OR deutsch* OR \"Steffan Mappus\" or some other names. Privacy and data protection Analysis, indexing and search runs on your own computer without using cloud services, so what you search for will not be seen, stored and spied by default like on external cloud services. Manage named entities like companies or politicians You can add names (for example companies or politicians) to the Thesaurus . Faceted search (Overview and interactive filters of entities) This names and concepts in thesaurus, ontologies or lists of names (which investigators with domain knowledge know best and which are often very relevant but an automatic named entity recognition by machine learning fails to recognize them) and/or Named entity recognition by machine learning enables an aggregated overview for this Named Entities like people, organizations or places: Extracted people and organizations Extracted email addresses, email domains Extracted locations and phone numbers Extracted lega codes and law clauses The screenshot shows you how you can use this overviews or how to use named entities as interactive filter to narrow down search results. So a click to a facet (i.e. an organization) will drill down the search results to fewer documents, matching this additional facet/filter, too. How to get this structured facets / overviews and leads by using watchlists and Open Data But you dont have to add each potential name yourself to add some structure or watchlists like for example names of important people or politicians. Import Open Data to the Lists and Ontologies Manager List of names of people of interest like politicians Get a list of names of politians from your country for example from Wikipedias structured database Wikidata. So you get another search facet and overview about this potentialy relevant Names occuring in the documents. Open Data list of location names Another option is to filter by town names of your country: Get a list of town names of your country, for example from GeoNames , from Openstreetmap or from Wikidata. So you get another search facet and overview about \"Locations\" occurring in the documents, which are in your country. So you can filter for towns in your country and see which people occurre within the documents which maybe are yet unknown and not in your lists of persons but very interesting for the searched context (i.e. your country). Fuzzy search No human writer, no OCR software and no machine learning model is perfect. There are typos in documents, wrong recognized chars by OCR and occurring but not recognized names by Named Entity Recognition by machine learning (its biased statistics not artificial intelligence magic). So consider to fuzzy search for similar name variants: As intelligent human considering more cases and conplexity of languages than a dumb computer trying natural language processing by powerful fuzzy search operators or by following UI maybe finding some (but often not as many as a creative human knowing limitations of automatic fuzzy search) useful variants easier: Big Data Leaks with large amounts of files and text often need long processing time Huge leaks can be large document sets with thousands, hundred thousands or millions of document files. Today it is mostly no problem to search in even such large amounts of documents with modern Open Source research tools even on older hardware like used by many journalists, if the documents are indexed in a open source search engine. Adding and indexing new documents over time is no problem, too, since only new and changed documents will be indexed on a recrawl of your file directories. But the initial text extraction, OCR and indexing such large document collections with large amounts of files to a search engine can take days or weeks (a former project to index a large document collection for collaborative investigative research took even weeks on modern hardware). But you don't have to wait such a long time until you can start with investigations in such big leaks: Lack of IT admins? More user interfaces (UI) for investigative journalists Most of the below described features and methods to pick low-hanging fruits very fast had been develped years ago by a powerful modular, flexible and extendable Python based plugin architecture, but they needed Linux admin knowledge on Unix command line tools, Extract Transform Load (ETL) pipelines and related command line tools, most investigative journalists do not have. Like the last releases, the coming Open Source release in March (testing and improving stuff which yet in public Git repository) will provide additional web user interfaces and new default settings, so some features can be easier used even by freelance journalists or members of a team, if a specialized datajournalist or admin is busy or temporary not available: Get low-hanging fruits very fast Many relevant information can be low-hanging fruits from a computing power perspective. So Open Semantic Search can run the extraction and analysis of documents in multiple stages: Very fast indexing of filenames first Before the plain text will be extracted from documents, the path names and file names of all documents will be indexed first. So you can search very early all the path names and filenames while standard text extraction from your documents is yet running. So you have a faceted search / interactive filters for file paths and file extensions like .pdf for PDF documents or XLS for Excel sheets. If you on this base find interesting paths or documents, you can prioritize files for running text extraction to enable full text search of the content. And even in this early stage you may have faceted search for some of the keywords or names from your domain knowledge in thesaurus or lists of names if they occur in this filenames (dependent on file naming). Named Entity Recognition (NER) by machine learning (ML) later Useful Named Entity Recognition of entities (works often even if not yet in your lists of names) by machine learning later needs up to ten times longer than entity extraction by your thesaurus or ontologies. So this step can be done later after text extraction, too while you are able to fulltext search all your documents earlier and maybe yet work with faster extracted entities from your domain knowledge by thesaurus or ontology based entity extraction. So set the ML NER plugin to be processed later (next release will have an UI config option \"Do NER by ML later\" in Config / Named Entity Recognition) Slow OCR latest For most document sets OCR as the slowest task takes most time for mostly only few additional infos. So the very slow OCR tasks will run after all plain text was extracted from all documents and so you can find most texts (which are stored in text formats in the document files) by full text search much earlier. This is the new default setting in Config / OCR: Prioritize text extraction and OCR tasks With this first analysis results or even only the path names and filenames you can often early identify potentially interesting files or directories for which you want to prioritize extraction and analysis. There you can click on \"Prioritize\" to open an UI where you can set the priority of certain files: Share load on multiple computers: Additional parallel ETL workers Maybe your team have an admin and more computers and servers? If one computer is slow, for more performance on the initial data import and analysis you can scale by adding additional computers or servers running a Open Semantic ETL Docker container which runs additional parallel workers to share the heavy analysis load between multiple computers. ]","title":"Investigate massive leaks for investigative reporting"},{"location":"investigative_journalism/howto-analyze-leaks/#how-investigative-journalists-can-start-to-investigate-analyze-and-explore-massive-leaks-of-large-document-collections-with-many-files-much-faster","text":"A (yet quick and very dirty and work in progress for coming workshop) tutorial on new UIs (more easier to use UIs planned, so it will be extended and imports of entities will get easier next weeks and months) for faster picking of low hanging fruits in big data leak analysis with limited computing resources: After an overview of useful and powerful data and text analysis methods (which need long document processing time on initial import, if very large amount of files ) it describes some methods, config options and new user interfaces for investigative journalists which empower investigative journalism to pick low hanging fruits of huge leaks faster and start to investigate and analyze a big leak much earlier .","title":"How investigative journalists can start to investigate, analyze and explore massive leaks of large document collections with many files much faster"},{"location":"investigative_journalism/howto-analyze-leaks/#free-open-source-search-tools-for-leak-analysis-exploration-and-discovery-for-investigative-reporting","text":"There are many powerful and useful research features by free software for information retrieval and integrated open source tools for data analysis to explore and analyze the contents of leaks and document collections by faceted search and overviews, thesaurus, ontologies, lists of named entities (batch search) for automatic entity extraction of known entities and other domain knowledge like important concepts, by machine learning (ML) tools (\"artificial intelligence\" AI) for natural language processing (NLP) like automatic named entity recognition (NER) of yet unknown entities, fuzzy search and other powerful Text mining features which are useful in investigative journalism:","title":"Free Open Source search tools for leak analysis, exploration and discovery for investigative reporting"},{"location":"investigative_journalism/howto-analyze-leaks/#full-text-search","text":"The full text search user interface allows you to search the index with powerful search operators i.e. for germany OR german* OR deu OR Berlin OR deutsch* OR \"Steffan Mappus\" or some other names.","title":"Full text search"},{"location":"investigative_journalism/howto-analyze-leaks/#privacy-and-data-protection","text":"Analysis, indexing and search runs on your own computer without using cloud services, so what you search for will not be seen, stored and spied by default like on external cloud services.","title":"Privacy and data protection"},{"location":"investigative_journalism/howto-analyze-leaks/#manage-named-entities-like-companies-or-politicians","text":"You can add names (for example companies or politicians) to the Thesaurus .","title":"Manage named entities like companies or politicians"},{"location":"investigative_journalism/howto-analyze-leaks/#faceted-search-overview-and-interactive-filters-of-entities","text":"This names and concepts in thesaurus, ontologies or lists of names (which investigators with domain knowledge know best and which are often very relevant but an automatic named entity recognition by machine learning fails to recognize them) and/or Named entity recognition by machine learning enables an aggregated overview for this Named Entities like people, organizations or places:","title":"Faceted search (Overview and interactive filters of entities)"},{"location":"investigative_journalism/howto-analyze-leaks/#extracted-people-and-organizations","text":"","title":"Extracted people and organizations"},{"location":"investigative_journalism/howto-analyze-leaks/#extracted-email-addresses-email-domains","text":"","title":"Extracted email addresses, email domains"},{"location":"investigative_journalism/howto-analyze-leaks/#extracted-locations-and-phone-numbers","text":"","title":"Extracted locations and phone numbers"},{"location":"investigative_journalism/howto-analyze-leaks/#extracted-lega-codes-and-law-clauses","text":"The screenshot shows you how you can use this overviews or how to use named entities as interactive filter to narrow down search results. So a click to a facet (i.e. an organization) will drill down the search results to fewer documents, matching this additional facet/filter, too.","title":"Extracted lega codes and law clauses"},{"location":"investigative_journalism/howto-analyze-leaks/#how-to-get-this-structured-facets-overviews-and-leads-by-using-watchlists-and-open-data","text":"But you dont have to add each potential name yourself to add some structure or watchlists like for example names of important people or politicians. Import Open Data to the Lists and Ontologies Manager","title":"How to get this structured facets / overviews and leads by using watchlists and Open Data"},{"location":"investigative_journalism/howto-analyze-leaks/#list-of-names-of-people-of-interest-like-politicians","text":"Get a list of names of politians from your country for example from Wikipedias structured database Wikidata. So you get another search facet and overview about this potentialy relevant Names occuring in the documents.","title":"List of names of people of interest like politicians"},{"location":"investigative_journalism/howto-analyze-leaks/#open-data-list-of-location-names","text":"Another option is to filter by town names of your country: Get a list of town names of your country, for example from GeoNames , from Openstreetmap or from Wikidata. So you get another search facet and overview about \"Locations\" occurring in the documents, which are in your country. So you can filter for towns in your country and see which people occurre within the documents which maybe are yet unknown and not in your lists of persons but very interesting for the searched context (i.e. your country).","title":"Open Data list of location names"},{"location":"investigative_journalism/howto-analyze-leaks/#fuzzy-search","text":"No human writer, no OCR software and no machine learning model is perfect. There are typos in documents, wrong recognized chars by OCR and occurring but not recognized names by Named Entity Recognition by machine learning (its biased statistics not artificial intelligence magic). So consider to fuzzy search for similar name variants: As intelligent human considering more cases and conplexity of languages than a dumb computer trying natural language processing by powerful fuzzy search operators or by following UI maybe finding some (but often not as many as a creative human knowing limitations of automatic fuzzy search) useful variants easier:","title":"Fuzzy search"},{"location":"investigative_journalism/howto-analyze-leaks/#big-data-leaks-with-large-amounts-of-files-and-text-often-need-long-processing-time","text":"Huge leaks can be large document sets with thousands, hundred thousands or millions of document files. Today it is mostly no problem to search in even such large amounts of documents with modern Open Source research tools even on older hardware like used by many journalists, if the documents are indexed in a open source search engine. Adding and indexing new documents over time is no problem, too, since only new and changed documents will be indexed on a recrawl of your file directories. But the initial text extraction, OCR and indexing such large document collections with large amounts of files to a search engine can take days or weeks (a former project to index a large document collection for collaborative investigative research took even weeks on modern hardware). But you don't have to wait such a long time until you can start with investigations in such big leaks:","title":"Big Data Leaks with large amounts of files and text often need long processing time"},{"location":"investigative_journalism/howto-analyze-leaks/#lack-of-it-admins-more-user-interfaces-ui-for-investigative-journalists","text":"Most of the below described features and methods to pick low-hanging fruits very fast had been develped years ago by a powerful modular, flexible and extendable Python based plugin architecture, but they needed Linux admin knowledge on Unix command line tools, Extract Transform Load (ETL) pipelines and related command line tools, most investigative journalists do not have. Like the last releases, the coming Open Source release in March (testing and improving stuff which yet in public Git repository) will provide additional web user interfaces and new default settings, so some features can be easier used even by freelance journalists or members of a team, if a specialized datajournalist or admin is busy or temporary not available:","title":"Lack of IT admins? More user interfaces (UI) for investigative journalists"},{"location":"investigative_journalism/howto-analyze-leaks/#get-low-hanging-fruits-very-fast","text":"Many relevant information can be low-hanging fruits from a computing power perspective. So Open Semantic Search can run the extraction and analysis of documents in multiple stages:","title":"Get low-hanging fruits very fast"},{"location":"investigative_journalism/howto-analyze-leaks/#very-fast-indexing-of-filenames-first","text":"Before the plain text will be extracted from documents, the path names and file names of all documents will be indexed first. So you can search very early all the path names and filenames while standard text extraction from your documents is yet running. So you have a faceted search / interactive filters for file paths and file extensions like .pdf for PDF documents or XLS for Excel sheets. If you on this base find interesting paths or documents, you can prioritize files for running text extraction to enable full text search of the content. And even in this early stage you may have faceted search for some of the keywords or names from your domain knowledge in thesaurus or lists of names if they occur in this filenames (dependent on file naming).","title":"Very fast indexing of filenames first"},{"location":"investigative_journalism/howto-analyze-leaks/#named-entity-recognition-ner-by-machine-learning-ml-later","text":"Useful Named Entity Recognition of entities (works often even if not yet in your lists of names) by machine learning later needs up to ten times longer than entity extraction by your thesaurus or ontologies. So this step can be done later after text extraction, too while you are able to fulltext search all your documents earlier and maybe yet work with faster extracted entities from your domain knowledge by thesaurus or ontology based entity extraction. So set the ML NER plugin to be processed later (next release will have an UI config option \"Do NER by ML later\" in Config / Named Entity Recognition)","title":"Named Entity Recognition (NER) by machine learning (ML) later"},{"location":"investigative_journalism/howto-analyze-leaks/#slow-ocr-latest","text":"For most document sets OCR as the slowest task takes most time for mostly only few additional infos. So the very slow OCR tasks will run after all plain text was extracted from all documents and so you can find most texts (which are stored in text formats in the document files) by full text search much earlier. This is the new default setting in Config / OCR:","title":"Slow OCR latest"},{"location":"investigative_journalism/howto-analyze-leaks/#prioritize-text-extraction-and-ocr-tasks","text":"With this first analysis results or even only the path names and filenames you can often early identify potentially interesting files or directories for which you want to prioritize extraction and analysis. There you can click on \"Prioritize\" to open an UI where you can set the priority of certain files:","title":"Prioritize text extraction and OCR tasks"},{"location":"investigative_journalism/howto-analyze-leaks/#share-load-on-multiple-computers-additional-parallel-etl-workers","text":"Maybe your team have an admin and more computers and servers? If one computer is slow, for more performance on the initial data import and analysis you can scale by adding additional computers or servers running a Open Semantic ETL Docker container which runs additional parallel workers to share the heavy analysis load between multiple computers. ]","title":"Share load on multiple computers: Additional parallel ETL workers"},{"location":"lexemes/","text":"Import Wikidata dictionary (lexemes) to Solr synonyms config Open Source tool to import Wikidata lexemes to Solr synonyms config The free Open Source tool wikidata-lexemes-to-solr-synonyms imports lexemes (dictionary including different grammar forms/lexical forms for each lexical entry) from Wikidata to Apache Solr search engine synonyms config. Grammar rules for search and information retrieval Natural language processing of unstructured text is complicated, since the same words/meanings can occur in different grammar forms. Stemming To find other grammar forms of words, the search engine uses stemming heuristics, which for example cuts suffixes like -ing to find other grammar forms of the same words. But such automatic heuristics can fail, especially for example on irregular verbs, where for example another grammatical form like \"went\" is not only \"go\" with a suffix (like f.e. -ing). Human curated dictionary (Linked Data Knowledge Graph of Lexemes as Open Data from WikiData) By considering lexemes by import of Wikidata Lexemes as Solr search engine synonyms you find documents including many such more complicated / irregular grammar forms, too. Hint: You can have a look and browse the structured data of lexemes in the structured data base and linked open data knowledge graph WikiData by the web user interface Ordia . Import Wikidata lexemes to Solr synonyms config The lexemes import tool can be configured by following command line parameters and will be integrated to our web UI next. Command line options Usage: wikidata-lexemes-to-solr-synonyms [options] Options: -h, --help show this help message and exit -s SOLR, --solr=SOLR Solr URI like http://localhost:8983/solr/ -c CORE, --core=CORE Solr core/index name -r RESOURCE, --resource=RESOURCE Solr managed synonyms resource where to store the results -l LANGUAGE, --language=LANGUAGE Language (Wikidata entity) Free Open Source Software (FOSS) The Python & SPARQL based import tool is Free Software under the GPL license. The Open Source code is available in the Github repository opensemanticsearch/lexemes .","title":"Import Wikidata dictionary (lexemes) to Solr synonyms config"},{"location":"lexemes/#import-wikidata-dictionary-lexemes-to-solr-synonyms-config","text":"","title":"Import Wikidata dictionary (lexemes) to Solr synonyms config"},{"location":"lexemes/#open-source-tool-to-import-wikidata-lexemes-to-solr-synonyms-config","text":"The free Open Source tool wikidata-lexemes-to-solr-synonyms imports lexemes (dictionary including different grammar forms/lexical forms for each lexical entry) from Wikidata to Apache Solr search engine synonyms config.","title":"Open Source tool to import Wikidata lexemes to Solr synonyms config"},{"location":"lexemes/#grammar-rules-for-search-and-information-retrieval","text":"Natural language processing of unstructured text is complicated, since the same words/meanings can occur in different grammar forms.","title":"Grammar rules for search and information retrieval"},{"location":"lexemes/#stemming","text":"To find other grammar forms of words, the search engine uses stemming heuristics, which for example cuts suffixes like -ing to find other grammar forms of the same words. But such automatic heuristics can fail, especially for example on irregular verbs, where for example another grammatical form like \"went\" is not only \"go\" with a suffix (like f.e. -ing).","title":"Stemming"},{"location":"lexemes/#human-curated-dictionary-linked-data-knowledge-graph-of-lexemes-as-open-data-from-wikidata","text":"By considering lexemes by import of Wikidata Lexemes as Solr search engine synonyms you find documents including many such more complicated / irregular grammar forms, too. Hint: You can have a look and browse the structured data of lexemes in the structured data base and linked open data knowledge graph WikiData by the web user interface Ordia .","title":"Human curated dictionary (Linked Data Knowledge Graph of Lexemes as Open Data from WikiData)"},{"location":"lexemes/#import-wikidata-lexemes-to-solr-synonyms-config","text":"The lexemes import tool can be configured by following command line parameters and will be integrated to our web UI next.","title":"Import Wikidata lexemes to Solr synonyms config"},{"location":"lexemes/#command-line-options","text":"Usage: wikidata-lexemes-to-solr-synonyms [options] Options: -h, --help show this help message and exit -s SOLR, --solr=SOLR Solr URI like http://localhost:8983/solr/ -c CORE, --core=CORE Solr core/index name -r RESOURCE, --resource=RESOURCE Solr managed synonyms resource where to store the results -l LANGUAGE, --language=LANGUAGE Language (Wikidata entity)","title":"Command line options"},{"location":"lexemes/#free-open-source-software-foss","text":"The Python & SPARQL based import tool is Free Software under the GPL license. The Open Source code is available in the Github repository opensemanticsearch/lexemes .","title":"Free Open Source Software (FOSS)"},{"location":"rdf2ocr/","text":"Improve OCR by SKOS thesaurus or RDF ontology The free Open Source tool Solr Ontology Tagger extracts words of a Resource Description Format (RDF) ontology and Simple Knowledge Organization System (SKOS) thesaurus to improve automatic text recognition by OCR by adding the words to a custom dictionary (the \"user words\" list) for the open source optical character recognition tool Tesseract OCR . How to improve OCR of concepts or names in thesaurus or ontology By the out of the box integration with the Tesseract OCR user word list or custom dictionary , your concepts, words and names of named entities like organizations, places, locations or persons that are important for you so you added them to your thesaurus or which are included in lists of names or ontologies (for example lists of names of relevant persons from internal databases or metadata sources or from open data sources like Wikidata ) you defined for faceted search/interactive filters and/or analytics/aggregated overviews are recognized better by OCR of scanned documents , too. Integrate SKOS Thesaurus or RDF ontology with Tesseract OCR dictionary Therefore your additional domain knowledge / vocabulary from thesaurus, lists and ontologies is used additional to the default OCR dictionaries by the Tesseract option --user-words /etc/opensemanticsearch/ocr/dictionary.txt . In documents on paper names often in uppercase Since in many scanned legacy files on paper names are fully written in uppercase and for dumb computers and so OCR an \"A\" is another char than an \"a\", this autogenerated custom OCR dictionary / OCR wordlist includes the complete uppercase variant of each name or word, too. Open Standards for Thesauruses and Ontologies (RDF & SKOS) Since using open standards Resource Description Framework (RDF) for ontologies and Simple Knowledge Organization System (SKOS) for thesauri, knowledge bases, lists of entities, ontologies or taxonomies you do not have to add or manage all important names yourself: Open Data vocabularies like Wikidata So you can use linked open data sources and databases like Wikidata , the vocabularies of the European Union or the Unesco Thesaurus. Free Open Source Software (FOSS) Since the tool and used libraries are free Open Source Software based on Python & rdflib, the full source code is included inside the downloadable packages and hosted on Github .","title":"Improve OCR by SKOS thesaurus or RDF ontology"},{"location":"rdf2ocr/#improve-ocr-by-skos-thesaurus-or-rdf-ontology","text":"The free Open Source tool Solr Ontology Tagger extracts words of a Resource Description Format (RDF) ontology and Simple Knowledge Organization System (SKOS) thesaurus to improve automatic text recognition by OCR by adding the words to a custom dictionary (the \"user words\" list) for the open source optical character recognition tool Tesseract OCR .","title":"Improve OCR by SKOS thesaurus or RDF ontology"},{"location":"rdf2ocr/#how-to-improve-ocr-of-concepts-or-names-in-thesaurus-or-ontology","text":"By the out of the box integration with the Tesseract OCR user word list or custom dictionary , your concepts, words and names of named entities like organizations, places, locations or persons that are important for you so you added them to your thesaurus or which are included in lists of names or ontologies (for example lists of names of relevant persons from internal databases or metadata sources or from open data sources like Wikidata ) you defined for faceted search/interactive filters and/or analytics/aggregated overviews are recognized better by OCR of scanned documents , too.","title":"How to improve OCR of concepts or names in thesaurus or ontology"},{"location":"rdf2ocr/#integrate-skos-thesaurus-or-rdf-ontology-with-tesseract-ocr-dictionary","text":"Therefore your additional domain knowledge / vocabulary from thesaurus, lists and ontologies is used additional to the default OCR dictionaries by the Tesseract option --user-words /etc/opensemanticsearch/ocr/dictionary.txt .","title":"Integrate SKOS Thesaurus or RDF ontology with Tesseract OCR dictionary"},{"location":"rdf2ocr/#in-documents-on-paper-names-often-in-uppercase","text":"Since in many scanned legacy files on paper names are fully written in uppercase and for dumb computers and so OCR an \"A\" is another char than an \"a\", this autogenerated custom OCR dictionary / OCR wordlist includes the complete uppercase variant of each name or word, too.","title":"In documents on paper names often in uppercase"},{"location":"rdf2ocr/#open-standards-for-thesauruses-and-ontologies-rdf-skos","text":"Since using open standards Resource Description Framework (RDF) for ontologies and Simple Knowledge Organization System (SKOS) for thesauri, knowledge bases, lists of entities, ontologies or taxonomies you do not have to add or manage all important names yourself:","title":"Open Standards for Thesauruses and Ontologies (RDF &amp; SKOS)"},{"location":"rdf2ocr/#open-data-vocabularies-like-wikidata","text":"So you can use linked open data sources and databases like Wikidata , the vocabularies of the European Union or the Unesco Thesaurus.","title":"Open Data vocabularies like Wikidata"},{"location":"rdf2ocr/#free-open-source-software-foss","text":"Since the tool and used libraries are free Open Source Software based on Python & rdflib, the full source code is included inside the downloadable packages and hosted on Github .","title":"Free Open Source Software (FOSS)"},{"location":"rss-feed-manager-python-django/","text":"RSS-Feed Manager Managing newsfeeds and imports RSS-Feed manager is a web app and user interface for managing imports of RSS-Newsfeeds. Usage Read the user documentation . Installation download the module solr-search-rss-feed-manager-python-django Copy the directory rss_manager from the zip file into your Django apps directory Enable the new app: Add \" rss_manager \" to your INSTALLED_APPS setting like this: INSTALLED_APPS = ( ... 'rss_manager', ) * Include the querytagger URLconf in your project urls.py like this: url(r'^rss_manager/', include('rss_manager.urls')), * Setup a cron getting http://localhost:8000/rss_manager/import periodically (i.e. every 5 minutes). Roadmap Plans for the feature ( please donate ): We want to add tagging functionality to the RSS importer, so you can add metadata or tags to all news imported with this feed, i.e. a special topic). We want to add tagging for RSS-Feed management, so you can organize many newsfeeds in the management interface with tags.","title":"RSS-Feed Manager"},{"location":"rss-feed-manager-python-django/#rss-feed-manager","text":"","title":"RSS-Feed Manager"},{"location":"rss-feed-manager-python-django/#managing-newsfeeds-and-imports","text":"RSS-Feed manager is a web app and user interface for managing imports of RSS-Newsfeeds.","title":"Managing newsfeeds and imports"},{"location":"rss-feed-manager-python-django/#usage","text":"Read the user documentation .","title":"Usage"},{"location":"rss-feed-manager-python-django/#installation","text":"download the module solr-search-rss-feed-manager-python-django Copy the directory rss_manager from the zip file into your Django apps directory Enable the new app: Add \" rss_manager \" to your INSTALLED_APPS setting like this: INSTALLED_APPS = ( ... 'rss_manager', ) * Include the querytagger URLconf in your project urls.py like this: url(r'^rss_manager/', include('rss_manager.urls')), * Setup a cron getting http://localhost:8000/rss_manager/import periodically (i.e. every 5 minutes).","title":"Installation"},{"location":"rss-feed-manager-python-django/#roadmap","text":"Plans for the feature ( please donate ): We want to add tagging functionality to the RSS importer, so you can add metadata or tags to all news imported with this feed, i.e. a special topic). We want to add tagging for RSS-Feed management, so you can organize many newsfeeds in the management interface with tags.","title":"Roadmap"},{"location":"search-list/","text":"Search Solr with lists Searching a list of queries and show for each entry if there are results. Usage Read the user documentation . Installation Download and install the module solr-search-list . Setup the Django Webapp If you want to use the web user interface (webapp): * download the module opensemanticsearch-search-list-python-django * Copy the directory search_list from the zip file into your Django apps directory * Enable the new app: Add \" search_list \" to your INSTALLED_APPS setting like this: INSTALLED_APPS = ( ... 'search_list', ) * Include the search_list URLconf in your project urls.py like this: url(r'^search-list/', include('search_list.urls')),","title":"Search Solr with lists"},{"location":"search-list/#search-solr-with-lists","text":"Searching a list of queries and show for each entry if there are results.","title":"Search Solr with lists"},{"location":"search-list/#usage","text":"Read the user documentation .","title":"Usage"},{"location":"search-list/#installation","text":"Download and install the module solr-search-list .","title":"Installation"},{"location":"search-list/#setup-the-django-webapp","text":"If you want to use the web user interface (webapp): * download the module opensemanticsearch-search-list-python-django * Copy the directory search_list from the zip file into your Django apps directory * Enable the new app: Add \" search_list \" to your INSTALLED_APPS setting like this: INSTALLED_APPS = ( ... 'search_list', ) * Include the search_list URLconf in your project urls.py like this: url(r'^search-list/', include('search_list.urls')),","title":"Setup the Django Webapp"},{"location":"skos2solr/","text":"SKOS thesaurus to Solr synonyms (SKOS to Solr) To be able to find words, concepts and names and URIs by synonyms, aliases or (alternate) labels, too, the thesaurus manager and ontologies manager apps are integrated with the Solr search server . Converts Simple Knowledge Organization System (SKOS) thesaurus to Apache Solr synonyms config file This Python component converts or exports the thesauri from the open standard Simple Knowledge Organization System (SKOS) to an Solr synonyms configuration file for for a Solr synonym filter . Find words by alternate labels, synonyms, RDFS labels and URIs Therefore it adds to the Solr synonym config file all alternate SKOS labels or aliases from thesaurus entries as synonym for their prefered labels. So you will find documents containing only alternate labels, too. It supports synonyms like SKOS:altLabel or SKOS:hiddenlabel adding them as synonyms for SKOS:prefLabel and RDFS:label. Additionally it adds the labels of an URI of an entity as synonym for this URI, so you can find and filter documents containing only the URIs by the URIs labels, too. Alternate Solr plugin An alternate for expansion of terms by SKOS vocabularies is to use the Solr plugin Lucene SKOS . The disadvantage of this alternate is, that the plugin version has to be compatible with your Solr version and its more complex plugin interface, which could change more often than the simple Solr synonym config format.","title":"SKOS thesaurus to Solr synonyms (SKOS to Solr)"},{"location":"skos2solr/#skos-thesaurus-to-solr-synonyms-skos-to-solr","text":"To be able to find words, concepts and names and URIs by synonyms, aliases or (alternate) labels, too, the thesaurus manager and ontologies manager apps are integrated with the Solr search server .","title":"SKOS thesaurus to Solr synonyms (SKOS to Solr)"},{"location":"skos2solr/#converts-simple-knowledge-organization-system-skos-thesaurus-to-apache-solr-synonyms-config-file","text":"This Python component converts or exports the thesauri from the open standard Simple Knowledge Organization System (SKOS) to an Solr synonyms configuration file for for a Solr synonym filter .","title":"Converts Simple Knowledge Organization System (SKOS) thesaurus to Apache Solr synonyms config file"},{"location":"skos2solr/#find-words-by-alternate-labels-synonyms-rdfs-labels-and-uris","text":"Therefore it adds to the Solr synonym config file all alternate SKOS labels or aliases from thesaurus entries as synonym for their prefered labels. So you will find documents containing only alternate labels, too. It supports synonyms like SKOS:altLabel or SKOS:hiddenlabel adding them as synonyms for SKOS:prefLabel and RDFS:label. Additionally it adds the labels of an URI of an entity as synonym for this URI, so you can find and filter documents containing only the URIs by the URIs labels, too.","title":"Find words by alternate labels, synonyms, RDFS labels and URIs"},{"location":"skos2solr/#alternate-solr-plugin","text":"An alternate for expansion of terms by SKOS vocabularies is to use the Solr plugin Lucene SKOS . The disadvantage of this alternate is, that the plugin version has to be compatible with your Solr version and its more complex plugin interface, which could change more often than the simple Solr synonym config format.","title":"Alternate Solr plugin"},{"location":"solr/","text":"Solr Server (Daemon) Solr package for Debian and Ubuntu This Debian package and Ubuntu package is a preconfigurated Apache Solr server running as a daemon providing important settings like the integration of the thesaurus editor and ontologies manager, settings for better performance, disabled logging and security settings and a more current Solr version than the packages of the Debian or Ubuntu standard repositories. Settings of preconfigured Solr package Disabled Logfiles Disabled logfiles : we don't want to write each search query to Solr logs. If you want to switch on logging for debugging purposes, switch on file and console in the config file /var/solr/log4j.properties Autocommits Automatic commits to the index after 15 seconds after adding or update of documents (autocommit=15000) Running as daemon Automatic start on booting running as a daemon in Debian GNU/Linux or Ubuntu Linux. Increase maximum RAM settings of the Java Virtual Machine (JVM) Automatic memory settings : In most cases no manual setting of Java virtual machine options is needed anymore. Allows the Java VM to use as much RAM as possible on this server, so you won't have problems because of default Java Virtual Machine (JVM) maximal RAM settings (option -Xmx ) if indexing large amounts of data or large documents. Swappiness Disabled swappiness , so the system will only swap if necessary. So it doesn't do so to optimize RAM for running software swapping the Solr index and search caches automatically after some time because they are not used for some time. Why? Even if some parts of the Solr index and caches in RAM are not used for a long time (i.e. if search isn't used for the night or some days) and that RAM could be used by other software meanwhile to read hundrets of MB or some GB from Swap on slower harddisks to RAM again because of using again while the first search after long time would lead to timeouts and errors on maybe important searches, which then could take tens of seconds longer. Access only from localhost For security reasons access to the Solr search server is only possible from the same computer. So access is only possible from localhost , so that if you set a password to the User Interfaces module solr-php-ui and the search apps nobody without an account on your computer or an account for a service on your computer can read all the data from Solr. To enable Solr remote admin access from other computers than localhost you have to edit jetty-http.xml and delete the default=\"127.0.0.1\" from the config option \"host\" . Then restart Solr by service solr restart . Warning: You don't want to enable access to an unprotected Solr server with the possibility to read, add, change or delete all indexed data for everybody on the intranet or internet! So if the computers are part of a network you can not fully trust, you have to protect the IP of the Solr server or the Solr port for example by a firewall. Solr schema There are additional fields and stemming configured in the Solr schema. You can read the XML schema config in /var/solr/data/core1/conf/managed-schema which is based on the Solr example config set /opt/solr/server/solr/configsets , so you can use a diff tool to compare and see the config additions. Additionally the ETL and search tool adds & uses some additional fields which are created automatically using the Solr dynamic fields feature configured for the schema because of type endings like _b _s or _tt. You can see such additional fields using the table view.","title":"Solr Server (Daemon)"},{"location":"solr/#solr-server-daemon","text":"","title":"Solr Server (Daemon)"},{"location":"solr/#solr-package-for-debian-and-ubuntu","text":"This Debian package and Ubuntu package is a preconfigurated Apache Solr server running as a daemon providing important settings like the integration of the thesaurus editor and ontologies manager, settings for better performance, disabled logging and security settings and a more current Solr version than the packages of the Debian or Ubuntu standard repositories.","title":"Solr package for Debian and Ubuntu"},{"location":"solr/#settings-of-preconfigured-solr-package","text":"","title":"Settings of preconfigured Solr package"},{"location":"solr/#disabled-logfiles","text":"Disabled logfiles : we don't want to write each search query to Solr logs. If you want to switch on logging for debugging purposes, switch on file and console in the config file /var/solr/log4j.properties","title":"Disabled Logfiles"},{"location":"solr/#autocommits","text":"Automatic commits to the index after 15 seconds after adding or update of documents (autocommit=15000)","title":"Autocommits"},{"location":"solr/#running-as-daemon","text":"Automatic start on booting running as a daemon in Debian GNU/Linux or Ubuntu Linux.","title":"Running as daemon"},{"location":"solr/#increase-maximum-ram-settings-of-the-java-virtual-machine-jvm","text":"Automatic memory settings : In most cases no manual setting of Java virtual machine options is needed anymore. Allows the Java VM to use as much RAM as possible on this server, so you won't have problems because of default Java Virtual Machine (JVM) maximal RAM settings (option -Xmx ) if indexing large amounts of data or large documents.","title":"Increase maximum RAM settings of the Java Virtual Machine (JVM)"},{"location":"solr/#swappiness","text":"Disabled swappiness , so the system will only swap if necessary. So it doesn't do so to optimize RAM for running software swapping the Solr index and search caches automatically after some time because they are not used for some time. Why? Even if some parts of the Solr index and caches in RAM are not used for a long time (i.e. if search isn't used for the night or some days) and that RAM could be used by other software meanwhile to read hundrets of MB or some GB from Swap on slower harddisks to RAM again because of using again while the first search after long time would lead to timeouts and errors on maybe important searches, which then could take tens of seconds longer.","title":"Swappiness"},{"location":"solr/#access-only-from-localhost","text":"For security reasons access to the Solr search server is only possible from the same computer. So access is only possible from localhost , so that if you set a password to the User Interfaces module solr-php-ui and the search apps nobody without an account on your computer or an account for a service on your computer can read all the data from Solr. To enable Solr remote admin access from other computers than localhost you have to edit jetty-http.xml and delete the default=\"127.0.0.1\" from the config option \"host\" . Then restart Solr by service solr restart . Warning: You don't want to enable access to an unprotected Solr server with the possibility to read, add, change or delete all indexed data for everybody on the intranet or internet! So if the computers are part of a network you can not fully trust, you have to protect the IP of the Solr server or the Solr port for example by a firewall.","title":"Access only from localhost"},{"location":"solr/#solr-schema","text":"There are additional fields and stemming configured in the Solr schema. You can read the XML schema config in /var/solr/data/core1/conf/managed-schema which is based on the Solr example config set /opt/solr/server/solr/configsets , so you can use a diff tool to compare and see the config additions. Additionally the ETL and search tool adds & uses some additional fields which are created automatically using the Solr dynamic fields feature configured for the schema because of type endings like _b _s or _tt. You can see such additional fields using the table view.","title":"Solr schema"},{"location":"solr-ontology-tagger/","text":"Ontology tagger for Solr (Automatic tagging by RDF ontologies & SKOS thesaurus) Annotator for Apache Solr by Resource Description Framework (RDF) ontology & Simple Knowledge Organization System (SKOS) thesaurus The auto-tagger Ontology Tagger for Apache Solr is the preconfigured search engine component for automatic tagging or auto-classification of documents in an Apache Solr index for faceted search by labels in data structures like ontologies in the open standard RDF & thesauruses in the open standard SKOS or linked open data sources and databases like Wikidata . Automatic tagger for faceted search with Solr You can structure, filter and navigate your indexed documents or datasets by faceted search based on structures like thesauri, knowledge bases, lists of entities, ontologies or taxonomies available in open standards for the semantic web or linked data formats like Resource Description Format (RDF) or Simple knowledge organization system (SKOS) . Free Open Source Software (FOSS) Since the Ontology based auto-tagging tool and library is free Open Source Software based on Python & rdflib, the full source code is included inside the downloadable packages and hosted on Github . User interface (UI) for managing ontologies and thesauri for automatic tagging A simple web app based user interface (UI) for easily configuring Solr with ontologies or thesauri for faceted search is provided by the Python Django App Ontologies Manager , and it's code is available inside our distribution packages and on Github too. Poor mans entity linking without disambiguation Since for most use cases not so important if you work mainly with your own datasets and domain specific knowledge instead of universal databases with many ambigous concepts or names, at the moment there is no disambiguation integrated for automatic tagging or poor mans entity linking. Please donate so we can integrate methods and UIs to disambiguate homonyms and different entities with same names or same labels. Automatic ontology tagger and annotator for Elastic search Our search engine distribution is based on Apache Solr. Please donate with the subject \"Elasticsearch ontology tagger\" if you want to use these integrated tools for Elasticsearch , since a generalization of this relatively small part of the search engine specific code would cost only few hours of effort or configure an alternate Ontology Annotator with an Elastic search plugin . Open Source tools for entity linking, dictionary based entity extraction or dictionary based annotation Other methods, open source frameworks and free tools for automatic tagging, entity linking, entity extraction or disambiguation by machine learning: Apache Stanbol Entity Linking Engine BioSolr ontology annotator - Ontology tagger for Solr or Elastic Search Fast entity linker - Entity linking with disambiguation by machine learning Dexter NEL SolrTextTagger Solr Dictionary Annotator - Microservice for Spark Datafari OntologyUpdateProcessor - Solr update processor plugin Apache UIMA dictionary annotator Apache UIMA concept mapper","title":"Ontology tagger for Solr (Automatic tagging by RDF ontologies & SKOS thesaurus)"},{"location":"solr-ontology-tagger/#ontology-tagger-for-solr-automatic-tagging-by-rdf-ontologies-skos-thesaurus","text":"","title":"Ontology tagger for Solr (Automatic tagging by RDF ontologies &amp; SKOS thesaurus)"},{"location":"solr-ontology-tagger/#annotator-for-apache-solr-by-resource-description-framework-rdf-ontology-simple-knowledge-organization-system-skos-thesaurus","text":"The auto-tagger Ontology Tagger for Apache Solr is the preconfigured search engine component for automatic tagging or auto-classification of documents in an Apache Solr index for faceted search by labels in data structures like ontologies in the open standard RDF & thesauruses in the open standard SKOS or linked open data sources and databases like Wikidata .","title":"Annotator for Apache Solr by Resource Description Framework (RDF) ontology &amp; Simple Knowledge Organization System (SKOS) thesaurus"},{"location":"solr-ontology-tagger/#automatic-tagger-for-faceted-search-with-solr","text":"You can structure, filter and navigate your indexed documents or datasets by faceted search based on structures like thesauri, knowledge bases, lists of entities, ontologies or taxonomies available in open standards for the semantic web or linked data formats like Resource Description Format (RDF) or Simple knowledge organization system (SKOS) .","title":"Automatic tagger for faceted search with Solr"},{"location":"solr-ontology-tagger/#free-open-source-software-foss","text":"Since the Ontology based auto-tagging tool and library is free Open Source Software based on Python & rdflib, the full source code is included inside the downloadable packages and hosted on Github .","title":"Free Open Source Software (FOSS)"},{"location":"solr-ontology-tagger/#user-interface-ui-for-managing-ontologies-and-thesauri-for-automatic-tagging","text":"A simple web app based user interface (UI) for easily configuring Solr with ontologies or thesauri for faceted search is provided by the Python Django App Ontologies Manager , and it's code is available inside our distribution packages and on Github too.","title":"User interface (UI) for managing ontologies and thesauri for automatic tagging"},{"location":"solr-ontology-tagger/#poor-mans-entity-linking-without-disambiguation","text":"Since for most use cases not so important if you work mainly with your own datasets and domain specific knowledge instead of universal databases with many ambigous concepts or names, at the moment there is no disambiguation integrated for automatic tagging or poor mans entity linking. Please donate so we can integrate methods and UIs to disambiguate homonyms and different entities with same names or same labels.","title":"Poor mans entity linking without disambiguation"},{"location":"solr-ontology-tagger/#automatic-ontology-tagger-and-annotator-for-elastic-search","text":"Our search engine distribution is based on Apache Solr. Please donate with the subject \"Elasticsearch ontology tagger\" if you want to use these integrated tools for Elasticsearch , since a generalization of this relatively small part of the search engine specific code would cost only few hours of effort or configure an alternate Ontology Annotator with an Elastic search plugin .","title":"Automatic ontology tagger and annotator for Elastic search"},{"location":"solr-ontology-tagger/#open-source-tools-for-entity-linking-dictionary-based-entity-extraction-or-dictionary-based-annotation","text":"Other methods, open source frameworks and free tools for automatic tagging, entity linking, entity extraction or disambiguation by machine learning: Apache Stanbol Entity Linking Engine BioSolr ontology annotator - Ontology tagger for Solr or Elastic Search Fast entity linker - Entity linking with disambiguation by machine learning Dexter NEL SolrTextTagger Solr Dictionary Annotator - Microservice for Spark Datafari OntologyUpdateProcessor - Solr update processor plugin Apache UIMA dictionary annotator Apache UIMA concept mapper","title":"Open Source tools for entity linking, dictionary based entity extraction or dictionary based annotation"},{"location":"solr-php-ui/","text":"title: Solr-PHP-UI: Solr client and user interface for search (UI) authors: - Markus Mandalka Solr-PHP-UI: Solr client and user interface for search (UI) This search user interface component of the open source search engine Open Semantic Search is a PHP based lightweight search client with complete and responsive user interface for searching with Apache Lucene / Solr (open source enterprise search server). So you can install / run an search engine on standard LAMP or standard PHP webspace without effort (based on vanilla PHP and Solr-PHP-Client). It doesn't need a database but only connection to or an installation of a standard Solr server/service (ready packages available). Just install the Debian GNU/Linux or Ubuntu Linux package or copy the source to PHP webspace and search with a flexible responsive UI: Features Fulltext search Facetted search (interactive filters) Viewer for preview Analyze text Sorting Different views: Views Result list (with snippets and highlighted search terms) Preview Images* Table (sortable and filterable) Analyze search results or all documents with data visualizations: Wordcloud Trend chart Networks, connections and relations (graph) Configuration Edit /etc/solr-php-ui/config.php (if installed from a package) or config/config.php (if installed from source) for setting another language or for adding custom facets (additional fields and interactive filters). Used libraries Used libraries and frameworks (all included in the package) * solr-php-client (Solr client library for PHP) * jquery (javascript library for easier accessing the dom model) * Zurb Foundation (CSS and UI lib for responsive web design) * d3js (javascript library for visualizations) * nvd3 (d3js based javascript library for charts) * Cytoscape.js (graph) Dashboards Solr-PHP-UI is a lightweight solution which doesn't need a database or special libraries. So the configuration options for the users are limited. If you want to configure dashboards, add some calls of a view into iframes, show some Solr-PHP-UI RSS feeds (i.e. show results of some configured search feeds in Drupal views) or you have to setup a user interface with a database like Hue (Django/Python) or Sarnia (Drupal/PHP): Alternates Alternate components (Open Source user interfaces for Solr): * Velocity (Java) * Solarium (PHP) * Drupal Solr and Sarnia (PHP and Drupal) * Hue Hadoop User Interface (Python and Django) * Blacklight (Ruby on rails)","title":"Index"},{"location":"solr-php-ui/#solr-php-ui-solr-client-and-user-interface-for-search-ui","text":"This search user interface component of the open source search engine Open Semantic Search is a PHP based lightweight search client with complete and responsive user interface for searching with Apache Lucene / Solr (open source enterprise search server). So you can install / run an search engine on standard LAMP or standard PHP webspace without effort (based on vanilla PHP and Solr-PHP-Client). It doesn't need a database but only connection to or an installation of a standard Solr server/service (ready packages available). Just install the Debian GNU/Linux or Ubuntu Linux package or copy the source to PHP webspace and search with a flexible responsive UI:","title":"Solr-PHP-UI: Solr client and user interface for search (UI)"},{"location":"solr-php-ui/#features","text":"Fulltext search Facetted search (interactive filters) Viewer for preview Analyze text Sorting Different views:","title":"Features"},{"location":"solr-php-ui/#views","text":"Result list (with snippets and highlighted search terms) Preview Images* Table (sortable and filterable) Analyze search results or all documents with data visualizations: Wordcloud Trend chart Networks, connections and relations (graph)","title":"Views"},{"location":"solr-php-ui/#configuration","text":"Edit /etc/solr-php-ui/config.php (if installed from a package) or config/config.php (if installed from source) for setting another language or for adding custom facets (additional fields and interactive filters).","title":"Configuration"},{"location":"solr-php-ui/#used-libraries","text":"Used libraries and frameworks (all included in the package) * solr-php-client (Solr client library for PHP) * jquery (javascript library for easier accessing the dom model) * Zurb Foundation (CSS and UI lib for responsive web design) * d3js (javascript library for visualizations) * nvd3 (d3js based javascript library for charts) * Cytoscape.js (graph)","title":"Used libraries"},{"location":"solr-php-ui/#dashboards","text":"Solr-PHP-UI is a lightweight solution which doesn't need a database or special libraries. So the configuration options for the users are limited. If you want to configure dashboards, add some calls of a view into iframes, show some Solr-PHP-UI RSS feeds (i.e. show results of some configured search feeds in Drupal views) or you have to setup a user interface with a database like Hue (Django/Python) or Sarnia (Drupal/PHP):","title":"Dashboards"},{"location":"solr-php-ui/#alternates","text":"Alternate components (Open Source user interfaces for Solr): * Velocity (Java) * Solarium (PHP) * Drupal Solr and Sarnia (PHP and Drupal) * Hue Hadoop User Interface (Python and Django) * Blacklight (Ruby on rails)","title":"Alternates"},{"location":"solr-ranking-analysis/","text":"Solr Relevance Ranking Analysis and Visualization Tool Relevance Ranking Analysis and Visualization for easier Solr relevancy tuning This Python Django based Open Source tool and web user interface (UI) for easier Solr Relevancy analysis helps while one is performing search relevance tuning and relevancy ranking debugging. Therefore the tool summarize and visualizes the relevance ranking and scoring by field boosts (qf), term weights (TF/IDF) and boost function (bf) score of documents found by an Apache Solr search query. Usage Open the web user interface (UI) on the server/port/path you run this Django web app (see section \"Installation\"). Copy the full Solr query (URL) to the field \"Query\" of the form in the web user interface (UI) Click the button \"Analyze relevance ranking\" Visual summary So you get an visual summary of the relevance ranking of the found documents: Ranking details By clicking the button \"Show details\" you get the full details of the scoring calculation for each document: Visualization The button \"Chart\" in the top bar shows a more compact visualization: Installation and configuration The tool can be used with other Apache Solr environments than Open Semantic Search. You find the documentation of the installation and configuration in the README.md . Free Open Source Software The tool is Free Software. You find the full Source Code on GitHub","title":"Solr Relevance Ranking Analysis and Visualization Tool"},{"location":"solr-ranking-analysis/#solr-relevance-ranking-analysis-and-visualization-tool","text":"","title":"Solr Relevance Ranking Analysis and Visualization Tool"},{"location":"solr-ranking-analysis/#relevance-ranking-analysis-and-visualization-for-easier-solr-relevancy-tuning","text":"This Python Django based Open Source tool and web user interface (UI) for easier Solr Relevancy analysis helps while one is performing search relevance tuning and relevancy ranking debugging. Therefore the tool summarize and visualizes the relevance ranking and scoring by field boosts (qf), term weights (TF/IDF) and boost function (bf) score of documents found by an Apache Solr search query.","title":"Relevance Ranking Analysis and Visualization for easier Solr relevancy tuning"},{"location":"solr-ranking-analysis/#usage","text":"Open the web user interface (UI) on the server/port/path you run this Django web app (see section \"Installation\"). Copy the full Solr query (URL) to the field \"Query\" of the form in the web user interface (UI) Click the button \"Analyze relevance ranking\"","title":"Usage"},{"location":"solr-ranking-analysis/#visual-summary","text":"So you get an visual summary of the relevance ranking of the found documents:","title":"Visual summary"},{"location":"solr-ranking-analysis/#ranking-details","text":"By clicking the button \"Show details\" you get the full details of the scoring calculation for each document:","title":"Ranking details"},{"location":"solr-ranking-analysis/#visualization","text":"The button \"Chart\" in the top bar shows a more compact visualization:","title":"Visualization"},{"location":"solr-ranking-analysis/#installation-and-configuration","text":"The tool can be used with other Apache Solr environments than Open Semantic Search. You find the documentation of the installation and configuration in the README.md .","title":"Installation and configuration"},{"location":"solr-ranking-analysis/#free-open-source-software","text":"The tool is Free Software. You find the full Source Code on GitHub","title":"Free Open Source Software"},{"location":"solr-relevance-ranking-analysis/","text":"Solr Relevance Ranking Analysis and Visualization Tool Relevance Ranking Analysis and Visualization for easier Solr relevancy tuning This Python Django based Open Source tool and web user interface (UI) allows for easier Solr Relevancy analysis and is helpful while one is performing search relevance tuning and relevancy ranking debugging. The tool summarizes and visualizes the relevance ranking and scoring by field boosts (qf), term weights (TF/IDF) and the boost function (bf) score of documents found by an Apache Solr search query. Usage Open the web user interface (UI) on the server/port/path you run this Django web app (see section \"Installation\"). Copy the full Solr query (URL) to the field \"Query\" of the form in the web user interface (UI) Click the button \"Analyze relevance ranking\" Visual summary So you get a visual summary of the relevance ranking of the found documents: Ranking details By clicking the button \"Show details\" you get the full details of the scoring calculation for each document: Visualization The button \"Chart\" in the top bar shows a more compact visualization: Installation and configuration The tool can be used with other Apache Solr environments than Open Semantic Search. You can find the documentation on installation and configuration in the README.md . Free Open Source Software The tool is Free Software. You can find the full Source Code on GitHub","title":"Solr Relevance Ranking Analysis and Visualization Tool"},{"location":"solr-relevance-ranking-analysis/#solr-relevance-ranking-analysis-and-visualization-tool","text":"","title":"Solr Relevance Ranking Analysis and Visualization Tool"},{"location":"solr-relevance-ranking-analysis/#relevance-ranking-analysis-and-visualization-for-easier-solr-relevancy-tuning","text":"This Python Django based Open Source tool and web user interface (UI) allows for easier Solr Relevancy analysis and is helpful while one is performing search relevance tuning and relevancy ranking debugging. The tool summarizes and visualizes the relevance ranking and scoring by field boosts (qf), term weights (TF/IDF) and the boost function (bf) score of documents found by an Apache Solr search query.","title":"Relevance Ranking Analysis and Visualization for easier Solr relevancy tuning"},{"location":"solr-relevance-ranking-analysis/#usage","text":"Open the web user interface (UI) on the server/port/path you run this Django web app (see section \"Installation\"). Copy the full Solr query (URL) to the field \"Query\" of the form in the web user interface (UI) Click the button \"Analyze relevance ranking\"","title":"Usage"},{"location":"solr-relevance-ranking-analysis/#visual-summary","text":"So you get a visual summary of the relevance ranking of the found documents:","title":"Visual summary"},{"location":"solr-relevance-ranking-analysis/#ranking-details","text":"By clicking the button \"Show details\" you get the full details of the scoring calculation for each document:","title":"Ranking details"},{"location":"solr-relevance-ranking-analysis/#visualization","text":"The button \"Chart\" in the top bar shows a more compact visualization:","title":"Visualization"},{"location":"solr-relevance-ranking-analysis/#installation-and-configuration","text":"The tool can be used with other Apache Solr environments than Open Semantic Search. You can find the documentation on installation and configuration in the README.md .","title":"Installation and configuration"},{"location":"solr-relevance-ranking-analysis/#free-open-source-software","text":"The tool is Free Software. You can find the full Source Code on GitHub","title":"Free Open Source Software"},{"location":"solr-search-csv-python-django/","text":"CSV Manager User interface (webapp) for structured import of CSV spreadsheets. Usage See the user documentation Installation Download the module opensemanticsearch-search-csv-python-django Copy the directory csvmanager from the zip file into your Django apps directory Enable the new app: Add \" csvmanager \" to your INSTALLED_APPS setting like this: INSTALLED_APPS = ( ... 'csvmanager', ) Include the search_list URLconf in your project urls.py like this: url(r'^csvmanager/', include('csvmanager.urls')),","title":"CSV Manager"},{"location":"solr-search-csv-python-django/#csv-manager","text":"User interface (webapp) for structured import of CSV spreadsheets.","title":"CSV Manager"},{"location":"solr-search-csv-python-django/#usage","text":"See the user documentation","title":"Usage"},{"location":"solr-search-csv-python-django/#installation","text":"Download the module opensemanticsearch-search-csv-python-django Copy the directory csvmanager from the zip file into your Django apps directory Enable the new app: Add \" csvmanager \" to your INSTALLED_APPS setting like this: INSTALLED_APPS = ( ... 'csvmanager', ) Include the search_list URLconf in your project urls.py like this: url(r'^csvmanager/', include('csvmanager.urls')),","title":"Installation"},{"location":"solr-search-querytagger-python-django/","text":"Web interface for tagging all results of a Solr search query Tags all results of a search query. Usage See the user documentation Installation download the module solr-search-querytagger-python-django Copy the directory querytagger from the zip file into your Django apps directory Enable the new app: Add \" querytagger \" to your INSTALLED\\APPS setting like this: INSTALLED_APPS = ( ... 'querytagger', ) Include the querytagger URLconf in your project urls.py like this: url(r'^querytagger/', include('querytagger.urls')),","title":"Web interface for tagging all results of a Solr search query"},{"location":"solr-search-querytagger-python-django/#web-interface-for-tagging-all-results-of-a-solr-search-query","text":"Tags all results of a search query.","title":"Web interface for tagging all results of a Solr search query"},{"location":"solr-search-querytagger-python-django/#usage","text":"See the user documentation","title":"Usage"},{"location":"solr-search-querytagger-python-django/#installation","text":"download the module solr-search-querytagger-python-django Copy the directory querytagger from the zip file into your Django apps directory Enable the new app: Add \" querytagger \" to your INSTALLED\\APPS setting like this: INSTALLED_APPS = ( ... 'querytagger', ) Include the querytagger URLconf in your project urls.py like this: url(r'^querytagger/', include('querytagger.urls')),","title":"Installation"},{"location":"tagger/","text":"Open Semantic Tagger Tagger is a light weight responsive web app for tagging web pages and documents. It stores the tags for the documents, files or web pages in the Django database and makes them available in RDF.","title":"Open Semantic Tagger"},{"location":"tagger/#open-semantic-tagger","text":"Tagger is a light weight responsive web app for tagging web pages and documents. It stores the tags for the documents, files or web pages in the Django database and makes them available in RDF.","title":"Open Semantic Tagger"},{"location":"trigger/filemonitoring/","text":"Filesystem monitoring (Re)index new or changed files within seconds and find new or changed content in document files much earlier Despite some limitations and after a first initialization time of the file monitoring tools on start , monitoring files and directories is a fast and efficient method to hold up to date the search index for your files or fileshare: Instead of (re)crawling all directories and checking each file if new or changed, the operating system notifies the filemonitoring service when a file was changed or added. So changed documents or new files can be indexed and found within seconds and without recrawl often, since frequent recrawls would burn many resources and can take a while, if there are tens of thousands of files, hundreds of thousands of files or even more. Commandline tool (CLI) (Package: opensemanticsearch-trigger-filemonitoring ) Monitor path(s) given as command line parameter: opensemanticsearch-filemonitoring *pathname* Monitor all files and directories listed in configfilename : opensemanticsearch-filemonitoring --fromfile *configfilename* containing one directory or file per line. Automatically starting system service (Daemon) (Package: opensemanticsearch-trigger-filemonitoring-daemon ) The daemon runs in the background as a system service and will start automatically while booting. Directories and files to monitor It monitors the paths and files listed in the config file (one directory or file per line) /etc/opensemanticsearch/filemonitoring/files Restart daemon after config changes After config changes (i.e. adding new paths or filenames which are not subpaths of yet monitored paths) restart it with: systemctl restart opensemanticetl-filemonitoring File monitoring on remote fileserver (Package: opensemanticsearch-trigger-filemonitoring-daemon-remote ) The file monitoring daemon is available as a lightweight remote package for remote fileservers, which only calls the search API of the remote search server. Directories and files to monitor List the directories or files you want to monitor into the config file /etc/opensemanticsearch/filemonitoring/files containing one directory or file per line. Config remote API address Change the IP address of your search servers and the URL of its API in the config file /etc/opensemanticsearch/filemonitoring/config : config['api'] = \"http://192.168.1.1/search-apps/api\" Blacklisting Like on the search server (later in the indexing process), the remote filemonitoring daemon supports blacklisting too, so there is no network load to API because of changed files in blacklisted directories, blacklisted filename patterns or blacklisted file endings. Limitations Even if filesystem monitoring needs much fewer resources than frequent recrawls, you should be aware of some limitations: Inotify config You have to increase the kernel parameter fs.inotify.max_user_watches to a size fitting your count of monitored directories and files. Well prepare an easy to change conig file for that in the next version. Initialization time On start the filemonitor has to walk recursivly trough all monitored directories and subdirectories to set up an inotify monitoring for this directories. This can take a while: The initialization time can need some minutes to even some dozens of minutes, if the fileserver has hundred thousands of directories. There will be some CPU load and high IO load on the harddrives while initialization. But after this initial directories scan and monitoring setup, it needs much fewer resources than recrawls. While this initialization time not all directories are scanned and so not initialized for monitoring yet, so the file monitoring can not trigger (re)indexing of new or changed files. Passed filesystem changes So while initialization time after boot or restart of the filemonitoring service and because sometimes the system can by so busy, that the file monitoring can pass some notify events for file changes, you should do from time to time an classic recrawl. This can run at night or on weekend and be started automatically by the Cron service. Used methods and libraries Technically the Linux kernels inode notify subsystem inotify notifies when the filesystem writes a file in a monitored directory, so we can react relatively fast when there is new or changed file. So the command line tool or the daemon OpenSemanticSearch-Trigger-Filemonitoring based on the Python library PyInotify will notice that and will start the crawler (communicating over REST-API ) to (re)index the file that is new or was changed .","title":"Filesystem monitoring"},{"location":"trigger/filemonitoring/#filesystem-monitoring","text":"","title":"Filesystem monitoring"},{"location":"trigger/filemonitoring/#reindex-new-or-changed-files-within-seconds-and-find-new-or-changed-content-in-document-files-much-earlier","text":"Despite some limitations and after a first initialization time of the file monitoring tools on start , monitoring files and directories is a fast and efficient method to hold up to date the search index for your files or fileshare: Instead of (re)crawling all directories and checking each file if new or changed, the operating system notifies the filemonitoring service when a file was changed or added. So changed documents or new files can be indexed and found within seconds and without recrawl often, since frequent recrawls would burn many resources and can take a while, if there are tens of thousands of files, hundreds of thousands of files or even more.","title":"(Re)index new or changed files within seconds and find new or changed content in document files much earlier"},{"location":"trigger/filemonitoring/#commandline-tool-cli","text":"(Package: opensemanticsearch-trigger-filemonitoring ) Monitor path(s) given as command line parameter: opensemanticsearch-filemonitoring *pathname* Monitor all files and directories listed in configfilename : opensemanticsearch-filemonitoring --fromfile *configfilename* containing one directory or file per line.","title":"Commandline tool (CLI)"},{"location":"trigger/filemonitoring/#automatically-starting-system-service-daemon","text":"(Package: opensemanticsearch-trigger-filemonitoring-daemon ) The daemon runs in the background as a system service and will start automatically while booting.","title":"Automatically starting system service (Daemon)"},{"location":"trigger/filemonitoring/#directories-and-files-to-monitor","text":"It monitors the paths and files listed in the config file (one directory or file per line) /etc/opensemanticsearch/filemonitoring/files","title":"Directories and files to monitor"},{"location":"trigger/filemonitoring/#restart-daemon-after-config-changes","text":"After config changes (i.e. adding new paths or filenames which are not subpaths of yet monitored paths) restart it with: systemctl restart opensemanticetl-filemonitoring","title":"Restart daemon after config changes"},{"location":"trigger/filemonitoring/#file-monitoring-on-remote-fileserver","text":"(Package: opensemanticsearch-trigger-filemonitoring-daemon-remote ) The file monitoring daemon is available as a lightweight remote package for remote fileservers, which only calls the search API of the remote search server.","title":"File monitoring on remote fileserver"},{"location":"trigger/filemonitoring/#directories-and-files-to-monitor_1","text":"List the directories or files you want to monitor into the config file /etc/opensemanticsearch/filemonitoring/files containing one directory or file per line.","title":"Directories and files to monitor"},{"location":"trigger/filemonitoring/#config-remote-api-address","text":"Change the IP address of your search servers and the URL of its API in the config file /etc/opensemanticsearch/filemonitoring/config : config['api'] = \"http://192.168.1.1/search-apps/api\"","title":"Config remote API address"},{"location":"trigger/filemonitoring/#blacklisting","text":"Like on the search server (later in the indexing process), the remote filemonitoring daemon supports blacklisting too, so there is no network load to API because of changed files in blacklisted directories, blacklisted filename patterns or blacklisted file endings.","title":"Blacklisting"},{"location":"trigger/filemonitoring/#limitations","text":"Even if filesystem monitoring needs much fewer resources than frequent recrawls, you should be aware of some limitations:","title":"Limitations"},{"location":"trigger/filemonitoring/#inotify-config","text":"You have to increase the kernel parameter fs.inotify.max_user_watches to a size fitting your count of monitored directories and files. Well prepare an easy to change conig file for that in the next version.","title":"Inotify config"},{"location":"trigger/filemonitoring/#initialization-time","text":"On start the filemonitor has to walk recursivly trough all monitored directories and subdirectories to set up an inotify monitoring for this directories. This can take a while: The initialization time can need some minutes to even some dozens of minutes, if the fileserver has hundred thousands of directories. There will be some CPU load and high IO load on the harddrives while initialization. But after this initial directories scan and monitoring setup, it needs much fewer resources than recrawls. While this initialization time not all directories are scanned and so not initialized for monitoring yet, so the file monitoring can not trigger (re)indexing of new or changed files.","title":"Initialization time"},{"location":"trigger/filemonitoring/#passed-filesystem-changes","text":"So while initialization time after boot or restart of the filemonitoring service and because sometimes the system can by so busy, that the file monitoring can pass some notify events for file changes, you should do from time to time an classic recrawl. This can run at night or on weekend and be started automatically by the Cron service.","title":"Passed filesystem changes"},{"location":"trigger/filemonitoring/#used-methods-and-libraries","text":"Technically the Linux kernels inode notify subsystem inotify notifies when the filesystem writes a file in a monitored directory, so we can react relatively fast when there is new or changed file. So the command line tool or the daemon OpenSemanticSearch-Trigger-Filemonitoring based on the Python library PyInotify will notice that and will start the crawler (communicating over REST-API ) to (re)index the file that is new or was changed .","title":"Used methods and libraries"}]}